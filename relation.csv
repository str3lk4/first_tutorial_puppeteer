Titulo,Endereco,Imagem,Conteudo
Resultados do TerraLAB no mês de Abril,http://www2.decom.ufop.br/terralab/resultados-do-terralab-no-mes-de-abril/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2022/07/ABRIL-730x350.png,"Essa publicação tem como objetivo mostrar a evolução  do TerraLAB ao longo de cada mês. No artigo anterior mostramos o avanço dos atuais projetos em desenvolvimento no TerraLAB envolvendo as equipes de UX, Engenharia, Análise de Dados, Infraestrutura, Gestão de Projetos e Marketing, sendo eles: o Serviço de Geocodificação em Massa (SGM) e o BatCaverna. 

Sistema de Geolocalização em Massa (SGM):  Na área de logística e de pesquisa de mercado, a geocodificação criou certas facilidades que, até então, não existiam e proporcionaram uma transformação nos métodos de planejamento estratégico, organizacional e de distribuição de mercadorias. 

Um serviço de geocodificação em massa na nuvem deve ser desenvolvido pela grande aplicabilidade em diversas empresas de diferentes ramos, pois garante perspectiva de crescimento logístico, financeiro e tecnológico. Além disso, na grande maioria das vezes, dados sobre os sistemas sociais são vinculados a um endereço de logradouro ao invés de uma coordenada geográfica. Este é, por exemplo, o caso de eventos de saúde ou criminais. Consequentemente, o serviço de geocodificação é útil para a espacialização destes eventos e para o entendimento das maioria dos sistemas sociais. Com essas informações o projeto AWS SGM: Projeto de Geocodificação, busca trabalhar no problema: Como conhecer a localização de um logradouro, sem ir a campo?

BatCaverna: O aplicativo tem como objetivo fazer a coleta detalhada do patrimônio espeleológico da área delimitada no estado de Minas Gerais e busca contribuir para o incremento do banco de dados das cavernas presentes. A principal ferramenta será a de cadastro de pontos de controle ou de feições com as suas respectivas descrições e imagens associadas. Aliado a esse cadastro de pontos, o aplicativo irá coletar os caminhos feitos pelo o usuário e, durante esse caminho, ele terá a liberdade de criar quantos pontos forem necessários. Outra funcionalidade importante, que está atribuída a esse aplicativo, é a de visualizar os caminhamentos, tanto de uma forma individual, clicando sobre o caminho realizado, quanto de forma geral que será exibida em seu menu principal. O sistema também contará com um gerador de relatórios baseado nos pontos e fotografias tiradas em campo.

Na última semana do mês de abril ocorreu a validação mensal do trabalho realizado pelas equipes do TerraLAB e iremos, aqui, apresentar os pacotes de entregas realizados pelos times no mês que terminou.

Gerência Técnica

Na parte do frontend do IG são mostrados os avanços no serviço de Áreas de Influência, especificamente nos componentes de Isócronas e de Ferramentas Geográficas. Sobre o Crawler utilizado pela aplicação, trazemos novidades sobre o serviço de Escalonamento e nos testes automatizados da aplicação, nos quais houve um avanço de 30% de cobertura total. Já no servidor de dados geográficos, foram mostrados o serviço de Controle de camadas de mapas e a funcionalidade de Adição de Geoserviços.

Infraestrutura
O time de infra do TerraLab trabalhou esse mês com atividades operacionais, realizando a separação de ambientes das aplicações do laboratório (em ambientes de produção e de teste, por meio do CI/CD), a aprimoração da automatização de deploy das aplicações do Lab utilizando do serviço Terraform, na construção de um repositório privado para imagens, prestando auxílio às outras equipes e dando início ao processo seletivo para o time de Infraestrutura.

D&A
Os integrantes da equipe de Data & Analytics entregaram os Dashboards de análise qualitativa dos dados, e narram seu desenvolvimento desde a proposta, comentando as demandas extraordinárias que surgiram no processo, até exibir os resultados obtidos pelo software, os comparando aos modelos de análise projetados.



UX
A equipe de UX ficou responsável pela criação de telas para o serviço de Rotas do time de Inteligência Geográfica, e por realizar uma pesquisa de mercado sobre a aplicação Batcaverna. Além disso, é comentado o início do processo seletivo do time de UX, contando agora com os candidatos a Analista de Negócios. Enfim, são comentados os próximos serviços que serão desenvolvidos pelo time.

Marketing

A equipe de marketing continuou seu trabalho com as redes sociais e o desenvolvimento e publicação de artigos neste mês. Além disso, realizaram uma análise do processo seletivo que havia se encerrado, dando as boas vindas aos novos participantes do TerraLAB. Também foram realizados o realinhamento de tarefas entre os membros do time e a análise das métricas das redes utilizadas no Lab. Por fim, o time avançou na configuração de ferramentas importantes para a realização de seus serviços.

Batcaverna
A equipe responsável pelo projeto de aplicativo móvel Batcaverna apresenta os seus serviços prestados e avanços no mês de Abril. Iniciamos com uma recapitulação do que foi realizado no mês anterior, seguido de uma explicação sobre o serviço de Exportação de Projetos e o serviço de Gerenciamento de Projetos, tudo isso pareado com demonstração visual do avanço obtido na aplicação. 

Nós estamos há onze meses trabalhando nos projetos aqui citados. Utilizamos o primeiro mês para planejamento e esta é, portanto, nossa decima entrega. Caso você represente uma empresa ou seja um/uma profissional experiente, gostaríamos do seu feedback para que possamos melhorar continuamente o nosso trabalho.

O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab!

Este artigo foi escrito por Luka Menin, revisado por Prof. Rodrigo Silva."
O que aprendi com o TerraLab,http://www2.decom.ufop.br/terralab/o-que-aprendi-com-o-terralab/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2022/05/BANNER-O-QUE-APRENDI-COM-O-TERRALAB.png,"Émuito comum, em todo tipo de canal midiático, encontrarmos enxertos quase que sensacionalistas sobre como sermos mais produtivos, como alcançarmos o sucesso, como ter maior inteligência emocional e entre outros. O comum desses escritos é que todos tentam retificar experiências sensíveis sob fórmulas e processos palpáveis, como receitas de bolos, num passo a passo em que no fim se tem um produto objetivo e irredutível.

Ainda assim, no final do dia, cada um é um. Uma pessoa que nunca andou de bicicleta, por mais que leia todos os livros, assista inúmeros vídeos, observe os mais proficientes atletas, nunca receberá desses meios a experiência real do exercício daquilo que observa. É preciso botar a mão no guidão e provavelmente ralar as canelas.

Por mais que todos nós saibamos disso, não deixamos de ser atraídos pela necessidade da experiência subjetiva por trás de todo exercício que a humanidade é capaz de realizar. Há uma atração inevitável pelo ‘macete’, pela ‘manha’ por trás das coisas. E assim seguimos, fantasiando a realidade através da experiência imaginária, da preparação teórica e do arquétipo da perfeição.

Isso acontece pela nossa capacidade de imaginar e projetar esse conhecimento efetivamente no dia-a-dia. Talvez nos sintamos menos ansiosos, ou quem saiba aconteça exatamente o contrário. Essa é a maior armadilha da produção subjetiva que existe: não há certo ou errado nesse mundo imaterial; as leis da relatividade com certeza são as maiores regentes desse espaço ambíguo. Acredito que o leitor já esteja se perguntando o que esses parágrafos metafísicos e filosóficos tem haver com o TerraLab ou com tecnologia e ciência. E lhes adianto: o relato que quero compartilhar é exatamente isso, seja lá o que isso for.

A minha experiência é única e subjetiva, pautada pelo meu perfil psicológico, carga cultural, vivência e todos os fatores que me tornam diferente de cada um ao meu redor. Ainda assim, existe algum valor na transmissão daquilo que achamos ser sublime, único, especial. Só não haverá ao longo desse artigo, a tentativa de fazer o leitor engolir a minha perspectiva como algo assertivo, mas sim completamente intimista e particular. Onde outro alguém ocupando o mesmo espaço pode viver de forma completamente diferente e chegar a resultados completamente opostos; e queiramos ou não, a variedade e diferenças que existem na humanidade são nossos maiores trunfos. Aceitar e trabalhar esse fator é uma qualidade extremamente útil – e creia ou não, aprendi isso com o TerraLab.

O TerraLab, como todos sabem, é um satélite institucional da Universidade Federal de Ouro Preto, voltado para a produção científica e capacitação dos estudantes interessados em navegar no sideral tecnológico. Através de seu famigerado processo seletivo, passamos a ocupar postos estruturados seguindo os moldes industriais e referenciais científicos mais bem sucedidos que existem por aí. Do Spotify ao Uncle Bob, essa fábrica laboratorial passa a metabolizar a força estudantil da melhor maneira possível, de forma a sedimentar uma base lunar acadêmica no ciberespaço selvagem do mercado da tecnologia da informação.

O perfil estudantil que entra nesse foguete é mais variado do que espécies de ervilhas. Das profundezas da terra, brotam os mais diversos personagens em busca de experiências, glórias e fama. É muito comum a grande parte desse ecossistema evadir durante ou logo após o processo seletivo. A maioria acredita que essa viagem espacial será guiada pelo conhecimento ancestral de uma frota de vulcanos extremamente especializados, mas se assusta ao perceber que são eles próprios quem estão no controle da embarcação.

Isso é, o laboratório oferece uma vasta gama de assistência técnica para os mais diversos setores e ambientes com os quais qualifica os estudantes. Depende de cada um deles se empoderar do conteúdo que lhes é proporcionado e também dialogar com os estudantes inseridos no laboratório. Mas, a maioria dos indivíduos é talvez desinteressada, preguiçosa, ou simplesmente incapaz de se arriscar no relacionamento interpessoal e se comunicar com as pessoas ao redor. É claro, isso tudo é suposição, e cada um é cada um, mas grande parte dos estudantes com o qual tive a oportunidade de me relacionar, na maioria das vezes simplesmente se isola num silencio profundo, restando a montanha ir até Maomé todas as vezes.

A outra parte, que fica, tem dois destinos quase certeiros: após se nutrir do conhecimento científico do laboratório, cria coragem para se lançar nas constelações do mercado e aciona os compartimentos de fuga de nossa nave sem deixar saudade. Isso acontece porque a grande maioria está interessada em um estágio remunerado, e não as qualidades mais profundas e especiais do laboratório, as quais vou abordar mais a frente. É claro, nem todos são tão interesseiros assim, outra grande parte simplesmente sucumbe ao peso psicológico de dar conta da faculdade, de outras atividades extra-curriculares ou dos fatores da vida pessoal e também acabam abandonando o navio, sem se dar conta que existe nesse ambiente, todo auxílio e apoio para que possam se desenvolver na medida do seu próprio tempo.

Se alguém me perguntar, no fim das contas, sobre o que grande parte desses dois parágrafos querem dizer de mais profundo, a pauta inicial desse enxerto emerge novamente. Existem tantos fatores subjetivos nesse cenário que é completamente impossível dizer o que acontece de verdade ou não. Se questionarmos cada um dos estudantes, colheremos os mais diferentes cenários e desculpas sendo difícil estabelecer a realidade dos fatos. Mas a grande maioria nunca se pronuncia ou tenta acessar as pessoas ao redor, e é aí onde se estabelece o ponto chave dos eventos.

O hiper luxuoso termo soft-skills é uma das outras pautas que os tipos de artigos que citei no início deste texto tentam sensibilizar e retificar. Relacionar-se é o intuito da humanidade, e só através da execução do papel social que podemos nos realizar. E o TerraLab é sobre isso. Os que ficam, ficam não porque em suma tem alta capacidade técnica e carga científica, mas sim por que são capazes de quebrar as barreiras do isolamento social, da realidade imaginária, pondo suas subjetividades a prova no campo de guerra da realidade; onde as vozes gritam, explodem, mas também se elevam entre o discurso, o diálogo e o debate.

No fim, não escapamos dessa questão tão profunda que abordei no início do texto: a subjetividade é única e intransferível, mas tem a necessidade desesperadora de se manifestar e compartilhar para que dessa forma, possa se realizar. A maior barreira em nós, somos nós mesmos, e tentamos a todo momento, rompê-la para ir além, e o TerraLab e assim como todo espaço nesse mundo é sobre isso. Não existe processo, ou receita prévia nessa esfera do universo. Tudo é tentativa e erro, ensaio e execução.

Os que ficam, são levados ao extremo disso. Num curto período de tempo, ocupamos diversas posições e responsabilidades e cabe a cada um tirar o melhor proveito disso. Eu tive o privilégio de chegar ao laboratório com uma base técnica maior que os colegas de minha área, isso me proporcionou uma bolsa de estudos que se tornou um ótimo estágio. Mas não foi somente a capacidade técnica que me proporcionou tudo isso. Dado minha ínfima curiosidade, cavei cada buraco possível do laboratório, me intrometi nos mais diversos cantos, gerei confusão nos mais variados setores, fui de cabo a rabo por todo canto, através da boca e do interesse pessoal. 

Isso tudo são características pessoais e íntimas, outros colegas que hoje estão no mesmo lugar que eu seguiram caminhos diferentes: focaram toda sua energia num único ambiente e os levaram ao seus limites, se colocando à prova de maneiras diferentes. É por isso que não existe fórmula e o segredo se baseia no comunicar, na comunhão de nossa subjetividade com o outro, na realização da atividade social como um todo.

O TerraLab é mais que tudo, um espaço de relacionamento e conexões. Seu treinamento se baseia não só na formação técnica, mas na formação social. Seu amadurecimento não é simplesmente científico, mas intimista. Os limites não são profissionais e sim interpessoais. E nada disso se ensina, mas de alguma forma se aprende, se vive, se consagra na maravilha do existir. E foi isso, o que aprendi com o TerraLab.

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Siga-nos nas redes sociais para saber mais!

Este artigo foi escrito por Bernardo Emery, revisado por Prof. Rodrigo Silva."
Por que o fluxo de usuário pode determinar o sucesso de seu produto digital?,http://www2.decom.ufop.br/terralab/por-que-o-fluxo-de-usuario-pode-determinar-o-sucesso-de-seu-produto-digital/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2022/05/Por-que-o-fluxo-de-usuario-pode-determinar-o-sucesso-de-seu-produto-digital.png,"Para chegar a um objetivo, existe uma sequência de passos que inevitavelmente deve ser percorrida pelas pessoas. Essa ideia pode ser vista como um fluxo. Quando se trata de uma situação de uso de um produto digital, a forma como o usuário é guiado tem total impacto sobre as chances de que ele chegue ao seu objetivo. No momento em que o usuário alcança o que procura, ele recebe um ganho de valor. Nesse sentido, não é difícil observar que, pela perspectiva das partes interessadas no sucesso do negócio, a maneira com que o usuário atinge esse objetivo ganha relevância.

O usuário conseguiu atingir seu objetivo? Se sim, qual é a sua impressão, naquele momento, sobre a experiência que ele teve até chegar lá? Quantos usuários conseguem chegar ao destino que é desejado pelo seu negócio?

Todas as fases que o usuário passa, até encontrar o que procura, têm significado e podem o estimular ou desestimular, a depender de como essa trajetória foi desenvolvida.

O usuário sentirá a diferença

A forma como o fluxo de usuário é construído pode contribuir de modo efetivo em uma otimização de fatores como o tempo e o esforço que são exigidos de um usuário em uma interação com o produto. É desejável que se tenha clareza nos passos do processo que é realizado pelo indivíduo, auxiliando-o para que a finalidade daquele processo seja cumprida.

Pensar no ponto de partida e no ponto final é importante, no entanto, existe uma trajetória entre essas duas fases. Assim,  o fluxo pode tanto impactar positivamente, tornando o percurso mais simples, mais intuitivo e prazeroso, quanto pode arruinar a experiência, resultando em uma percepção negativa, em frustração, gasto de tempo e de energia.

A construção de um fluxo de usuário

Antes de iniciar o fluxo de usuário é preciso que se conheça o usuário, pensar em suas necessidades, seus desafios, suas escolhas, seus objetivos. Com o usuário em mente, é hora de traçar o caminho que o usuário irá percorrer em seu produto digital. Tornar a ideia visível é uma ótima maneira de melhorar a percepção sobre ela e enxergar os acertos e erros que estão sendo cometidos. Na elaboração do fluxo de usuário, use esse recurso como vantagem. 

Pegue um papel, um lápis, uma caneta…o que você tiver à disposição para dar mais vida a sua ideia de fluxo de usuário e para ajudá-lo no planejamento. Você também pode utilizar ferramentas que o permitam fazer ilustrações, diagramas, fluxogramas, como o Figma e o Miro.
Faça formas e depois defina os significados que elas terão em seu fluxo. Alguns modelos podem adotar um círculo para um evento, pontos de entrada e saída de um fluxo; um retângulo/quadrado para as tarefas, ações; um losango para as decisões, escolhas; e uma flecha para fazer as conexões que serão representadas no fluxo, formando uma visualização dos caminhos que podem ser percorridos.
Pense no fluxo que será representado e construído. Pense no usuário, pense no seu produto. O usuário está em que tela? O que o usuário pode fazer nesta tela? Para qual tela ele quer ser levado? Qual funcionalidade ele busca?
Como um exemplo, visualize que um usuário deseja receber conteúdos e novidades personalizadas para seu perfil, considerando os seus interesses, em uma versão de site de tecnologia fictícia do “TerraLAB”. O usuário se encontra em uma tela com notícias sobre tecnologia na qual é apresentada uma seção com dois botões que direcionam a pessoa para duas possibilidades: fazer login ou cadastrar-se.

O usuário deseja receber aqueles conteúdos exclusivos, conforme foi oferecido naquela seção. Com isso, ele tem de tomar uma decisão:

Ele pode “Fazer login” e utilizar seus dados para usar seu perfil cadastrado e, dessa forma, continuar personalizando os conteúdos de acordo com as categorias de notícia e interações que ele demonstrou mais interesse. Outra alternativa é “Cadastrar-se”, nesse caminho ele poderá realizar o cadastro de um novo perfil.

Observe que o usuário teve que tomar uma decisão e essa decisão o levará a caminhos diferentes. Ainda assim, ambos os caminhos o levarão a seu objetivo de ter um perfil cadastrado no site de tecnologia fictício do “TerraLAB” e, a partir disso, conseguir receber conteúdos personalizados do site.
O usuário escolherá o caminho que for mais adequado a situação que ele se encontra. Tomando o caminho “Fazer o login” ele seguirá com mais fluxos, de acordo com a aplicação. Existe, por exemplo, a possibilidade de que o usuário não se lembre da sua senha, dessa forma, uma etapa seguinte que permita ao usuário recuperar, lembrar e/ou redefinir a sua senha deve ser considerada necessária.
Um exemplo de modelo de fluxograma para o fluxo de usuário abordado nesta situação é:
Seja utilizando o lápis e papel, ou utilizando uma ferramenta em software, essa situação abordada pode ser melhor percebida como um todo e analisada, desde etapas anteriores até as etapas que darão sequência. O limite é o tamanho de seu fluxo. Lembre-se, contudo, de que um bom fluxo não se resume a ter um caminho gigantesco, nem mesmo a ter inúmeras possibilidades que deixam o usuário até perdido.

Perceba como é relevante usar o fluxograma para visualizar etapas desnecessárias e etapas que estão faltando para tornar o fluxo de usuário mais agradável, intuitivo e com garantias de que o caminho leve o usuário a seu objetivo.
O usuário deve ser guiado

Considerando um pressuposto de que o usuário não sabe inicialmente qual é o caminho que ele deve seguir para alcançar o seu alvo, é necessário que o fluxo pelo qual ele deve passar seja o mais claro, intuitivo e objetivo, o quanto for possível. A ideia a ser mantida é de que o usuário precisa ser conduzido e um fluxo bem elaborado é uma ferramenta essencial para isso. E, ainda que a pessoa em questão tenha uma noção avançada de uso do sistema, adotar essa medida trará consideráveis benefícios como os já mencionados: redução de tempo, redução de esforço e efeito positivo na usabilidade.

Dessa forma, uma combinação entre o fluxo que favoreça a navegação do usuário e uma interface com elementos que o guiem, com uma arquitetura de informação adequadamente planejada, com seções bem definidas, hierarquicamente coesas e visualmente assimiláveis são chaves para oferecer uma melhor experiência de uso.

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Siga-nos nas redes sociais para saber mais!

Este artigo foi escrito por Patrick Tadeu, revisado por Prof. Rodrigo Silva."
Resultados do processo seletivo 2022/1,http://www2.decom.ufop.br/terralab/resultados-do-processo-seletivo-2022-1/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2022/04/BANNER-RESULTADOS-PROCESSO-SELETIVO--730x350.png,"Com o fim do mês de março, veio também a conclusão do processo seletivo do Programa Trainee TerraLab 2022/1. Há três semanas os candidatos finalizaram todos os cursos e entregaram todos os desafios propostos por cada área, dando fim ao processo do laboratório. Esse artigo tem como objetivo explicar mais sobre o processo e apresentar os resultados alcançados por cada equipe.

Esse programa realizado pelo TerraLab já vem, há alguns anos, trazendo novas oportunidades para alunos de graduação e pós-graduação da UFOP (Universidade Federal de Ouro Preto). O mesmo é organizado em 5 sprints (ciclos de atividades propostas por cada equipe) e, nesse período, os trainees passam por diversos cursos e desafios. Nessas cinco semanas, o TerraLab mais uma vez capacitou diversos estudantes para trabalharem com atividades da área de tecnologia da informação, indo desde equipes de infraestrutura, passando pelo desenvolvimento de software e UX, até o marketing.

Abaixo, podemos acompanhar como foi realizado o processo de seleção de novos integrantes para o laboratório em cada área específica do TerraLab, conhecendo seus desafios e as entregas realizadas pelos trainees.

Infraestrutura

A equipe de Infraestrutura trabalhou nessas semanas com o objetivo de entregar uma aplicação disponível na EC2 (plataforma de computação em nuvem da Amazon) contando ainda com a criação de ambientes de teste, desenvolvimento e produção para a mesma. Nos três sprints iniciais, o time passou por cursos de capacitação sobre Linux, Gitlab e o serviço de máquinas virtuais da Amazon, além de se familiarizarem com Dockers. Após esse período, os trainees entregaram, nas semanas seguintes, o deploy da aplicação desenvolvida nas máquinas virtuais e a organização no git separando as diferentes estruturas do processo.

Data Analytics (D&A)

Na área de Data Analytics, os trainees passaram por cursos que os capacitaram na utilização do Python, Git e SQL. Essas são as principais ferramentas utilizadas no TerraLab para o processo de extração, transformação e análise de dados e, depois dessas três primeiras semanas, essas foram as atividades realizadas pelos trainees: com a ajuda de seus novos conhecimentos, ficaram responsáveis pela análise e extração de dados de um banco disponibilizado pela equipe.

Backend

No backend, os trainees aprofundaram seus conhecimentos em Git, Node (junto do typescript) e Python através dos cursos disponibilizados pelo TerraLab. Com esses novos conhecimentos, a equipe foi desafiada a colocar em prática o que foi aprendido através da escrita de códigos que suprissem as necessidades propostas nos desafios, utilizando as ferramentas introduzidas e exploradas nos cursos das primeiras semanas do processo.

Frontend

O objetivo da equipe de frontend, através de cursos nas três primeiras semanas e desafios nas duas últimas, era produzir códigos executáveis de interfaces gráficas utilizando o ecossistema de tecnologias do TerraLab. Nos cursos, desenvolveram seus conhecimentos em Git, Typescript e React e, ao fim deles, produziram uma aplicação básica para testar seus conhecimentos em React e outra para avaliar a capacidade do trainee de trabalhar com o stack utilizado no laboratório.

User Experience (UX)

No processo do time de UX, os trainees tinham como meta o desenvolvimento de suas habilidades na área através de cursos e, feito isso, a aplicação de seus conhecimentos em um caso previamente definido. Para isso, foi realizada a capacitação desde os fundamentos básicos de UX até o modo de utilização do Figma, principal ferramenta utilizada para os processos de criação da equipe. Por fim, foi disponibilizado um case que foi analisado pelos trainees para  gerar briefings e personas que, por sua vez, deram origem a uma tela e seu styleguide (constando as principais cores, fontes e ícones utilizados)

Marketing

Por último, os trainees da área de marketing realizaram cursos que cobriram Marketing de Conteúdo e Inbound Marketing, além de se aprofundarem no conceito de SEO (ranqueamento de páginas nos algoritmos de plataformas de pesquisa, como Google). Após o desenvolvimento dessas habilidades, foram realizados diversas poblicações em redes sociais (contando com a produção de artes originais), o desenvolvimento de um calendário editorial para as mesmas, além da produção dos artigos de conclusão do processo seletivo (esse mesmo que você está lendo) e de resultados do laboratório no mês de fevereiro de 2022.

Conclusão

O Programa Trainee do TerraLab do primeiro semestre de 2021 se mostrou extremamente efetivo na capacitação e, de certa forma até, na profissionalização de seus participantes. Os times agora possuem integrantes qualificados em suas respectivas áreas e que poderão dar continuidade no trabalho realizado dentro do laboratório, participando das atividades, treinando novos trainees nos próximos processos de efetivação e se tornando parte essencial da nova equipe do TerraLab.
Nesse período, foram utilizadas plataformas que disponibilizam cursos na área de tecnologia como, por exemplo, Alura, Rock University e Udemy. Através dessas plataformas, os participantes do processo seletivo adquiriram novos conhecimentos (ou aprofundaram conhecimentos já existentes) necessários para ingressar no mercado de trabalho da área de Tecnologia da Informação, alcançando dessa forma novas possibilidades e oportunidades na vida profissional dos estudantes.

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. 

O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab!

Este artigo foi escrito por Luis Felipe Fonseca Campioto e revisado por Prof. Rodrigo Silva."
Resultados do TerraLAB no mês de Março,http://www2.decom.ufop.br/terralab/resultados-do-terralab-no-mes-de-marco/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2022/04/Marco-730x350.png,"Esta é uma sequência de artigos que tem como objetivo mostrar a evolução dos integrantes do TerraLAB ao longo de cada mês. No artigo anterior mostramos parte do desenvolvimento dos dois projetos que estão em andamento no TerraLAB envolvendo as nossas equipes de UX, Engenharia, Análise de Dados, Infraestrutura, Gestão de Projetos e Marketing, são eles: o Serviço de Geocodificação em Massa (SGM) e o BatCaverna. 

Sistema de Geolocalização em Mass (SGM):  Na área de logística e de pesquisa de mercado, a geocodificação criou certas facilidades que, até então, não existiam e proporcionaram uma transformação nos métodos de planejamento estratégico, organizacional e distributivo de mercadorias. Um serviço de geocodificação em massa na nuvem deve ser desenvolvido pela grande aplicabilidade em diversas empresas de diferentes ramos, pois garante perspectiva de crescimento logístico, financeiro e tecnológico. Além disso, na grande maioria das vezes, dados sobre os sistemas sociais são vinculados a um endereço de logradouro ao invés de uma coordenada geográfica. Este é, por exemplo, o caso de eventos de saúde ou criminais. Consequentemente, o serviço de geocodificação é útil para a espacialização destes eventos e para o entendimento das maioria dos sistemas sociais. Com essas informações o projeto AWS SGM: Crawler de Geocodificação, busca trabalhar no problema: Como conhecer a localização de um logradouro, sem ir a campo?
BatCaverna: O aplicativo tem como objetivo fazer a coleta detalhada do patrimônio espeleológico da área delimitada no estado de Minas Gerais e busca contribuir para o incremento do banco de dados das cavernas presentes. A principal ferramenta será a de cadastro de pontos de controle ou de feições com as suas respectivas descrições e imagens associadas. Aliado a esse cadastro de pontos, o aplicativo irá coletar os caminhos feitos pelo o usuário e, durante esse caminho, ele terá a liberdade de criar quantos pontos forem necessários. Outra funcionalidade importante, que está atribuída a esse aplicativo, é a de visualizar os caminhamentos, tanto de uma forma individual, clicando sobre o caminho realizado, quanto de forma geral que será exibida em seu menu principal. O sistema também contará com um gerador de relatórios baseado nos pontos e fotografias tiradas em campo.

Na última semana do mês de março ocorreu a validação mensal do trabalho realizado pelas equipes do TerraLAB e iremos, aqui, apresentar os pacotes de entregas realizados pelos times no mês que terminou.

Gerência Técnica

Na parte do frontend do IG foram realizados testes automatizados (que incluíram, por exemplo, a função de perfis, a seleção de rotas, etc) além de correções na estilização do app, para ficar mais próximo do protótipo, e correção de bugs. No crawler utilizado pela aplicação foram realizados testes automatizados e o escalonamento de chaves de API. Já no servidor de dados geográficos, foi finalizada a importação de camadas raster.

Infraestrutura

O time de infra do TerraLab trabalhou esse mês com atividades operacionais (auxiliando outros setores do laboratório em diversos frontes), com a separação dos ambientes de Produção e Teste (voltados ao backend e seu crawler) e trabalhou, também, na parte de automatização, envolvendo um Terraform criado pela equipe para o crawler de backend.

DATA & ANALYTICS (D&A)

Os integrantes da equipe de Data & Analytics continuam no processo de geocodificação dos dados captados pelo crawler e, nesse relatório, apresentam a porcentagem dos dados analisados. Além disso, expõe também os locais, em um mapa, desses dados.

USER EXPERIENCE (UX)

A equipe de UX ficou responsável pela componentização de elementos de tela (para maior facilidade em situações de alterações dos elementos gráficos), pelo serviço de ferramentas geográficas e, por fim, iniciaram o projeto do produto Batcaverna, realizando uma pesquisa de mercado para aumentar a assertividade na escolha de um negócio para utilização da plataforma.

Marketing

A equipe de marketing continuou seu trabalho com as redes sociais e o desenvolvimento e publicação de artigos neste mês. Além disso, realizaram as métricas das redes, mostrando especificamente como cada uma sofreu alterações e participaram de uma reunião em que ocorreu o realinhamento de tarefas para integrar ainda mais os novos participantes do time. Por fim, o time desenvolveu dois novos artefatos com o intuito de administrar de forma eficaz a organização de tarefas de cada colaborador.

Nós estamos há dez meses trabalhando nos projetos aqui citados. Utilizamos o primeiro mês para planejamento e esta é, portanto, nossa nona entrega. Caso você represente uma empresa ou seja um/uma profissional experiente, gostaríamos do seu feedback para que possamos melhorar continuamente o nosso trabalho.

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. 

O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab!

Este artigo foi escrito por Luis Felipe Fonseca Campioto e revisado por Prof. Rodrigo Silva."
Resultados do TerraLAB no mês de Fevereiro,http://www2.decom.ufop.br/terralab/resultados-do-terralab-no-mes-de-fevereiro/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2022/03/Fevereiro-730x350.png,"Esta é uma sequência de artigos que tem como objetivo mostrar a evolução dos integrantes do TerraLAB ao longo de cada mês. No artigo anterior mostramos parte do desenvolvimento dos dois projetos que estão em andamento no TerraLAB envolvendo as nossas equipes de UX, Engenharia, Análise de Dados, Infraestrutura, Gestão de Projetos e Marketing, são eles: o Serviço de Geocodificação em Massa (SGM) e o BatCaverna. 

Sistema de Geolocalização em Mass (SGM):  Na área de logística e de pesquisa de mercado, a geocodificação criou certas facilidades que, até então, não existiam e proporcionaram uma transformação nos métodos de planejamento estratégico, organizacional e distributivo de mercadorias. Um serviço de geocodificação em massa na nuvem deve ser desenvolvido pela grande aplicabilidade em diversas empresas de diferentes ramos, pois garante perspectiva de crescimento logístico, financeiro e tecnológico. Além disso, na grande maioria das vezes, dados sobre os sistemas sociais são vinculados a um endereço de logradouro ao invés de a uma coordenada geográfica. Este é comumente os casos dos eventos de saúde ou criminais. Consequentemente, o serviço de geocodificação é útil para a espacialização destes eventos e para o entendimento das maioria dos sistemas sociais. Com essas informações o projeto AWS SGM: Crawler de Geocodificação, busca trabalhar no problema: Como conhecer a localização de um logradouro, sem ir a campo?
BatCaverna: O aplicativo tem como objetivo fazer a coleta detalhada do patrimônio espeleológico da área delimitada no estado de Minas Gerais e busca contribuir para o incremento do banco de dados das cavernas presentes. A principal ferramenta será a de cadastro de pontos de controle ou de feições com as suas respectivas descrições e imagens associadas. Aliado a esse cadastro de pontos, o aplicativo irá coletar os caminhos feitos pelo o usuário e, durante esse caminho, ele terá a liberdade de criar quantos pontos forem necessários. Outra funcionalidade importante que está atribuída a esse aplicativo é a de visualizar os caminhamentos, tanto de uma forma individual, clicando sobre o caminho realizado, quanto de forma geral que será exibida em seu menu principal. O sistema também contará com um gerador de relatórios baseado nos pontos e fotografias tiradas em campo.

Na última semana do mês de fevereiro ocorreu a validação mensal do trabalho realizado pelas equipes do TerraLAB e, especialmente neste relatório, iremos apresentar os pacotes de entregas realizados pelos times desde novembro até fevereiro. 

SIG (SISTEMA DE INTELIGÊNCIA GEOGRÁFICA)

No back-end, Higor Duarte apresenta o que foi alcançado pela sua equipe: foi realizado a documentação do código, dos testes e das rotas e a implementação de testes automatizados e de um modelo de multiprocessamento (responsável por possibilitar que mais de um usuário utilize o programa ao mesmo tempo). Além disso, foram feitos diversos experimentos para análise de desempenho do sistema.

No front-end, Bernrado Emery e Gabriel Niquini fazem a demonstração de toda a timeline de processos realizados pelo seu time desde julho de 2021, começando com o início conceitual e conhecimento do produto, passando pelo desenvolvimento e aprimoramento do código, até alcançar um protótipo definitivo para o produto.

Já sobre o crawler (“robô” automatizado responsável pela busca e coleta de dados na internet) desenvolvido, acompanhamos junto com o Gustavo Castro os resultados desse período. Foram realizadas pouco mais de 400 mil geocodificações em um período de 30 dias, utilizando 5 APIs e com uma taxa de falha de 2% (falhas são respostas nulas ou que apresentam erros na requisição do serviço).

BATCAVERNA

A Product Owner (PO) do Batcaverna nos apresenta nesse vídeo os resultados alcançados dentro do projeto. Foram realizados avanços nos processos de coleta de dados e validação de formulários: os dados inseridos pelos usuários nos formulários já estão sendo salvos e validados de acordo com os requisitos dos espaços preenchidos.

APP SEGURANÇA DA MULHER

No dia 8 de março, foi comemorado o Dia Internacional da Mulher e, lembrando da importância dessa data no reconhecimento dos feitos das mulheres, a Ouvidoria Feminina Athenas (do Departamento de Direito da UFOP), em parceria com o  laboratório TerraLab, lançou o app Segurança da Mulher. Neste aplicativo, os usuários podem registrar casos de assédio, violência ou qualquer tipo de desconforto sofridos em certos estabelecimentos e, com essas informações, é criado um nível de vulnerabilidade de cada local. Além disso, os responsáveis por esses estabelecimentos possuem a chance de responder às denúncias, promovendo uma possibilidade de retratamento por parte deles.

DATA ANALYTICS

No vídeo deste mês, o gerente de Data Analytics, Rafael Christian traz os avanços realizados pela sua equipe na base do geocodificador e nas ferramentas de análise para dados geoespaciais. Primeiramente, na base de dados, foram realizadas duas novas atualizações que englobam a padronização dos dados e a remoção daqueles dados que se encontram fora da cidade. Em seguida, somos apresentados ao novo fluxo de ETL (em ETLs, os dados são extraídos de uma base, convertidos para formatos úteis e, depois, armazenados em outro sistema) criado pela equipe com a função de validar a base de conhecimento através da checagem dos dados coletados pelo crawler (processo responsável pela busca e coleta de dados na internet). Além disso, foi desenvolvida uma biblioteca para análise de dados com todas as funções documentadas e testadas que pode, ainda, ser instalada através do PYPI (repositório de software oficial de terceiros para Python). Para finalizar, no front-end a equipe desenvolveu um código com objetivo de fornecer as funções da biblioteca de forma intuitiva para os usuários.

INFRAESTRUTURA

No vídeo, Ana Luiza Almeida e Emanuel Xavier, discorrem sobre os feitos realizados pela sua equipe durante esse período. O time de infraestrutura trabalhou com os crawlers de infra (desenvolvido por eles mesmos) e de backend, ajudou no Front GIS e no frontend D&A e auxiliou no desenvolvimento da dashboard do D&A. Além disso, somos apresentados ao modo de funcionamento do Serviço de Decodificação Serverless.

USER EXPERIENCE

O gerente de UX Vitor Hugo Fonseca nos apresenta os resultados das atividades de sua equipe. A mesma trabalha com um serviço de inteligência geográfica que atende ao setor de expansão de redes de supermercado na escolha de locações para novas lojas. Dentro desse escopo, o time passou pelas fases de planejamento através de briefings, entrevista com o cliente e a criação de personas (personagens fictícios criados para ajudar na pesquisa de público-alvo). Além disso, foram desenvolvidos critérios para aceitação de novos usuários e histórias desses usuários para facilitar o desenvolvimento da UI. Passando para o Serviço de Inteligência Geográfica, foi feita a padronização na nomenclatura dos elementos interativos do código.

MARKETING

Por fim, o gerente de marketing, Luka Menin, traz as atividades realizadas pela equipe de marketing durante o último mês. Além do relatório de todos os artigos postados no blog e as publicações realizadas nas redes sociais, são apresentados os processos e resultados percorridos pelo trainee de marketing, inscrito durante o processo seletivo que se iniciou no mês de janeiro. Por último, a equipe está no processo de aprendizado de novas técnicas e tecnologias e, pela falta de um integrante dedicado ao marketing, ainda não produziu nenhum documento sobre o assunto.

Nós estamos há dez meses trabalhando nos projetos aqui citados. Utilizamos o primeiro mês para planejamento e esta é, portanto, nossa oitava entrega. Caso você represente uma empresa ou seja um/uma profissional experiente, gostaríamos do seu feedback para que possamos melhorar continuamente o nosso trabalho.

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. 

O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab!"
Hands-On Parte 4: Infraestrutura na AWS – Finalizando o projeto de funções lambda na AWS,http://www2.decom.ufop.br/terralab/hands-on-parte-4-infraestrutura-na-aws-finalizando-o-projeto-de-funcoes-lambda-na-aws/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2022/02/Hands-On-Parte-4-Infraestrutura-na-AWS-Implementando-funcoes-lambda-na-AWS-Banner-730x350.png,"Hoje finalizaremos nosso hands-on de uma arquitetura na AWS. Cada lambda irá receber um código em python para realizar suas tarefas, dessa forma, o conhecimento será direcionado a como acessar recursos da AWS e como fazer requisições a uma API pelo Python.

Este artigo tutorial é auxiliar ao vídeo gravado pelo trainee Guilherme Ferreira Rocha, que está no Youtube do TerraLAB. Veja aqui:

Esta é a parte 4 deste tutorial, então aconselhamos que, caso ainda não esteja acompanhando desde a primeira parte, volte para entender exatamente o que estamos arquitetando. O repositório HandsOn AWS-TerraLAB contém todo o código utilizado nos tutoriais.

Dando continuidade
LAMBDA 2:

Nossa segunda lambda, irá receber as informações do pokémon pelo trigger conectado a nossa fila MQ. Para iniciar, crie um arquivo chamado lambda2.py no seu diretório do serverless. Ela manterá o padrão inicial de parâmetros e retornos que comentamos na lambda 1.

O primeiro passo é recuperar a informação do pokémon que estará no event. Para recuperar, vamos acessar o dicionário, na chave “messages” na posição zero e em sequência na chave “data”. Esses dados precisam ser decodificados pela biblioteca base64, nativa do python e depois convertidos para dicionário python. Adicione a seus imports:

import base64

Feito isso, o código de recuperação das informações ficará como o seguinte:

        message = event[""messages""][0][""data""]
        message = base64.b64decode(message).decode(""utf-8"")
        pokemon = json.loads(message)

Agora, para finalizar o fluxo da arquitetura proposta, precisamos enviar tais informações ao S3 Bucket, para isso precisamos que elas estejam em formato de arquivo json. Vamos então escrever esse arquivo em uma pasta temporária da lambda:

        temp = f""/tmp/{pokemon['name']}.json""
        file = open(temp, ""w"")
        json.dump(pokemon, file, indent=4)
        file.close()

Para fazer o upload do arquivo, utilizaremos novamente a biblioteca boto3 (adicione ela a seus imports). Um dos critérios estabelecidos na nossa arquitetura, era particionar o S3 por tipos de pokémon, ou seja, cada pokémon estará contido em uma pasta com o nome do seu tipo, por exemplo “eletric”. O código de upload será o seguinte:

        client = boto3.client(""s3"")
        client.upload_file(
            temp,""bucket-pokemon-tutorial-v2"",     f""{pokemon['types'][0]['type']['name']}/{pokemon['name']}.json"",
        )


Pronto, finalizamos a lambda 2. Alterando a mensagem de retorno e dando alguns toques informativos, seu código deverá ficar assim:

import base64
import json
import boto3
 
 
def main(event, context):
    try:
        print(""Getting consumed message."")
        message = event[""messages""][0][""data""]
 
        print(""Decoding message."")
        message = base64.b64decode(message).decode(""utf-8"")
 
        print(""Converting to dictionary (like a json)"")
        pokemon = json.loads(message)
 
        temp = f""/tmp/{pokemon['name']}.json""
        file = open(temp, ""w"")
        json.dump(pokemon, file, indent=4)
        file.close()
 
        print(""Connecting to s3."")
        client = boto3.client(""s3"")
        print(
            f""Sending {pokemon['name']} json to s3 partition {pokemon['types'][0]['type']['name']}""
        )
        client.upload_file(
            temp,
            ""bucket-pokemon-tutorial-v2"",
            f""{pokemon['types'][0]['type']['name']}/{pokemon['name']}.json"",
        )
        print(""Function is over."")
 
        body = {
            ""message"": f""Pokémon {pokemon['name']} sent to s3 partition {pokemon['types'][0]['type']['name']}"",
            ""input"": event,
        }
        response = {""statusCode"": 200, ""body"": json.dumps(body)}
        print(f""Lambda return = {body}"")
    except Exception as error:
        body = {
            ""message"": f""Error: {error}"",
            ""input"": event,
        }
        response = {""statusCode"": 400, ""body"": json.dumps(body)}
        print(f""Lambda return = {body}"")
    return response


Assim como lambda 1, para execução da lambda 2 também é necessário o arquivo “requirements.txt”. Nesse caso, usaremos o mesmo, sem modificações. Basta digitar o seguinte comando:

serverless plugin install -n serverless-python-requirements


Feito tudo isso, agora é só realizar o deploy, novamente abra o terminal no diretório do seu serverless e rode o comando:

serverless deploy

Pronto, nossa arquitetura está completa!

POPULANDO NOSSO BUCKET:

Para popular nosso S3 Bucket com alguns pokémons, vamos criar um pequeno script para ir realizando chamadas para a nossa arquitetura. Crie um outro diretório separado para seu script e adicione dois arquivos, o “poke-names.txt” e o “start.py”. 

O arquivo texto irá conter nomes de pokémons a cada linha, você pode acessar o repositório no github para copiar esse arquivo. 

Já nosso código de início, irá utilizar a biblioteca requests para fazer requisições para a lambda 1 para cada nome no arquivo txt.

Uma informação importante que precisamos é a URL da lambda 1. Para recuperar sua URL vá no console da AWS > pesquise por Lambda > vá em funções no painel lateral esquerdo > tutorial-pokemon-dev-lambda1-pokemon > Configuração > Gatilhos > E aqui você terá visibilidade do Endpoint de API Gateway, ou seja, a URL que necessitamos.

Com a URL em mãos, o código “start.py” ficará da seguinte forma (lembre de alterar a URL para a da sua lambda)

import json
 
import requests
 
 
def call_lambda(pokemon: str):
    params = {""pokemon"": pokemon}
    response = requests.get(
      ""https://t6gv53ghih.execute-api.us-east-1.amazonaws.com/dev/lambda1/pokemon"",
        params=params,
    ) # coloque a SUA url (endpoint) 
    print(
        f'\n============\nSTATUS CODE: {response.status_code}\nREASON: {response.reason}\nRESPONSE: {json.loads(response.text)[""message""]}\n'
    )
 
 
if __name__ == ""__main__"":
    with open(""poke-names.txt"", ""r"") as file:
        pokemons = [pokemon.rstrip(""\n"") for pokemon in file]
    print(pokemons)
 
    for pokemon in pokemons:
        call_lambda(pokemon)


Rode seu código no terminal e ao finalizar, você terá seu S3 populado com os pokémons que foram enviados.

Conclusão

Finalizamos aqui o tutorial de como criar uma arquitetura na AWS. Passamos por vários serviços da plataforma e com isso você recebeu um conhecimento básico para ser capaz de estruturar um problema na nuvem. A grande dica a partir de agora é pensar em problemas, algoritmos, trabalhos que você já teve que realizar e pensar em como você poderia utilizar dos serviços da AWS para arquitetar uma solução em nuvem para eles.

Se você se interessou pelo assunto, temos mais sobre o tema em nosso blog. 

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Siga-nos nas redes sociais para saber mais!

Este artigo foi escrito por Guilherme Ferreira Rocha, revisado por Prof. Rodrigo Silva."
Hands-On Parte 3: Infraestrutura na AWS – Implementando funções lambda na AWS,http://www2.decom.ufop.br/terralab/hands-on-parte-3-infraestrutura-na-aws-implementando-funcoes-lambda-na-aws/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2022/02/Design-sem-nome-730x350.png,"Hoje continuaremos nosso hands-on de uma arquitetura na AWS. Cada lambda irá receber um código em python para realizar suas tarefas, dessa forma, o conhecimento será direcionado a como acessar recursos da AWS e como fazer requisições a uma API pelo Python.

Este artigo tutorial é auxiliar ao vídeo gravado pelo trainee Guilherme Ferreira Rocha, que está no Youtube do TerraLAB. Veja aqui:

Esta é a parte 3 deste tutorial, então aconselhamos que, caso ainda não esteja acompanhando desde a primeira parte, volte para entender exatamente o que estamos arquitetando. O repositório HandsOn AWS-TerraLAB contém todo o código utilizado nos tutoriais.

Dando continuidade
LAMBDA 1:

Iniciando com a codificação da primeira lambda, crie um arquivo chamado lambda1.py no diretório do seu serverless. As funções lambdas têm um padrão de definição da função e de seu retorno. Na definição precisamos receber um evento e um contexto, onde, o evento irá conter as informações vindas do trigger e o contexto fornece informações sobre o ambiente de execução da lambda. Para o retorno da função precisamos de um dicionário com um código de status http e um corpo de mensagem. Para nossa lambda definiremos o retorno de sucesso e o retorno de falha utilizando um try except. Esse código padrão deverá ficar assim para nossa arquitetura:

    import json
 
    def main(event, context):
    try:
	  ######
        # código será inserido aqui
        ######
        body = {
            ""message"": f"""",
            ""input"": event,
        }
        response = {""statusCode"": 200, ""body"": json.dumps(body)}
        print(f""Lambda return = {body}"")
    except Exception as error:
        body = {
            ""message"": f""Error: {error}"",
            ""input"": event,
        }
        response = {""statusCode"": 400, ""body"": json.dumps(body)}
        print(f""Lambda return = {body}"")
    return response

A lambda 1 possui um gatilho http, o qual irá conter o nome do pokémon que queremos obter informações. Para recuperar esse nome, vamos acessar o dicionário event, na chave “queryStringParameters” e em sequência na chave “pokemon” como na linha a seguir:

pokemon = event[""queryStringParameters""][""pokemon""]


Para realizarmos uma requisição à API PokéAPI utilizaremos a biblioteca requests do python. Adicione ela seus imports:

import requests


Continuando no código, faremos um request à API com o método GET para o nome do pokémon desejado, e depois converteremos a resposta em um dicionário python, desta forma:

response = requests.get(f""https://pokeapi.co/api/v2/pokemon/{pokemon}"")
responseDict = json.loads(response.text)


O próximo passo seria enviar essa informação recebida da API para a fila MQ, mas antes, precisaremos recuperar as credenciais de acesso ao MQ que estão salvas no Secrets Manager. Para fazer isso precisaremos da biblioteca boto3, a qual é capaz de se conectar com os serviços da AWS. Adicione ela a seus imports:

import boto3

Seguindo os códigos de exemplo da própria AWS, iremos estabelecer uma conexão com o serviço, e tentar recuperar os valores do segredo. Caso essa operação seja bem sucedida, converteremos esses valores em um dicionário python, como no código a seguir:

    client = boto3.client(""secretsmanager"")
    secret_name = ""tutorial/mq/users""
    try:
        secret_value = client.get_secret_value(SecretId=secret_name)
    except Exception as e:
        raise e
    else:
        secretDict = json.loads(secret_value[""SecretString""])
        username = secretDict[""username""]
        password = secretDict[""password""]


Agora, com as credenciais em mãos, precisamos de mais duas informações para criar a conexão com a fila, o IP do broker e a porta que faremos a comunicação. Para isso, utilizaremos também a biblioteca boto3 para recuperar o IP, e a porta iremos utilizar a 61614 do protocolo STOMP de mensagens. O código ficará desta forma:

client = boto3.client(""mq"")
  brokerInfo = client.describe_broker(BrokerId=""fila-pokemon"")
  ipport = (brokerInfo[""BrokerInstances""][0][""IpAddress""], ""61614"")


Finalmente, iremos enviar as informações para a fila. Aqui estaremos utilizando a biblioteca stomp.py para realizar a comunicação. Adicione ela a seus imports:

import stomp

Aqui, precisamos estabelecer uma conexão com a fila, informar qual protocolo de segurança será utilizado e por fim, enviar a mensagem em formato json, iremos também adicionar um tempo de espera pequeno para garantir o sucesso do envio. O protocolo de segurança iremos adquirir pelo pacote ssl nativo do python. Adicione ssl e time a seus imports.

import ssl
import time

O envio para fila deve ficar desta forma:

conn = stomp.Connection([ipport])
        conn.set_ssl(for_hosts=[ipport], ssl_version=ssl.PROTOCOL_TLS)
        conn.connect(username, password, wait=True)
        conn.send(
            body=json.dumps(responseDict),
            destination=""save-pokemon"",
        )
        time.sleep(0.8)


Pronto, nossa lambda 1 está pronta, falta apenas mudarmos a mensagem de retorno para algo que faça sentido. O código completo deverá estar assim (com alguns prints adicionais informativos):

import json
import ssl
import time
 
import boto3
import requests
import stomp
 
 
def main(event, context):
    try:
        print(""Getting query params"")
        pokemon = event[""queryStringParameters""][""pokemon""]
 
        print(f""Requesting {pokemon} to PokeAPI."")
        response = requests.get(f""https://pokeapi.co/api/v2/pokemon/{pokemon}"")
        responseDict = json.loads(response.text)
        print(f""Name = {responseDict['name']}, Types = {responseDict['types']}"")
 
        print(""Connecting to Secrets Manager."")
        client = boto3.client(""secretsmanager"")
        secret_name = ""tutorial/mq/users""
        try:
            print(""Getting secrets values"")
            secret_value = client.get_secret_value(SecretId=secret_name)
        except Exception as e:
            raise e
        else:
            secretDict = json.loads(secret_value[""SecretString""])
            username = secretDict[""username""]
            password = secretDict[""password""]
 
            print(""Connecting to MQ."")
            client = boto3.client(""mq"")
            print(""Getting broker IP"")
            brokerInfo = client.describe_broker(BrokerId=""fila-pokemon"")
            ipport = (brokerInfo[""BrokerInstances""][0][""IpAddress""], ""61614"")
            print(f""ip/port = {ipport}"")
 
            conn = stomp.Connection([ipport])
            conn.set_ssl(for_hosts=[ipport], ssl_version=ssl.PROTOCOL_TLS)
            conn.connect(username, password, wait=True)
            conn.send(
                body=json.dumps(responseDict),
                destination=""save-pokemon"",
            )
            time.sleep(0.8)
            print(""Function is over."")
 
        body = {
            ""message"": f""Pokémon {responseDict['name']} sent to queue"",
            ""input"": event,
        }
        response = {""statusCode"": 200, ""body"": json.dumps(body)}
        print(f""Lambda return = {body}"")
    except Exception as error:
        body = {
            ""message"": f""Error: {error}"",
            ""input"": event,
        }
        response = {""statusCode"": 400, ""body"": json.dumps(body)}
        print(f""Lambda return = {body}"")
    return response


Antes de realizarmos o deploy do nosso serverless, precisamos configurar os requisitos de bibliotecas do python que as lambdas terão necessidade de acessar. Crie o arquivo “requirements.txt” no diretório do seu serverless e escreva em cada linha dele as bibliotecas que precisamos, como a seguir:

boto3
requests
stomp.py


Agora, abra o terminal (no diretório do serverless) e digite o seguinte comando:

serverless plugin install -n serverless-python-requirements

Com esse comando, o campo de plugin é adicionado ao arquivo “serverless.yml“ e para finalizar nossa configuração, adicionaremos ainda ao final do arquivo “serverless.yml“ o nome do executável python do seu computador. Por exemplo, se para compilar um código em python você utiliza “python3 seuprograma.py”, você deverá informa “python3” na configuração seguinte:

custom:
  pythonRequirements:
    pythonBin: python3


Feito tudo isso, agora é só realizar o deploy, novamente abra o terminal no diretório do seu serverless e rode o comando:

serverless deploy


Se tudo correr como o esperado, aparecerá um link após a execução da lambda com o qual você pode testar se ela está funcionando como deveria. Basta copiar o link para seu navegador de preferência e adicionar  ?pokemon=<pokemon-escolhido> ao fim da url e clicar enter. Um exemplo, se você escolheu o pokémon pikachu, vai ficar ?pokemon=pikachu e irá aparecer os prints informativos correspondentes ao pokémon pikachu. 

Conclusão

Aprendemos nesse tutorial a configurar uma função lambda para solicitar informações a uma API por meio do python e suas bibliotecas. Além disso, vimos como decodificar a informação recebida para que ela possa ser utilizada. Por fim, aprendemos como colocar essa informação na fila, bem como conectar com ela.

No próximo tutorial, finalizaremos a arquitetura construída. 

Se você se interessou pelo assunto, temos mais sobre o tema em nosso blog. 

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Siga-nos nas redes sociais para saber mais!

Este artigo foi escrito por Guilherme Ferreira Rocha, revisado por Prof. Rodrigo Silva."
Resultados do TerraLAB no mês de Dezembro e Janeiro,http://www2.decom.ufop.br/terralab/resultados-do-terralab-no-mes-de-dezembro-e-janeiro/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2022/02/Banner-artigo-de-resultados-1.png,"Esta é uma sequência de artigos que tem como objetivo mostrar a evolução dos integrantes do TerraLAB ao longo de cada mês. No artigo anterior mostramos parte do desenvolvimento dos dois projetos que estão em andamento no TerraLAB envolvendo as nossas equipes de UX, Engenharia, Análise de Dados, Infraestrutura, Gestão de Projetos e Marketing, são eles: o Serviço de Geocodificação em Massa (SGM) e o BatCaverna. 

SGM:  Na área de logística e de pesquisa de mercado, a geocodificação criou certas facilidades, que até então não existiam, e proporcionou uma transformação nos métodos de planejamento estratégico, organizacional e distributivo de mercadorias. Um serviço de geocodificação em massa na nuvem, deve ser desenvolvido pela grande aplicabilidade em diversas empresas de diferentes ramos, pois garante perspectiva de crescimento logístico, financeiro e tecnológico. Além disso, na grande maioria das vezes, dados sobre os sistemas sociais são vinculados a um endereço de logradouro ao invés de a uma coordenada geográfica. Este é comumente os casos dos eventos de saúde ou criminais. Consequentemente, o serviço de geocodificação é útil para a espacialização destes eventos e para o entendimento das maioria dos sistemas sociais. Com essas informações o projeto AWS SGM: Crawler de Geocodificação, busca trabalhar no problema: Como conhecer a localização de um logradouro, sem ir a campo?
BatCaverna: O aplicativo tem como objetivo fazer a coleta detalhada do patrimônio espeleológico da área delimitada no estado de Minas Gerais e busca contribuir para o incremento do banco de dados das cavernas presentes. A principal ferramenta será a de cadastro de pontos de controle ou de feições com as suas respectivas descrições e imagens associadas. Aliado a esse cadastro de pontos, o aplicativo irá coletar os caminhos feitos pelo o usuário e, durante esse caminho, ele terá a liberdade de criar quantos pontos forem necessários. Outra funcionalidade importante que está atribuída a esse aplicativo é a de visualizar os caminhamentos, tanto de uma forma individual, clicando sobre o caminho realizado, quanto de forma geral que será exibida em seu menu principal. O sistema também contará com um gerador de relatórios baseado nos pontos e fotografias tiradas em campo.

Na última semana do mês de janeiro ocorreu a validação mensal do trabalho realizado pelas equipes do TerraLAB, acompanhe nos vídeos abaixo o que foi produzido e quais são as principais ferramentas e tecnologias que estão sendo utilizadas e estudadas nesses projetos.

ENGENHARIA 

O nosso gerente de Engenharia de Software, Higor Duarte de Oliveira, apresenta no vídeo a seguir a implementação de um servidor python e a utilização da biblioteca PyQGIS. Ele também fala da autenticação de rotas na API Gateway.

DATA ANALYTICS

No vídeo deste mês, o gerente de Data Analytics, Rafael Christian, apresenta os projetos desenvolvidos pela equipe de Data Analytics nos dois últimos meses. O primeiro deles foi a construção de um fluxo para fazer a extração, transformação e validação das geocodificações de endereços armazenados em um banco de dados, que está sendo gerado pelo crawler do laboratório. Além disso, a equipe trabalhou no desenvolvimento de uma API para permitir que os parceiros pudessem usar as ferramentas internas do Data Analytics para auxiliar na detecção de endereços mal geocodificados. E por último, a equipe trabalha em um Front-end para fornecer essas ferramentas de uma forma mais intuitiva.

INFRAESTRUTURA

No vídeo, Ana Luiza e Emanuel, apresentam os serviços ofertados durante o mês de Dezembro de 2021 e Janeiro de 2022 pelo time. Nesses 2 meses foram ofertados 3 serviços: Serviço Transferência de Conhecimento, Serviço de Geocodificação Serverless e Serviço de Capacitação da equipe. No Serviço de Transferência de Conhecimento foram escritos 4 artigos e vídeos da série de Hands-On  Arquitetura na AWS, sendo que 2 já foram publicados no blog do laboratório. No Serviço de Geocodificação foram desenvolvidos os testes automatizados e sua documentação, foi realizada a correção do processo de backup e documentação do mesmo, e a correção de erros apresentados no banco de dados em que eram colocados os endereços geocodificados. Por fim, no serviço de capacitação da equipe foram realizados 2 cursos de docker, sendo um na plataforma Alura e outro na plataforma Udemy.

USER EXPERIENCE

A seguir, o gerente de UX, Victor Hugo nos mostra o que o seu time realizou. Finalizaram as primeiras telas para ser apresentadas ao cliente como forma de mensurar o projeto, assim como as suas funcionalidades. Também foram criados os critérios de aceitação que serão direcionados aos desenvolvedores para não haver dúvidas em relação ao funcionamento de cada tela. Quanto ao projeto BatCaverna foi feita a marcação do caminhamento em tempo real e a realização do serviço de exportação do formulário em xml. 

MARKETING

Enfim, o gerente de marketing, Luka Menin, nos mostra os serviços prestados por sua equipe neste mês, em âmbito Operacional – postagens semanais nas redes sociais, artigos escritos por membros do Lab, Pesquisas e Relatórios voltadas para o processo seletivo anteriores, Ferramentas voltadas para o Blog e análise de resultados, a nova configuração do Google Analytics, e divulgação e início ao Processo Seletivo 2022.1.

Nós estamos há nove meses trabalhando nos projetos aqui citados. Utilizamos o primeiro mês para planejamento e esta é, portanto, nossa oitava entrega. Caso você represente uma empresa ou seja um/uma profissional experiente, gostaríamos do seu feedback para que possamos melhorar continuamente o nosso trabalho

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. 

O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab! 

Artigo escrito por Iasmin Salvador. Revisado por Prof. Rodrigo Silva."
Hands-On Parte 2: Infraestrutura na AWS – Subindo funções e recursos com o Serverless Framework,http://www2.decom.ufop.br/terralab/hands-on-parte-2-infraestrutura-na-aws-subindo-funcoes-e-recursos-com-o-serverless-framework/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2022/01/Hands-On-Parte-2-Infraestrutura-730x350.png,"Dando continuidade à nossa arquitetura, hoje iremos utilizar o Serverless Framework para fazer o deploy de nossas funções lambda e de um bucket de armazenamento, o S3, e também como criar gatilhos para despertar as funções.

Este artigo tutorial é auxiliar ao vídeo gravado pelo trainee Guilherme Ferreira Rocha, que está no Youtube do TerraLAB. Veja aqui:

Esta é a parte 2 deste tutorial, então aconselhamos que, caso ainda não esteja acompanhando desde a primeira parte, volte para entender exatamente o que estamos arquitetando. O repositório HandsOn AWS-TerraLAB contém todo o código utilizado nos tutoriais.

Retomando o Projeto

A principal parte de nossa arquitetura, são as funções lambdas, as quais são responsáveis por adquirir as informações sobre os pokémons, tal como armazenar essas informações da maneira que desejamos. Para invocarmos uma função a ser executada, o mais comum é a utilização de gatilhos, provenientes de serviços da própria provedora, em nosso caso, a AWS. Neste tutorial utilizaremos dois tipos de gatilho, sendo eles uma rota http, gerenciada pelo AWS API Gateway, e via mensagens na fila Amazon MQ. Outro serviço que faremos o deploy, é o S3 (Simple Storage Service), o qual irá armazenar as informações particionadas por tipo dos pokémons.

Para continuar o tutorial, você precisa ter o Serverless Framework instalado, neste artigo temos os passos para realizar a instalação. 

Para iniciar, abra o terminal no diretório que irá conter seu código serverless e digite o seguinte comando, para gerar um template básico em python 3 e aws:

serverless create --template aws-python3

Para mais detalhes sobre comandos do serverless, você pode encontrar na documentação do framework.

Feito isso você deverá obter esses dois arquivos principais:

|_handler.py

|_serverless.yml

Para esse tutorial, estaremos mexendo no arquivo serverless.yml para configurar todo o nosso deploy das funções lambda. A princípio, a configuração terá 3 partes principais: a do provedor, a das funções e a dos recursos. 

No provedor iremos especificar qual o runtime das nossa funções, o quanto de memória será alocadas para elas, a região dos serviços e algumas permissões, como no código a seguir (no vídeo, cada campo da configuração é mais detalhado e explicado).

service: tutorial-pokemon
 
frameworkVersion: '2'
 
provider:
 name: aws
 runtime: python3.8
 lambdaHashingVersion: 20201221
 region: us-east-1
 memorySize: 128
 iam:
   role:
     statements:
       - Effect: Allow
         Action:
           - s3:PutObject
           - mq:DescribeBroker
           - secretsmanager:GetSecretValue
           - kms:Decrypt
         Resource:
           - ""*""

Nas funções, temos que indicar qual arquivo e sua respectiva função será atrelada a Lambda assim como os seus respectivos gatilhos, caso existentes. Infelizmente um dos gatilhos que precisamos utilizar (gatilho MQ) ainda não tem suporte de configuração via serverless framework, será necessário posteriormente criá-lo manualmente pela AWS.

A seguir temos a configuração da lambda 1 e seu gatilho, e da lambda da 2 sem o gatilho definido.

functions:
 lambda1-pokemon:
   handler: lambda1.main
   events:
     - http:
         path: lambda1/pokemon
         method: GET
 lambda2-pokemon:
   handler: lambda2.main


E por último, os recursos necessários, como o S3 Bucket na configuração seguinte.

resources:
 Resources:
   BucketUpload:
     Type: AWS::S3::Bucket
     Properties:
       BucketName: bucket-pokemon-tutorial-v2


Dessa forma, o arquivo serverless.yml ficará assim:

service: tutorial-pokemon
 
frameworkVersion: '2'
 
provider:
 name: aws
 runtime: python3.8
 lambdaHashingVersion: 20201221
 region: us-east-1
 memorySize: 128
 iam:
   role:
     statements:
       - Effect: Allow
         Action:
           - s3:PutObject
           - mq:DescribeBroker
           - secretsmanager:GetSecretValue
           - kms:Decrypt
         Resource:
           - ""*""
 
 
functions:
 lambda1-pokemon:
   handler: lambda1.main
   events:
     - http:
         path: lambda1/pokemon
         method: GET
 lambda2-pokemon:
   handler: lambda2.main
 
resources:
 Resources:
   BucketUpload:
     Type: AWS::S3::Bucket
     Properties:
       BucketName: bucket-pokemon-tutorial-v2

Estamos aptos a fazer o deploy desses serviços, para isso usaremos o seguinte comando no terminal, no diretório do nosso arquivo serverless.yml :

serverless deploy

Após o sucesso do deploy, iremos nos direcionar ao console da AWS com o intuito de criar o gatilho da lambda 2, que ficou pendente. Para acessar a lambda 2 siga o seguinte caminho: digite lambda na pesquisa > vá até funções na barra lateral > e entre em tutorial-pokemon-dev-lambda2-pokemon

Clique em adicionar gatilho > MQ e preencha as informações da seguinte forma como no gif a seguir:

Agente do Amazon MQ: fila-pokemon

Tamanho do lote: 1

Nome da fila: save-pokemon

Segredo de acesso de origem: tutorial/mq/users

Pronto, nossa configuração do serverless está completa e pronta para receber nossos códigos das funções.

Conclusão

O serverless framework foi responsável pelo provisionamento de nossas funções e o seu S3 bucket de armazenamento. Com o arquivo serverless.yml criado, somos capazes de realizar rapidamente o deploy de múltiplas funções a cada mudança necessária, facilitando a vida do programador que poderá focar seus esforços apenas no código a partir de agora.

Se você se interessou pelo assunto, temos mais sobre o tema em nosso blog, e fique ligado para os próximos passos desse tutorial. 

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Siga-nos nas redes sociais para saber mais!

Este artigo foi escrito por Guilherme Ferreira Rocha, revisado por Prof. Rodrigo Silva."
Explicando as áreas de atuação do TerraLAB,http://www2.decom.ufop.br/terralab/explicando-as-areas-de-atuacao-do-terralab/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2022/01/1-730x350.png,"Este é um breve artigo voltado aos participantes de processos seletivos do TerraLAB para esclarecer o que é cada área de atuação dentro do laboratório.

Ao fim de cada seção serão listadas as funções referentes a cada área no formulário de inscrição do processo seletivo

Infraestrutura:

A equipe de infraestrutura do TerraLAB é responsável por prover e gerenciar os ambientes onde as aplicações do laboratório se encontram. Isso significa garantir que todas as aplicações do laboratório estejam no ambiente correto e em execução, além de garantir a segurança do mesmo. Para isso, é necessário o aprendizado em gerenciamento de nuvem, containers, máquinas virtuais, segurança de rede, dentre outras coisas. No laboratório utilizamos o docker para a utilização de containers e AWS como plataforma de computação em nuvem. 

Além disso, a equipe de infraestrutura tem como objetivo formar um profissional especializado em SRE – Site Reliability Engineering (Engenharia de Confiabilidade de Sites). Esse tipo de profissional é responsável pela implantação e monitoramento de uma aplicação, garantindo disponibilidade, segurança e automação.

Posições no TerraLAB:

Analista de Sistema (Levantamento e Análise de Requisito de Software)
Engenheiro DevOps

Teto Salarial (Mensal) no mercado brasileiro:

Analista de Sistema: R$ 9.217
Engenheiro DevOps: R$ 11.953
Gerente de Infraestrutura: R$ 22.000
Engenharia de Software:

A área de engenharia de software é composta pelo Back-end e pelo Front-end. O propósito da área de backend é a de projetar e construir aplicações com o intuito de fornecer serviços utilizados por um frontend, tais como, fornecimento de dados oriundo de um banco de dados, fornecimento de uma página Web, etc . O frontend é a interface gráfica que se comunica com o backend a fim de apresentar dados fornecidos pelo mesmo e com a qual o usuário final de um produto interage.

Back-end:

Uma das tecnologias mais utilizadas pelo mercado atualmente para a construção de aplicações backend é o software Node.JS também utilizado no Terralab e que se destaca pela execução de código escrito na linguagem de programação Javascript no lado do servidor.

Front-end:

O exercício de desenvolvimento de Frontend é a de projetar e construir interfaces gráficas com a qual usuários interagem. Essa área utiliza diversos recursos e conjuntos de ferramentas, conhecidos como frameworks, para alcançar seu objetivo. São diversas as tecnologias que compõem esse ecossistema.O TerraLAB trabalha especificamente com Javascript e Typescript, associando essas linguagens ao framework React, uma poderosa e elegante ferramenta capaz de produzir diversas aplicações web com qualidade e consistência.

Posições no TerraLAB:

Engenheiro de Teste (Aplicativo Móvel)
Engenheiro de Teste (Serviço WEB)
Engenheiro de Software (Frontend WEB)
Engenheiro de Software (Frontend Movel)
Engenheiro de Software (Backend)

Teto Salarial (Mensal) no mercado brasileiro:

Engenheiro de Software: R$ 22.259
Engenheiro de Teste: R$ 9.819
Desenvolvedor Fullstack: R$ 10.592
Desenvolvedor Frontend: R$ 4.000
Desenvolvedor Backend:  R$ 7.621
Data&Analytics:

A equipe de Data Analytics (D&A) é responsável por produzir análises nos dados gerados da empresa com o intuito de obter insights significativos que irão ajudar na tomada de decisão do time de negócios. Dentro da equipe de Data Analytics geralmente existem dois tipos de profissionais: Primeiro, temos o cientista de dados que é o profissional responsável por realizar análises exploratórias e desenvolver modelos estatísticos para servir de apoio para as áreas de negócio através de construções de relatórios e dashboards úteis. Esses profissionais irão trabalhar em colaboração com os engenheiros de dados. Esses são o segundo tipo de profissionais presentes na equipe de Data Analytics. Os engenheiros de dados são responsáveis por criar pipelines para extrair dados de diversas fontes, transformá-los e  armazená-los de uma maneira que os cientistas de dados possam utilizar para realizar as análises. 

No TerraLab atualmente a  equipe de Data Analytics atua com o desenvolvimento de crawler, fluxos de ETLs e análises de dados  que auxiliam todas as outras equipes do laboratório. Dessa forma, espera-se que os integrantes  sejam capazes de atuar nas duas áreas da equipe de Data Analytics, realizando todo o fluxo de ETL, a análises de dados e  a criação de modelos de aprendizado de máquina, podendo assim, auxiliar nas demandas de projetos desenvolvidos por outros times do laboratório, além de produzir ferramentas internas que podem auxiliar os clientes.

Posições no TerraLAB:

Analista de Geoprocessamento (Análise de dados espaciais e desenvolvimento/uso de ferramentas SIG )
Cientista de Dados

Teto Salarial (Mensal)no mercado brasileiro:

Engenheiro de Dados: R$ 12.290 
Cientista de Dados: R$ 12.696
User Experience:

A “User Experience”, mais conhecida como “Experiência do Usuário”, ou simplesmente “UX” é toda relação que uma pessoa tem com um produto ou serviço. Está diretamente ligada às sensações que o usuário vivencia ao utilizar um  produto que, além de resolver seu problema, deve oferecer uma experiência satisfatória.

Os processos de UX consistem em briefing, investigação, benchmarking, desenvolvimento de personas, prototipagem, testes de usabilidade e sua implementação. Utilizamos como principal ferramenta de prototipagem o Figma.

Posições no TerraLAB:

Design de UX (projetista de experiência do usuário)

Teto Salarial (Mensal) no mercado brasileiro:

User Experience Designer: R$ 10.449 
User Experience Research: R$ 10.396
Marketing:

O setor de Marketing é o responsável pela face pública do TerraLAB, por desenvolver a imagem/sentimento que os colaboradores, parceiros e clientes terão do TerraLAB e de seus produtos/serviços. Todos os projetos e produtos de software produzidos pelos diversos setores do laboratório passam pelo marketing eventualmente. Através de publicações nas redes sociais, artigos em nosso blog, palestras e workshops nós apresentamos ao público, colaboradores e às empresas de tecnologia parceiras do TerraLAB os conhecimentos e habilidades adquiridas por nossos membros em suas tarefas no Lab.

A equipe de marketing do TerraLAB atualmente atua utilizando um leque de redes sociais para representar a marca do Lab, como o Instagram, LinkedIn e Twitter, com auxílio da ferramenta Canva para a produção de conteúdo gráfico, mas a face principal do projeto é o nosso Blog em WordPress. Aprendemos também a utilizar de outras ferramentas essenciais para profissionais de marketing, como o Google Analytics para análise de desempenho de nossas redes. E tudo deve ser realizado pensando sempre nas perspectivas de três atores essenciais à existência do laboratório, seus colaboradores, seus clientes e seus parceiros.

Posições no TerraLAB:

Analista de Marketing e Comunicação
Jornalismo (Desenvolvimentos de peças gráficas e textuais)

Teto Salarial (Mensal) no mercado brasileiro:

Analista de Marketing: R$ 9.249
Especialista de Marketing: R$ 13.348
Equipe Negocial:

O Product Owner (PO) tem um papel importante dentro da metodologia ágil denominada como Scrum. Ele fica responsável por representar o cliente dentro do time, o qual está desenvolvendo o produto que foi solicitado. Dessa forma, sua função envolve desde a manutenção da estratégia de negócios até o design do produto, agregando valor para o negócio juntamente com o público.

Posições no TerraLAB:

Analista de Negócio (Modelamento de negócios de clientes e parceiros)
Gerente de Projetos (Deve estudar as diretivas do PMI)
Administrador (gestão de RH, aquisições, parcerias empresariais, contabilidade, etc)

Teto Salarial (Mensal) no mercado brasileiro:

Analista de negócio: R$ 10.000
Gerente de Projetos: R$ 17.770
Administrador: R$ 19.898
Palavras finais

Gostaria de saber mais como funciona o Laboratório de Software TerraLAB? Veja então este artigo sobre a Como o TerraLAB e suas equipes são estruturados. 

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. 

O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab! 

Artigo escrito por Luka Menin. Revisado por Prof. Rodrigo Cesar.

Referências:

https://www.guiadacarreira.com.br/salarios/quanto-ganha-um-profissional-de-ti/

https://www.educamaisbrasil.com.br/cursos-e-faculdades/engenharia-de-software/salario-de-engenheiro-de-software-carreira

https://www.roberthalf.com/blog/salaries-and-skills/the-13-highest-paying-it-jobs-in-2019

Essential Guide to the Highest Paying IT Careers

https://www.bls.gov/ooh/computer-and-information-technology/home.htm

Salário.com.br – Pesquise Salários, Mercado de Trabalho e Média Salarial"
Hands-On Parte 1: Infraestrutura na AWS – Subindo uma fila com o Terraform,http://www2.decom.ufop.br/terralab/hands-on-parte-1-infraestrutura-na-aws-subindo-uma-fila-com-o-terraform/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2022/01/Design-sem-nome-730x350.png,"Hoje daremos início a um tutorial de mão na massa onde iremos utilizar diversos serviços da AWS conectados em uma arquitetura. Com essa atividade você terá uma base em IaC (Infrastructure as code) com Serverless e Terraform, diversos serviços oferecidos pela plataforma, e também como utilizá-los em um código python.

Para começar a nossa arquitetura, utilizaremos o Terraform para subir a fila e os serviços necessários para seu funcionamento, ou seja, estaremos utilizando o conceito de Infrastructure as Code (IaC), que é um modo de provisionar a infraestrutura de forma automatizada.

Este artigo tutorial é auxiliar ao vídeo gravado pelo trainee Guilherme Ferreira Rocha, que está no Youtube do TerraLAB. Veja aqui:

Esta é a parte 1 deste tutorial, então aconselhamos que, continue acompanhando, para entender o que estamos arquitetando por completo. O repositório HandsOn AWS-TerraLAB contém todo o código utilizado nos tutoriais.

Um pouco mais sobre o projeto

A arquitetura que iremos implementar como exemplo, resumidamente, irá armazenar Pokémons de acordo com o tipo dele. O local de armazenamento, será um S3 Bucket da AWS, que é um serviço de armazenamento de objetos da Amazon. Para obter os dados de um pokémon, iremos utilizar a API PokéAPI.

Para que esse fluxo resumido da arquitetura aconteça, teremos funções lambda da AWS, sendo a primeira a que fará a requisição à API do pokémon e a segunda que irá guardar esses dados no S3 de forma particionada pelo tipo dele. A comunicação entre essas duas lambdas será feita por um fila de mensagens Amazon MQ.

imagem-1-arquitetura.png


Breve descrição dos serviços e ferramentas:

Serverless Framework: Com ele conseguimos subir nossas funções lambdas como IaC e outros recursos “sem servidor”  associados a lambda.
Terraform: também é uma solução IaC que irá provisionar a infraestrutura e subir recursos com ou sem servidor, no nosso caso que não sejam necessariamente dependentes da existência das lambdas.
Lambda: é um serviço de computação sem servidor que permite executar código sem provisionar ou gerenciar servidores.
S3: é um serviço de armazenamento de objetos que oferece escalabilidade, disponibilidade de dados, segurança e performance.
MQ: O Amazon MQ é um serviço gerenciado de agente de mensagens para o Apache ActiveMQ e RabbitMQ que facilita a configuração e a operação de agentes de mensagens na AWS.
(BÔNUS) draw.io: é uma ferramenta gratuita que foi utilizada para a construção do desenho da arquitetura. Ela basicamente serve para construir diagramas, fluxos, etc. No contexto de infraestrutura ela já contém uma grande quantidade de objetos que representam diversos serviços e funcionalidades das nuvens e é sempre uma boa dica desenhar sua arquitetura antes de construí-la para te ajudar a obter uma melhor visão de seu projeto.

Outro ponto importante são os valores dos serviços envolvidos. A grande dica é sempre conferir no site da própria AWS o custo para cada serviço. Como arquiteto de infraestrutura, você deve estar sempre preocupado com o preço da sua arquitetura, para que possa tomar as melhores decisões para otimizar esse custo.

Para realizar esse tutorial uma conta com avaliação gratuita da AWS é suficiente, mas lembre-se de deletar a infraestrutura ao final para que você não receba cobranças indesejadas eventualmente.

Implementando a infraestrutura

Primeiramente, vamos listar todos os serviços que devemos provisionar para que a fila funcione da maneira que desejamos. 

Amazon MQ: Serviço de host da nossa fila.
Secrets Manager: Serviço que irá armazenar as credenciais de login da fila.
KMS: Serviço que guarda a chave de criptografia das credenciais de login.
VPC/Security Group/Rules: Aqui liberaremos as portas necessárias para comunicação com a fila.

Para iniciar, iremos armazenar as credenciais de acesso pelo console da AWS. Para isso, é só ir em Secrets Manager > Segredos > Armazenar um novo segredo e seguir os passos do gif abaixo.

gif-1-secretsmanager.gif


No exemplo do gif acima, utilizamos a chave default de criptografia do KMS, caso você queira ter uma chave apenas para esse segredo você pode criar uma (antes de criar o segredo do passo anterior) indo em Key Management Service > Chaves gerenciadas pelo cliente > Criar.

Com as credenciais criadas, podemos partir para o Terraform. Caso você ainda não tenha instalado, você pode baixar e seguir os passos de instalação neste link. Crie um arquivo com a extensão .tf que irá conter nosso código de infraestrutura, no vídeo é explicado com mais detalhes as propriedades do código. Nele, primeiramente, precisamos informar qual o provedor estaremos utilizando, que no caso será o da AWS.

provider ""aws""{
    region = ""us-east-1""
}


Agora iremos acessar as credenciais que já criamos e decodificar em uma variável local.

data ""aws_secretsmanager_secret_version"" ""mq_user"" {
    secret_id = ""tutorial/mq/users""
}
 
locals {
    credentials = jsondecode(
        data.aws_secretsmanager_secret_version.mq_user.secret_string
    )
}


Agora, para liberarmos as portas de comunicação com a fila, iremos adicionar as regras no grupo de segurança/vpc default. As portas que iremos liberar serão as: 8162, 61614 e 61617.

data ""aws_vpc"" ""default"" {
    default = true
}
 
data ""aws_security_group"" ""default"" {
    name = ""default""
    vpc_id = data.aws_vpc.default.id
}
 
resource ""aws_security_group_rule"" ""ingress_rules_web"" {
  security_group_id = data.aws_security_group.default.id
  type              = ""ingress""
 
  cidr_blocks      = [""0.0.0.0/0""]
  description      = ""web console""
 
  from_port = 8162
  to_port   = 8162
  protocol  = ""tcp""
}
 
resource ""aws_security_group_rule"" ""ingress_rules_stomp"" {
  security_group_id = data.aws_security_group.default.id
  type              = ""ingress""
 
  cidr_blocks      = [""0.0.0.0/0""]
  description      = ""stomp""
 
  from_port = 61614
  to_port   = 61614
  protocol  = ""tcp""
}
 
resource ""aws_security_group_rule"" ""ingress_rules_openwire"" {
  security_group_id = data.aws_security_group.default.id
  type              = ""ingress""
 
  cidr_blocks      = [""0.0.0.0/0""]
  description      = ""open wire""
 
  from_port = 61617
  to_port   = 61617
  protocol  = ""tcp""
}


Por último, temos a própria fila.

resource ""aws_mq_broker"" ""fila-pokemon"" {
    broker_name = ""fila-pokemon""
    engine_type = ""ActiveMQ""
    engine_version = ""5.16.2""
    host_instance_type = ""mq.t3.micro""
    publicly_accessible = true
 
    user {
        username = local.credentials.username
        password = local.credentials.password
        console_access = true
    }
}


Todas essas propriedades necessárias para o código de cada serviço você encontra na documentação do terraform. E também sobre os serviços necessários para auxiliar o funcionamento do MQ você encontra na documentação da AWS MQ.

Para fazer o deploy da arquitetura, você precisa ter configurado o AWS CLI em seu PC. Então para iniciarmos o Terraform e realizar o deploy siga os seguintes comandos no seu terminal (no diretório do seu projeto):

terraform init
terraform plan
terraform apply

Esse processo leva cerca de 15 minutos para subir uma fila.

Conclusão

Com esse tutorial, você foi introduzido ao mundo de IaC e tomou conhecimento de alguns serviços disponíveis na AWS. Esses serviços podem ser utilizados em diversos propósitos ao se planejar uma arquitetura cloud. No próximo passo iremos utilizar uma IaC mais voltada para funções e micro serviços, o Serverless Framework.

Vale ressaltar, que ao final de deste projeto, você estará introduzido no mundo de cloud e com um conhecimento básico em diversos serviços oferecidos pela AWS, desta forma, possibilitando que você, ao se deparar com um problema real, já consiga ter ideias para arquitetar uma solução cloud para esse problema.

Se você se interessou pelo assunto, temos mais sobre o tema em nosso blog, e fique ligado para os próximos passos desse tutorial.

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Siga-nos nas redes sociais para saber mais!

Este artigo foi escrito por Guilherme Ferreira Rocha, revisado por Prof. Rodrigo Silva."
Como as Máquinas Aprendem?,http://www2.decom.ufop.br/terralab/como-as-maquinas-aprendem/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/11/Como-as-maquinas-aprendem-6-730x350.png,"Édifícil encontrar, hoje em dia, qualquer pessoa que não tenha pelo menos escutado os termos inteligência artificial ou aprendizado de máquina. O número de aplicações destas técnicas é enorme e a tendência é que continue crescendo. Mas afinal, como as máquinas aprendem?

Vamos começar com um pouco de história. 

Em 1936, no artigo, On Computable Numbers [1], Alan Turing, introduzia os princípios que definem o que conhecemos hoje como computador. Possivelmente, fascinado com o número de possibilidades e o poder de resolver problemas daquele construto que acabara de criar, em 1950, Turing já discutia no artigo, Computing Machinery and Intelligence [2], se computadores seriam capazes de pensar. Esta discussão está fora do escopo deste post mas já neste artigo, Turing discute, de forma ainda um pouco superficial, procedimentos para “ensinar” computadores. 
O grande problema quando estamos tentando ensinar um computador é não entendermos em detalhe como nós mesmos aprendemos as coisas. Existe uma vasta literatura na pedagogia sobre como crianças e adultos aprendem, mas os mecanismos físicos, químicos e biológicos deste processo ainda não são conhecidos em detalhe. Ainda assim, algoritmos de aprendizado de máquina existem e funcionam razoavelmente bem. Então, como estes algoritmos funcionam?

Definindo aprendizado

Um algoritmo de aprendizado de máquina existe para cumprir um ou mais objetivos. Assim, o primeiro passo é definir uma função de utilidade, U, que envelopa todos este objetivos de forma que quanto maior U, mais próximo o algoritmo está de cumprir este objetivo. Uma vez que U é definido matematicamente, podemos definir “aprendizado” como um problema de otimização da seguinte forma:

Então temos que encontrar os parâmetros internos, p, do algoritmo de forma a maximizar a função de utilidade, U. Fazendo uma analogia, bastante imprecisa, com o nosso próprio corpo, é como se p definisse como os neurônios no nosso cérebro estão conectados e como eles se comportam dados os estímulos sensoriais, internos e externos, que o nosso corpo recebe. 

Com uma definição matemática do problema de aprendizagem, podemos utilizar técnicas de otimização, ou programação matemática, para resolver o problema. Provavelmente, a técnica mais utilizada neste tipo de problema é a técnica de subida do gradiente. 

Se você não está interessado na parte matemática, mas achou o assunto interessante, foi um prazer. Dê uma lida no artigo Computing Machinery and Intelligence [2] brilhantemente escrito por Turing e nos vemos no próximo post :D. Se está interessado, siga comigo, está quase acabando 

Aprendendo

O gradiente de uma função U, definido como ∇U,  é um vetor que aponta para direção de maior crescimento da função. Ele é composto pelas derivadas parciais da função em relação a cada uma das variáveis. Em outros termos, o gradiente define qual a taxa de variação instantânea da função em relação a cada um dos parâmetros.

Esta informação é extremamente útil, pois assim o algoritmo sabe quais parâmetros devem ser ajustados e como eles devem ser ajustados de forma a maximizar a função de utilidade ∇U.

Neste contexto, aprender significa então executar o seguinte algoritmo:

Dado um conjunto de estímulos, ξ, e um conjunto de parâmetros, p, no tempo t, pt,

Calcular o gradiente de U em pt
Atualizar os parâmetros

pt+1 = pt + α∇U(pt)

Ir  para o passo (1) até atingir algum critério de parada

Onde α é conhecido como taxa de aprendizado e define o tamanho do passo a ser dado na direção do gradiente da função de utilidade. 

Assim, a cada iteração os parâmetros são ajustados de forma que o valor da função de utilidade fique cada vez maior. Eventualmente, este algoritmo converge para um máximo de U.  Note que, em muitos casos, o objetivo deste algoritmo pode ser minimizar alguma medida de erro. Neste caso, basta seguir no sentido oposto do gradiente fazendo a atualização dos parâmetros com a equação abaixo. 

  pt+1 = pt – α∇U(pt)

Até aqui, tudo parece muito bom. Temos um modelo matemático do que se deseja “aprender” e um algoritmo que executa o “aprendizado”. No entanto, nem tudo são flores. Definir uma função de utilidade adequada pode ser complicado. Além disso, o algoritmo acima pode demorar demais para convergir e ainda ficar preso em máximos locais [3]. Outro problema comum é o chamado overfitting [4], que acontece quando o algoritmo atinge valores altos de utilidade durante o treinamento mas este comportamento não se repete durante o teste.  

Existem na literatura, várias maneiras de contornar estes problemas, mas estes são tópicos para próximos posts. 

Você sabia que este artigo foi escrito pelo time do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Siga-nos nas redes sociais para saber mais!

Este artigo foi escrito por Prof. Rodrigo Silva e Alan Santandrea, revisado por Luka Menin.

Referências: 

[1] Turing, A. M. (1936). On computable numbers, with an application to the Entscheidungsproblem. J. of Math, 58(345-363), 5.

[2] Turing, A. M. (1950). Computing Machinery and Intelligence. Mind, 59, 433–460.

[3] Local Optimization Versus Global Optimization (machinelearningmastery.com)

[4] Roelofs, R., Fridovich-Keil, S., Miller, J., Shankar, V., Hardt, M., Recht, B., & Schmidt, L. (2019, December). A meta-analysis of overfitting in machine learning. In Proceedings of the 33rd International Conference on Neural Information Processing Systems (pp. 9179-9189)."
Resultados do TerraLAB no mês de Outubro,http://www2.decom.ufop.br/terralab/resultados-do-terralab-no-mes-de-outubro/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/11/Outubro.png,"Esta é uma sequência de artigos que tem como objetivo mostrar a evolução dos integrantes do TerraLAB ao longo de cada mês. No artigo anterior mostramos parte do desenvolvimento dos três projetos que estão em andamento no TerraLAB envolvendo as nossas equipes de UX, Engenharia, Análise de Dados, Infraestrutura, Gestão de Projetos e Marketing, são eles: o Serviço de Geocodificação em Massa (SGM) e o BatCaverna. 

SGM:  Na área de logística e de pesquisa de mercado, a geocodificação criou certas facilidades, que até então não existiam, e proporcionou uma transformação nos métodos de planejamento estratégico, organizacional e distributivo de mercadorias. Um serviço de geocodificação em massa na nuvem, deve ser desenvolvido pela grande aplicabilidade em diversas empresas de diferentes ramos, pois garante perspectiva de crescimento logístico, financeiro e tecnológico. Além disso, na grande maioria das vezes, dados sobre os sistemas sociais são vinculados a um endereço de logradouro ao invés de a uma coordenada geográfica. Este é comumente os casos dos eventos de saúde ou criminais. Consequentemente, o serviço de geocodificação é útil para a espacialização destes eventos e para o entendimento das maioria dos sistemas sociais. Com essas informações o projeto AWS SGM: Crawler de Geocodificação, busca trabalhar no problema: Como conhecer a localização de um logradouro, sem ir a campo?
BatCaverna: O aplicativo tem como objetivo fazer a coleta detalhada do patrimônio espeleológico da área delimitada no estado de Minas Gerais e busca contribuir para o incremento do banco de dados das cavernas presentes. A principal ferramenta será a de cadastro de pontos de controle ou de feições com as suas respectivas descrições e imagens associadas. Aliado a esse cadastro de pontos, o aplicativo irá coletar os caminhos feitos pelo o usuário e, durante esse caminho, ele terá a liberdade de criar quantos pontos forem necessários. Outra funcionalidade importante que está atribuída a esse aplicativo é a de visualizar os caminhamentos, tanto de uma forma individual, clicando sobre o caminho realizado, quanto de forma geral que será exibida em seu menu principal. O sistema também contará com um gerador de relatórios baseado nos pontos e fotografias tiradas em campo.

Na última semana do mês de outubro ocorreu a validação mensal do trabalho realizado pelas equipes do TerraLAB, acompanhe nos vídeos abaixo o que foi produzido e quais são as principais ferramentas e tecnologias que estão sendo utilizadas e estudadas nesses projetos.

GESTÃO DE PROJETOS

O gerente de projetos, Emanuel Xavier, nos conta que no mês de Outubro as equipes estão sendo reestruturadas para ter uma melhor distribuição do trabalho e evitar sobrecarga de alguns membros. Além disso o Jira foi atualizado para ser capaz de mostrar nos dashboards uma visão do trabalho de toda a equipe durante a sprint, e ao mesmo tempo os outros dashboards informam o trabalho dos serviços de cada equipe, usados para revisões mais detalhada e reuniões

ENGENHARIA 

O nosso gerente de Engenharia de Software, Higor Duarte de Oliveira, apresenta no vídeo a seguir a realização de um experimento no Geocodificador e API Gateway, e a coleta e análise de métricas referentes ao experimento. Ele também fala de como e por que foi criado um repositório Git, contendo um setup do Blog do TerraLab feito em WordPress e configurado para executar dentro de um container Docker. Enfim se mencionam a criação de um documento, para uso interno do departamento, com dicas de leitura de tutoriais e artigos técnicos na capacitação de membros da equipe.

DATA ANALYTICS

No vídeo deste mês, o gerente de Data Analytics, Alan Santandrea, apresenta as tarefas realizadas por seu time, que ajudou a equipe de Infraestrutura com o projeto do banco de dados e fez alterações na biblioteca de Análise Geo, a fim de melhorar as funções para produção em escala dos dados. Também foram testadas e implementadas algumas ideias de tratamento de entrada nos endereços e a concepção de uma aplicação para os clientes onde é possível renderizar mapas de dispersão, se os endereços solicitados existirem em nossa base ou a geolocalização dos mesmo for passada pelo usuário.

INFRAESTRUTURA

Agora o Guilherme Carolino, gerente de Infraestrutura do TerraLAB, nos apresenta os serviços ofertados durante o mês de Outubro pelo time. São 4 serviços: Serviço de Geocodificação Serverless, Serviço de Capacitação da equipe, Serviço Transferência de Conhecimento e Operacional. No Serviço de Geocodificação foram desenvolvidos o script para consumo de API, o Banco de Dados relacional e o monitoramento de tempo via Cloudwatch. No Serviço de Capacitação foram concluídos cursos de Segurança e Kubernetes. No Serviço de Transferência de Conhecimento foram confeccionados artigos e gravações de vídeos para publicação. Por fim, no Serviço de Operação, foram realizadas as demandas internas das equipes do laboratório.

USER EXPERIENCE

No vídeo a seguir, o gerente de UX, João Pedro Siqueira nos mostra o que o seu time realizou no mês de outubro. Em relação ao projeto GeoService (GS) a equipe de UX concluiu algumas tarefas como a elaboração de telas e cenários de testes para essas telas, a análise de um documento de briefing para gerar histórias de usuário e um benchmark de aplicações e serviços semelhantes ao GS; Quanto ao projeto BatCaverna foram concluídas as telas referentes à funcionalidade de configurações da aplicação.

MARKETING

No mês de Outubro o marketing deu uma desacelerada, pois todos os membros da equipe performam também posições técnicas (onde está o foco atual de entrega de resultados do TerraLab). O gerente de marketing, Luka Menin, nos fala sobre as mudanças na operação do departamento, mostra os artigos publicados (leia eles aqui e aqui) e o que foi postado nas redes sociais do Lab, decorre sobre as pesquisas realizadas pelo time e sobre a criação de um relatório detalhando o que é o “estado da arte” para o marketing de empresas de TI. Veja o vídeo abaixo:

Nós estamos há seis meses trabalhando nos projetos aqui citados. Utilizamos o primeiro mês para planejamento e esta é, portanto, nossa quinta entrega. Caso você represente uma empresa ou seja um/uma profissional experiente, gostaríamos do seu feedback para que possamos melhorar continuamente o nosso trabalho. 

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. 

O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab! 

Artigo escrito por Luka Menin. Revisado por Prof. Rodrigo Cesar."
O que são as histórias de usuário e como escrevê-las bem?,http://www2.decom.ufop.br/terralab/o-que-sao-as-historias-de-usuario-e-como-escreve-las-bem/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/10/O-que-sao-as-historias-de-usuario-e-como-escreve-las-bem-2-730x350.png,"No desenvolvimento de software e gerenciamento de produto, uma história de usuário é uma explicação informal e geral sobre um recurso de software escrito a partir da perspectiva do usuário final ou cliente. Neste artigo mostraremos o que são, como escrevê-las e os pontos positivos de se utilizar histórias de usuário.

Introdução

Uma história de usuário é uma descrição curta, informal e em linguagem simples de alguma funcionalidade de um sistema sob o ponto de vista do usuário. Utilizadas nos métodos ágeis de desenvolvimento de software, cada história deve ter valor de negócio na visão do cliente e é uma pequena parte da funcionalidade, não necessariamente uma especificação completa, o que minimiza a necessidade de uma extensa documentação. 

O intuito deste artigo é descrever o que são as histórias de usuários e como escrevê-las, pois com histórias de usuário você dá a uma equipe de desenvolvimento o contexto e o porquê do que está sendo desenvolvido. Isso os ajuda a entender como eles estão fornecendo valor para o negócio e para manter o usuário/cliente no foco das atenções, além de aprimorar a eficiência do time.

Como escrever uma boa história de usuário?

Uma boa história deve ser:

Independente (não se sobrepor a outra, sendo possível desenvolvê-las em qualquer ordem).
Negociável (Devem ser co-criadas pelo cliente e o time de desenvolvimento, sendo aptas a responder às mudanças quando necessário).
Valiosa (Precisa entregar o valor para o cliente, sendo que inclusive os aspectos técnicos precisam ser apresentados de forma que o cliente os considere importantes).
Estimável (A equipe precisa conseguir estimá-la e para isso precisa entendê-la).
Pequena (Boas histórias tendem a serem pequenas, reduzindo incertezas e facilitando as estimativas).
Testável (Permite ser validada e para isso, utilizamos critérios de aceitação).
Critérios de aceitação

Critérios de aceitação são os itens que precisam ser atendidos para que a história do usuário seja aceita por um usuário ou cliente. 

Nos critérios existem as informações necessárias para a construção e o funcionamento do sistema, tais como regras de negócio, restrições de acesso e mensagens.

Eles facilitam o entendimento de como a funcionalidade será executada pelo usuário e eliminam ambiguidades, trazendo mais clareza aos requisitos. Além disso, delimitam fronteiras para a história do usuário.

Devem ser escritos com linguagem clara o suficiente para serem entendidos por todos e serem independentes de implementação. Critérios de aceite definem o que fazer e não como fazer.

Criando histórias de usuário

Crie histórias de usuário de forma colaborativa, elas nunca devem ser simplesmente entregues a uma equipe de desenvolvimento, e sim eles devem ser inseridos em uma conversa onde o Product Owner e a equipe devem discutir as histórias juntos. Isso permite que você capture apenas a quantidade mínima de informações, reduza a sobrecarga de trabalho e acelere a entrega.

Dessa forma, adicione critérios de aceite, pois eles complementam a narrativa e permitem a você descrever as condições que devem ser atendidas para que a história seja feita. Além de enriquecerem a história, as tornam testáveis garantindo que possam ser demonstradas ou divulgadas para os usuários e outras partes interessadas.

Uma história de usuário geralmente segue o seguinte “padrão”:

Como um <tipo de usuário>, quero <algum recurso>, para que <algum motivo>.

Como um <tipo de usuário> – Para quem estamos construindo isso? Quem é o usuário? / Papel que desempenha alguma ação no produto em teste.

Gostaria de <algum recurso> – O que estamos construindo? Qual é a intenção? / Realizar alguma ação no produto sobre teste.

Para <alguma razão> – Por que estamos construindo? Qual é o valor para o cliente? / Algum resultado específico, ou que algo seja feito.

Para ficar mais claro, observe os exemplos abaixo:

Como um usuário não cadastrado

Gostaria de realizar o cadastro

Para que eu possa acessar as funcionalidades do aplicativo

Critérios de aceite:

– Um usuário não pode se cadastrar sem preencher todos os campos obrigatórios (nome, sobrenome, número de telefone, e-mail e senha).

– Um usuário não pode se cadastrar caso digite um número de telefone inválido.

– Um usuário cadastrado não pode se cadastrar novamente.

Como um vendedor

Gostaria de verificar se um livro está disponível no estoque

Para que eu possa vendê-lo ao cliente

Critérios de aceite:

– O vendedor não pode solicitar a busca se não informar o nome do livro. 

– O sistema, encontrando o livro, deve apresentar todos os dados do livro (nome completo, autores, editora, ano de edição).

– O sistema não encontrando o livro, deve informar que o livro não foi encontrado.

Considerações finais

Além de sua agilidade e leveza, as histórias mantêm o foco no usuário. Uma lista de histórias mantém a equipe focada em resolver problemas de usuários reais, aumentando a qualidade do produto final.

Nesse sentido, elas também permitem a colaboração possibilitando a equipe de trabalhar em conjunto para decidir como atender melhor o usuário. Ademais, elas impulsionam soluções criativas, incentivando o pensamento crítico e criativo da equipe sobre a melhor maneira de resolver para chegar na meta final.

Agora que você já sabe o que são e como criar as histórias de usuário, já pode utilizá-las para a aplicação do método ágil em sua equipe!

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Siga-nos nas redes sociais para saber mais!

Escrito por Italo Gustavo Guasti e revisado por Prof. Tiago Carneiro."
Um breve histórico sobre virtualização,http://www2.decom.ufop.br/terralab/um-breve-historico-sobre-virtualizacao/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/10/page-blog-1.png,"Avirtualização surgiu em 1960, quando computadores eram muito grandes e caros e, desta forma, era necessário compartilhar seus recursos para que não ficassem ociosos. Atualmente, os servidores são computadores relativamente robustos cujos recursos também precisam ser compartilhados. Eles são utilizados para hospedar diversos serviços como: autenticação (firebase), armazenamento e processamento de dados (servidor de bancos de dados), aplicativos web (google docs, whatsapp), etc. Muitas vezes, estes serviços são hospedados em um mesmo servidor e precisam ter a ilusão de que estão instalados um equipamento dedicado a ele, tendo limites de uso de memória, de disco e de processamento definidos pela pessoa que administra o servidor.  É a tecnologia de virtualização que oferece aos serviços essa ilusão.

Logo, podemos entender que as tecnologias de virtualização tem como objetivo permitir o compartilhamento de recursos computacionais, oferecendo aos usuários (sejam eles pessoas ou softwares) a ilusão de que possuem um equipamento dedicado a eles. Atualmente, a tecnologia de virtualização é utilizada por diversas empresas para oferecer servidores virtuais que podem ser alugados por qualquer usuário para a hospedagem de serviços que ele(a) próprio(a) ou sua empresa desenvolveu.  Desta maneira, esses usuários não precisam se preocupar com a manutenção dos equipamentos e com a administração e segurança da rede que os interconecta, nem mesmo com o espaço físico e com a energia elétrica que estes equipamentos demandam. 

Servidores onde se hospedam serviços virtuais

Este artigo tem como objetivo introduzir e distinguir conceitos como máquinas virtuais, containers docker mostrando como as tecnologias de virtualização evoluíram ao longo do tempo.

Isolamento de serviços por meio da virtualização

Uma vez que um servidor dificilmente vai executar apenas um único serviço a virtualização também é uma ótima estratégia para isolá-los. Como a virtualização oferece a cada serviço a ilusão de que eles possuem um equipamento exclusivo a eles dedicados, os serviços que possuem conjunto de dependências (bibliotecas ou frameworks, por exemplo) diferentes, ou que trabalhem com versões diferentes destas dependências, ou que demandam regras de firewall distintas, não irão interferir uns nos outros. 

Máquinas virtuais e Sistemas Operacionais

Nenhum aplicativo (software) conversa diretamente com a máquina (hardware). Os sistemas operacionais são programas de computador que intermediam essa conversa, tornando-a mais fácil e gerenciando todos os recursos da máquina. Assim, os aplicativos executam sobre o sistema operacional e quando precisam de qualquer recurso computacional da máquina, eles precisam solicitar ao sistema operacional que aloque este recurso, seja ele espaço de memória ou disco, seja tempo de processamento (CPU), ou qualquer acesso aos dispositivos de entrada (teclado, mouse, joystick) e saída (tela, impressora).

A virtualização permite que vários sistemas operacionais sejam simulados dentro de um outro sistema operacional e é uma prática amplamente utilizada até os dias de hoje. Alguns benefícios da virtualização são: Reduzir a quantidade de equipamentos utilizadas e aproveitar melhor um hardware potente (com partes ociosas), diminuir os custos com hardware, eliminar o hardware não confiável, diminuir o gasto com energia elétrica, diminuir o calor produzido pelas máquinas, obter mais espaço físico, etc.

Máquina virtual é o nome que recebem os softwares de virtualização que atuam desta maneira, permitindo a simulação de um sistema operacional dentro de um sistema virtual hospedeiro. As máquinas virtuais mais famosas são: VMware, Virtualbox, Parallels Desktop e Microsoft Hyper-V. Todavia, cada novo sistema operacional virtualizado acarreta em um custo alto de hardware, que é consumido apenas apenas para manter os diversas camadas de sistema operacional (sem o serviço). Por exemplo, é possível instalar o sistema operacional Windows dentro de uma máquina executando o sistema operacional Linux e vice-versa.  No entanto, em ambos os casos, serão consumidos recursos computacionais em dobro (memória, processamento e disco) para manter os dois sistemas operacionais em execução e ainda para manter o software de virtualização que cria a máquina virtual. Além disso, os serviços instalados sobre o sistema operacional virtualizado também demandará recursos e executará um pouco mais lento. Uma fragilidade crítica advém do fato de que se uma máquina virtual apresenta problema, então, todas as dependências dos serviços instalados sobre ela serão afetadas. É claro, o mesmo acontece se o sistema operacional falhar.

Containers e Dockers

Uma alternativa ao sistema de virtualização tradicional é a tecnologia Linux Container (LXC). Essa tecnologia move para o núcleo (do inglês, kernel) do sistema operacional as funcionalidades que permitiam as máquinas virtuais oferecer o compartilhamento de recursos computacionais e o isolamento dos ambientes de execução, oferecendo a ilusão de um equipamento exclusivo e dedicado aos usuários. Como estas funcionalidades se tornaram nativas ao núcleo do Linux, elas executam de forma mais rápida, podem ser compartilhadas por diversos serviços e evita a sobrecarga trazida por uma camada de virtualização. Caso esteja curioso, o núcleo do sistema operacional é seu principal componente, ele é responsável pela interface entre hardware e processos em execução, além de coordenar a comunicação entre esses processos. Infelizmente, a tecnologia de containers não funciona nativamente no Windows e no MacOS, é exclusiva do Linux.

A tecnologia de container isola um conjunto de processos e arquivos em um “container” (conceito fictício para representar o isolamento desse conjunto de arquivos). Ela também permite a um usuário controlar a quantidade máxima de hardware que um container poderá consumir (CPU, memória, disco, rede, etc). Desta forma, é como se ambientes totalmente isolados fossem criados, em que um não tem acesso e nem sabe da existência do outro, mas compartilham o mesmo núcleo do sistema operacional.

A empresa Docker criou o mais famoso e utilizado software ao redor da tecnologia  Linux Containers. Um serviço esse que possibilita diversas ações como: Criação de imagens Docker, isto é, conjuntos de códigos automatizados para criação de um container que também pode possuir o código fonte para outros programas (banco de dados, frameworks, etc); Iniciar e parar a execução de um container; Compartilhamento de volume entre o computador principal e os containers de forma automatizada; Criação de uma rede própria entre os containers; Etc. Desta forma, o uso de containers se torna muito mais prático e funcional.

Exemplo Prático com Docker

Esta seção de texto traz uma demonstração básica do funcionamento de containers Docker, para auxiliar na compreensão do artigo. Após fazer o download e instalar o programa Docker, você vai acessar o terminal do seu sistema operacional e seguir o passo a passo abaixo:

docker run –name some-postgres -e POSTGRES_PASSWORD=mySecret -d postgres

O comando “docker run” executa uma imagem docker qualquer. Os parâmetros “–name” e “-e” permitem a um usuário dar um nome ao container sendo criado e configurar variáveis de ambiente nesse container, respectivamente. Neste caso, o comando criará um container a partir de uma imagem do servidor de banco de dados “postgres”, conforme requisitado pelo último parâmetro. O parâmetro “-d” é o comando disattached para a imagem rodar em background, liberando o terminal após o comando ser executado . O container recebe o nome “some-postgres” e a variável de ambiente “POSTGRES_PASSWORD” recebe a senha do servidor de banco de dados. Caso você não tenha a imagem “postgres” no seu computador,  o docker irá fazer o download da imagem a partir da coleção de imagens disponíveis no Docker Hub, isto é,  um repositório para o download de imagens pré-configuradas. Ao final do comando “run”, o docker e irá te retornar o container ID no seguinte formato:

ee16c32b855f42d349f5d7f2820549294630a05877f53b09ab101e7ca8759caa

Caso o usuário necessite, ele poderá acessar o servidor Postgre no endereço localhost, em seu porto padrão 5432. Mas para isso, ele precisará primeiramente redirecionar este porto para o container que ele acaba de criar, por meio do parâmetro “-p 5432:5432” (host:docker port), redirecionando o porto 5432 do computador para o porto 5432 do seu container.

O comando docker ps retorna todos os containers dockers em execução no momento, e ainda mostra algumas informações como o ID do container, a imagem utilizada para sua criação, o estado do container (status), a data de criação (created),a porta em que o container está escutando.

docker ps

O comando abaixo é utilizado para interromper a execução do container cujo ID é fornecido como parâmetro. Caso seja interrompido com sucesso, o comando retorna o ID do próprio container. Neste caso, por não haver ambiguidades, as 4 primeiras letras do ID são suficientes para identificá-lo.

docker stop id_do_container

Arquitetura de Microsserviços

A partir da tecnologia de container, surge a arquitetura de microsserviços que se molda de forma que cada parte de um serviço fica isolada da outra e se comunicam através da rede que interliga os contêineres. Por exemplo, tradicionalmente, todos os componentes de uma aplicação WEB desenvolvida em PHP seriam instaladas em um único hardware (lado esquerdo da imagem abaixo). Na arquitetura de microsserviços, uma instância docker executaria e isolaria o servidor de banco de dados MYSQL, outra instância docker executaria o servidor de aplicação que foi desenvolvido em PHP-FPM, então, uma terceira instância docker hospedaria os componentes do site em um servidor HTTP da Apache. A arquitetura de microserviços confere maior estabilidade, manutenção mais fácil, melhor escalabilidade (é mais fácil escalar os containers docker) e atualização mais fácil e rápida. Por esses e outros benefícios a arquitetura de microsserviços é uma das arquiteturas mais utilizadas atualmente.

Considerações finais

A virtualização vem se tornando uma tecnologia cada vez mais necessária para a computação. Ela vem reduzindo o custo de recursos computacionais e vem permitindo que os usuários paguem apenas pelo uso efetivo destes recursos. Ela também vem aumentando a utilização de equipamentos, evitando desperdícios. Ela desonera os usuários do esforço de manutenção dos equipamentos e da segurança das redes que os conecta.

Neste artigo, abordamos as diversas formas de virtualização apresentando um breve histórico de como evoluíram. Na segunda metade do artigo, demos ênfase ao seu uso para o desenvolvimento de arquiteturas de microsserviços. No entanto, este ainda é um artigo superficial e introdutório. Diversas  tecnologias foram criadas ao redor da virtualização e existe uma extensa gama de conteúdos para se aprender. Gostaria de aprender mais sobre virtualização, arquitetura de microsserviços e as tecnologias que as rodeiam?  Você atua profissionalmente na área? Tem algo a acrescentar nesta discussão? Alguma dúvida sobre aquilo que apresentamos? O seu comentário é muito bem-vindo! Comente, compartilhe esta publicação e não deixe de acompanhar nosso blog e redes sociais.

Referências 

Docker Docs

Virtualização: uma realidade flashback

Linux Containers vs Docker – What is the Difference and Why Docker is Better

Artigo escrito por Guilherme Carolino .Revisado por Prof. Tiago Carneiro."
5 Dicas de UX/UI sobre como construir um bom formulário,http://www2.decom.ufop.br/terralab/5-dicas-de-ux-ui-sobre-como-construir-um-bom-formulario/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/09/BlogPost-29-09-1-730x350.png,"Aferramenta de formulários, nos dias atuais, é uma das mais utilizadas em diversas plataformas, tanto web quanto mobile, e é ideal para quem precisa solicitar feedback sobre algo, organizar inscrições para eventos, pedir avaliações, realizar pesquisas ou coleta de dados de diversos públicos e com diversos objetivos.

Uma das principais preocupações ao criar um é “qual seria a melhor interface para esses formulários?”. Nesse artigo você verá 5 dicas de como fazer um formulário mais interativo, eficaz e menos cansativo para os usuários.

1 – Agrupe todos os campos que forem relacionados

Quando se está construindo um formulário é necessário ficar bem atento com a sua estruturação. Um dos erros mais comuns nessa etapa é colocar as informações dos formulários de qualquer jeito e isso é uma má ideia do ponto de vista do usuário, pois o confunde em qual parte se encontra e o que se deseja coletar com essas perguntas.

Uma das melhores formas de se construir um bom formulário é manter as perguntas em uma sequência intuitiva, de forma lógica e sempre agrupando os campos relacionados de acordo com as perguntas feitas pelo formulário, assim estará sempre informado aos usuários em qual seção ele se encontra e qual é o objetivo de cada uma daquelas perguntas.

Bom agrupamento vs. Agrupamento incorreto
2 – Defina os campos em uma única coluna

Durante a elaboração de um formulário, é importante criá-lo com apenas uma coluna, pois o usuário terá uma velocidade de leitura e de compreensão muito mais rápida lendo de forma vertical, em comparação ao em formato de Z, onde o tempo é bem maior e a desorganização é mais comum de acontecer.  Porém toda regra possui sua exceção. Quando o campo é pequeno, como, por exemplo, município e estado, é possível manter em duas colunas sem problemas porque não é prejudicial ao desempenho do formulário.

Leitura em Uma coluna vs. Duas colunas
3 – Validação dos campos

Durante uma validação de qualquer campo que seja, é fundamental que o campo apresente para os seus utilizadores um feedback de sua ação no determinado campo preenchido, seja um feedback positivo ou negativo. É importante deixar claro que aquele campo é válido, com isso é possível prevenir que o usuário cometa um erro.

Abaixo estão dois modos de mensagem de feedback para os seus utilizadores. 

Feedback positivo vs. Sem feedback
Feedback Negativo vs. Sem feedback
4 – As opções devem ficar visíveis quando possuir mais de 6 opções

Ao criar uma secção do formulário em que o usuário precise fazer uma escolha entre uma das opções listadas, caso hajam mais de 6 opções de escolha é aconselhável que se utilize o componente de dropdown, pois este permite deixar o formulário menos poluído e requer apenas dois cliques: um para exibir as informações e outra para esconder as opções de seleções.

Assim o processo de escolha fica mais intuitivo para os seus usuários. Caso haja menos de 6 opções de escolha é agradável deixar todas as opções visíveis, como em botões. 

Muitas opções de escolha vistas em menu Dropdown vs. Sem Dropdown
5 – Formatação adequada para campos de respostas

O Instituto Baymard fez uma pesquisa sobre o estudo de usabilidade  e percebeu que quando os campos apresentam um tamanho muito maior do que o necessário, faz o usuário se questionar se preencheu o campo corretamente, se a resposta esperada era realmente aquela. Então o ideal é sempre ajustar o tamanho do campo conforme o tamanho da resposta, por exemplo: quando já possuírem campos definidos, como o CEP, colocar o tamanho do campo de acordo com o tamanho dos CEPs brasileiros.

Campos de tamanho ajustado ao seu conteúdo vs. Sem ajuste
Considerações finais

Pronto! Aproveite as dicas para criar formulários online de preenchimento de mais rápido, fácil e intuitivo para seus utilizadores.

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Siga-nos nas redes sociais para saber mais!

Artigo escrito por João Pedro Mendes. Revisado por Luka Menin."
Breve introdução ao Figma: Um editor vetorial para prototipagem e projetos de interfaces gráficas,http://www2.decom.ufop.br/terralab/breve-introducao-ao-figma-um-editor-vetorial-para-prototipagem-e-projetos-de-interfaces-graficas/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/09/page-blog-.png,"OFigma é uma ferramenta completa para a prototipagem e desenvolvimento colaborativo de  projetos de interfaces gráficas com o usuário (GUIs – Graphical User Interfaces). Ele próprio possui uma interface intuitiva, não importa o seu nível de experiência.

Alternativa online a produtos como Sketch, Adobe XD e Adobe Illustrator, o Figma permite o acesso simultâneo em um mesmo projeto, possibilitando o desenvolvimento em conjunto de um projeto em tempo real. Todo trabalho é salvo automaticamente, além de ser possível ver o histórico de versões.

Entendendo a interface do Figma
Figura 1. Visão geral da interface

Podemos separar a interface do Figma em quatro grandes áreas, sendo elas:

Área verde: Ferramentas mais importantes (Toolbar)
Área vermelha: Camadas de informação e páginas (Layers)
Área azul: Área de trabalho (Canvas)
Área amarela: Painel de propriedades (Property panel)
Como usar o Figma
Figura 2. Criando um Frame

O primeiro passo é escolher um formato, e para isso temos o Frame, que permite você escolher uma área da tela para criar seus projetos. Há várias opções como tela para celular, desktop, mídias sociais, entre outros. Frames também dão acesso a funcionalidades como Layout Grids, Layout Automático, Constraints e Prototipagem.

Figura 3. Exemplo de um Frame de postagem para Instagram e tela para Android.

Cada Frame vem com o tamanho padrão, podendo ser redimensionado. As informações de propriedades ficam na área amarela, Figura 1. Pode ser alterado cor, tamanho e ainda é possível adicionar efeitos, entre outras coisas.

No próximo passo vamos adicionar elementos ao Frame e começar a ver a hierarquia das camadas sendo formada. Cada elemento é adicionado no topo da hierarquia e irá sobrepor todos os elementos adicionados anteriormente.

Figura 4. Hierarquia dos elementos

Ao selecionar o elemento, suas propriedades serão apresentadas e editadas no painel à direita, podendo alterar cor, tamanho, adicionar efeitos como contorno, sombra, entre outros. 

Figura 5. Propriedade dos elementos

Fill: É o local onde será definida a cor. Para isso, basta clicar no quadradinho com a cor e aparecerão outras. Além da cor sólida, também é possível escolher entre Linear, Radial, Angular, Diamante e uma imagem.

Stroke: É contorno da forma.

Effects: Possui quatro tipos de efeitos:

Drop shadow: Sombra por trás;
Inner shadow: Sombra por dentro;
Layer blur: Borrado;
Background blur: Borra o fundo do elemento.
Visualizando o projeto

Ao clicar no botão de Play, abrirá uma nova guia com a apresentação em tamanho real

Botão para Apresentação do projeto
Figura 6. Apresentação do projeto
Considerações finais

O Figma é uma ferramenta extremamente poderosa, podendo ser utilizado para o desenvolvimento de diversos tipos de peças gráficas, desde imagens simples para publicação em redes sociais até o desenvolvimento de telas de aplicativos (GUIs) de alta fidelidade. Mesmo não tendo experiência, o usuário consegue aprender rápido e se adequar devido a sua interface intuitiva.  Por ter a opção de ser usado online, atrai usuários que não possuem muitos recursos de hardware. Outra grande vantagem é a variedade de plugins de ícones, tipografia, gráficos, entre outros, o que torna o fluxo de trabalho ainda mais rápido. Tudo é salvo automaticamente, podendo ser acessado de qualquer lugar.

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa.

O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Siga-nos nas redes sociais para saber mais!

Artigo escrito por Vitor Hugo Leles Fonseca. Revisado por Prof. Tiago Carneiro."
Full Stack ou especialista? A resposta pode estar em você!,http://www2.decom.ufop.br/terralab/full-stack-ou-especialista-a-resposta-pode-estar-em-voce/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/09/blog-730x350.png,"Énotório o crescimento de vagas no setor de TI em todo o mundo, mas especificamente no Brasil, vemos um crescimento de mais de 170% das vagas no período de 2007 até 2017, segundo dados da RAIS (Relação Anual de Informações Sociais). Logo, é normal que diversos profissionais de outras áreas ou até mesmo recém integrantes do mercado de trabalho busquem empregos nesse setor. Mas em um mundo de possibilidades, onde existem milhares de tecnologias, bibliotecas, frameworks, linguagens e outras tantas opções, surge uma dúvida que atinge principalmente os iniciantes no assunto: Devo ser um especialista ou me tornar Full Stack?

Você já deve saber que um desenvolvedor  Full Stack é basicamente aquele que reúne conhecimentos que vão desde o back-end até o front-end, ou seja, compreende o projeto como um todo e é capaz muitas vezes de realizar um projeto inteiro sozinho, sendo assim, é de se esperar que as empresas busquem sempre esse perfil de desenvolvedor, certo? Errado! O Full Stack é sim dotado de muitos conhecimentos e tem uma visão ampla sobre todos os processos do projeto, porém sai em desvantagem quase sempre quando a vaga é para uma equipe de desenvolvedores experientes e especialistas em suas respectivas áreas, principalmente, em grandes equipes onde não falta recurso para contratar o melhor profissional de cada área. Porém, ao mesmo tempo, um desenvolvedor Full Stack é realmente muito reconhecido no mercado. Nas equipes pequenas ou com recursos limitados, um desenvolvedor capaz de resolver problemas de várias áreas é perfeito para corte de gastos e para diminuir o gargalo de comunicação na equipe, já que ele será capaz de entender e repassar informações de todas as partes de um projeto.

Então o melhor é realmente ser Full Stack? A resposta é clássica: depende! Diversos artigos e desenvolvedores experientes defendem que essa escolha deve estar muito mais atrelada ao seu perfil como desenvolvedor do que ao seu interesse profissional, empregos não faltam em ambas as áreas, inclusive faltam profissionais, e aos montes! Por ser um mercado que não para de crescer principalmente no Brasil, falta muita mão de obra qualificada no setor. O mais importante aqui é realmente identificar seu perfil de desenvolvedor e tentar entender como você se imagina trabalhando com TI. Especialistas normalmente são pessoas que gostam de se destacar em determinada área e tem consciência de que precisam entender a fundo sobre sua área e principalmente se manter atualizado sobre ela. Ao mesmo passo que um desenvolvedor Full Stack também têm de estudar diversas áreas e se manter atualizado sobre todas elas, mas, como sabemos que é humanamente impossível saber tudo sobre todas as áreas, é indicado que mesmo desenvolvedores Full Stack escolham uma certa área para se aprofundar um pouco mais.

E então, já sabe para qual lado seguir? Sendo assim, fique de olho nos outros artigos do blog que quem sabe você encontra seu caminho. Aproveite para nos seguir nas redes sociais e ficar por dentro de tudo o que acontece no lab.

Artigo escrito por Filipe Rodrigues, revisado por Prof. Tiago Carneiro."
Resultados do TerraLAB no mês de Agosto,http://www2.decom.ufop.br/terralab/resultados-do-terralab-no-mes-de-agosto/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/09/Arte-em-acao-18.png,"Esta é uma sequência de artigos que tem como objetivo mostrar a evolução dos integrantes do TerraLAB ao longo de cada mês. No artigo anterior mostramos parte do desenvolvimento dos três projetos que estão em andamento no TerraLAB envolvendo as nossas equipes de UX, Engenharia, Análise de Dados, Infraestrutura, Gestão de Projetos e Marketing, são eles: o Serviço de Geocodificação em Massa (SGM), o aplicativo Segurança da Mulher e o BatCaverna. 

SGM:  Na área de logística e de pesquisa de mercado, a geocodificação criou certas facilidades, que até então não existiam, e proporcionou uma transformação nos métodos de planejamento estratégico, organizacional e distributivo de mercadorias. Um serviço de geocodificação em massa na nuvem, deve ser desenvolvido pela grande aplicabilidade em diversas empresas de diferentes ramos, pois garante perspectiva de crescimento logístico, financeiro e tecnológico. Além disso, na grande maioria das vezes, dados sobre os sistemas sociais são vinculados a um endereço de logradouro ao invés de a uma coordenada geográfica. Este é comumente os casos dos eventos de saúde ou criminais. Consequentemente, o serviço de geocodificação é útil para a espacialização destes eventos e para o entendimento das maioria dos sistemas sociais. Com essas informações o projeto AWS SGM: Crawler de Geocodificação, busca trabalhar no problema: Como conhecer a localização de um logradouro, sem ir a campo?
Segurança da Mulher: O aplicativo tem como objetivo apontar o nível de segurança, com relação à mulher, de diversos locais da cidade com base em denúncias anônimas feitas pelas usuárias sobre qualquer tipo de importunação sexual. Essas denúncias serão exibidas em um mapa, bem como o nível de segurança dos locais, para que qualquer pessoa possa consultá-las. O aplicativo também conta com um espaço para que o importunador tenha direito de resposta e para que os representantes dos locais denunciados exponham o seu lado da história. Ao mostrar um mapa detalhado com indicadores de zonas seguras e de perigo, espera-se poder contribuir para a livre circulação com segurança das usuárias do aplicativo.
BatCaverna: O aplicativo tem como objetivo fazer a coleta detalhada do patrimônio espeleológico da área delimitada no estado de Minas Gerais e busca contribuir para o incremento do banco de dados das cavernas presentes. A principal ferramenta será a de cadastro de pontos de controle ou de feições com as suas respectivas descrições e imagens associadas. Aliado a esse cadastro de pontos, o aplicativo irá coletar os caminhos feitos pelo o usuário e, durante esse caminho, ele terá a liberdade de criar quantos pontos forem necessários. Outra funcionalidade importante que está atribuída a esse aplicativo é a de visualizar os caminhamentos, tanto de uma forma individual, clicando sobre o caminho realizado, quanto de forma geral que será exibida em seu menu principal. O sistema também contará com um gerador de relatórios baseado nos pontos e fotografias tiradas em campo.

Na última semana do mês de agosto ocorreu a validação mensal do trabalho realizado pelas equipes do TerraLAB, acompanhe nos vídeos abaixo o que foi produzido e quais são as principais ferramentas e tecnologias que estão sendo utilizadas e estudadas nesses projetos.

USER EXPERIENCE

No vídeo a seguir, o gerente de UX, João Pedro Siqueira nos mostra o que o seu time realizou no mês de agosto. O backlog da equipe foi colocado no Jira, software voltado para o monitoramento de tarefas e acompanhamento de projetos, foi feita a divisão de cargos dentre os membros da da equipe e, por fim, além da prototipagem das telas, que foram validadas pelo cliente.

ENGENHARIA

O nosso gerente de engenharia, Vinícius de Paula, mostra no vídeo a seguir a reestruturação do GitLab e os backlogs dos times de desenvolvimento definidos e adicionados no Jira. Vinícius fala do planejamento de experimentos de análise de desempenho, análise de escalabilidade e impacto de diferentes arquiteturas. E, por fim, mostra os relatórios sobre API Gateway e Kubernetes, usados nas aplicações.

DATA ANALYTICS

No vídeo deste mês, o gerente de Data Analytics, Alan Santandrea, apresenta a biblioteca em Python criada pelo chapter do seu time. A biblioteca contempla funções boas para gerar insights e análises em cima de dados geoespaciais, contribuindo para aqueles que precisam tratar bases de dados de endereços ou querem gerar valor em cima de bases de dados geográficas. O vídeo mostra uma possível análise usando as funções da biblioteca e mostra como agregar valor à base além de algumas visualizações bem interessantes de mapas.

INFRAESTRUTURA

Agora o Guilherme Carolino, gerente de infraestrutura do TerraLAB, nos mostra o projeto feito pelo seu time que funciona como um serviço de geocodificação utilizando a tecnologia Serverless. O projeto possui duas lambdas principais: A primeira faz requests para APIs de geocodificação dados “N” endereços e escreve em uma fila de mensagens, que é instanciada em uma máquina virtual, enquanto a segunda lambda consome a fila de mensagem e escreve no banco de dados.

GESTÃO DE PROJETOS

No mês de Agosto, com o fim do processo seletivo, a equipe do TerraLAB cresceu. Isso levou os gerentes de projeto, Emanuel Xavier e Bernardo Santos, a realizarem o remanejamento das equipes para integrar os novos Trainees aos projetos do laboratório. Além disso, a forma de uso da ferramenta Jira foi estudada e implantada, se adequando tanto às necessidades do lab quanto à metodologia da Gerência de Projetos. Estabelecida a plataforma de controle de projetos e negócios, passamos a dispor de relatórios e gráficos que nos permitem monitorar e avaliar os avanços dos projetos, mostrados no vídeo a seguir.

MARKETING

No mês de agosto a equipe de marketing atualizou as abas de artigos científicos e empresas parceiras do lab em nosso site, além de desenvolver um documento de controle de publicações nas redes sociais. O backlog da equipe também foi adicionado no Jira. Os nossos artigos mais visualizados e as publicações que mais nos trouxeram resultados são mostrados no vídeo a seguir pela nossa gerente de marketing, Paloma Bento.

Nós estamos há quatro meses trabalhando nos projetos aqui citados. Utilizamos o primeiro mês para planejamento e esta é, portanto, nossa terceira entrega. Caso você represente uma empresa ou seja um/uma profissional experiente, gostaríamos do seu feedback para que possamos melhorar continuamente o nosso trabalho. 

Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab! 

Você sabe como a escassez de recursos humanos tem impactado negativamente as empresas de software? Você sabia que uma parceria conosco lhe ajudaria a reduzir esses impactos? Leia mais sobre estas duas questões neste artigo escrito pelo time TerraLAB. "
Mudança de carreira,http://www2.decom.ufop.br/terralab/mudanca-de-carreira/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/08/blog.png,"Que tal trabalhar com Tecnologia da Informação?

Permanecer no mesmo setor e seguir em uma mesma carreira por muito tempo, às vezes até a aposentadoria, era algo comum e visto com bons olhos até certo tempo atrás. Porém, nos dias atuais, em que o mercado de trabalho exige dinamismo e certas áreas estão em ascensão, enquanto outras estão saturadas de profissionais, as pessoas não costumam permanecer por muito tempo na mesma atividade. Também existem as que procuram fazer a transição de carreira a fim de serem absorvidas pelo mercado.

O presente artigo traz uma breve reflexão a respeito da transição de carreira para área de Tecnologia da Informação (TI), discutindo os desafios acarretados, as motivações para a ocorrência de tal mudança e o impacto do TerraLAB para o auxílio no aprendizado e na abertura de novas oportunidades.

O que é transição de carreira?

A transição de carreira é um processo que faz parte de uma boa gestão de carreira e que se feita de maneira correta poderá render bons frutos aos envolvidos. Para isso é necessário que cada escolha faça parte de um plano com objetivos bem definidos. Porém a transição de carreira também pode acontecer de forma inesperada em algumas situações, como por exemplo:

·    Quando a profissão está deixando de existir: O impacto da evolução tecnológica e a sua aplicação nas mais diversas áreas, faz com que novas profissões surjam e que profissões antigas simplesmente desapareçam ou com que o trabalho seja diferente e exija novas habilidades. Um exemplo é que com o surgimento da energia elétrica, surgiu a profissão de eletricista e a de acendedor de lamparinas deixou de existir.

·    Quando há um limite de idade: Certas profissões, como por exemplo um atleta de alto nível, possuem um limite natural que as impedem de serem exercidas para toda a vida. Nesses casos é comum que o atleta se torne treinador ou empresário, no mesmo ramo.

·    Estresse e saturação: A falta de tempo livre, o estresse excessivo e a cobrança permanente comum em algumas atividades podem gerar a necessidade e/ou desejo da realização de uma transição de carreira.

·    Crises e demissões em massa em determinados setores: As crises podem gerar mudanças estruturais e induzir diversos profissionais a se adaptar a novos contextos e realizar novas atividades, mesmo que de forma momentânea.

Fonte: Fundação Instituto de Administração
Os desafios

Atualmente, a alta oferta de vagas no setor de TI, na contramão de algumas áreas da engenharia, faz com que profissionais com certa afinidade e aptidão para o setor optem por realizar a transição de carreira. Um dos desafios para o profissional prestes a optar pela transição para o setor de TI é a qualificação. Segundo o artigo Como a escassez de mão de obra qualificada afeta as empresas de TI, a falta de capacitação têm relação direta com a produtividade das empresas e também com a diminuição considerável do período de vínculo empregatício do profissional de TI.

Outro desafio, é a necessidade de tempo para adaptação. Na maioria dos casos, o profissional que opta pela transição de carreira se encontra nessa situação por conta de um acontecimento inesperado que precisa ser gerido. Além disso,  deixar de exercer uma atividade na qual já se tem experiência e conhecimento para ir em busca de outras oportunidades não é algo que acontece da noite pro dia. É importante ter um bom planejamento financeiro aliado ao planejamento de carreira para que a transição ocorra de maneira saudável.

A motivação financeira  

Além das motivações já supracitadas, a transição de carreira pode acontecer com por motivação puramente financeira, visto que a também já citada escassez de mão de obra qualificada faz com que um profissional dotado das habilidades e expertise requeridas pelo mercado, seja altamente valorizado, como pode-se observar realizando uma pesquisa rápida. Atualmente, profissionais de TI são concorridos pelos departamentos de recursos humanos das empresas deste setor. Este profissional também pode atuar como freelancer em modalidade pessoa jurídica.

Fonte: Catho
Considerações finais

O TerraLAB, por meio de suas parcerias, com o auxílio de mentores e tutores altamente capacitados, aliados à troca de conhecimento entre os membros e outras vantagens elencadas em TerraLAB, oferece oportunidade de desenvolvimento profissional e pessoal em diversas das áreas do setor de Tecnologia da Informação, promovendo um ambiente vantajoso para as empresas e para os seus colaboradores.

Se você é aluno de qualquer curso da UFOP, da graduação ou pós, ou representa uma empresa, entre em contato conosco e conheça mais sobre a iniciativa, nossa equipe, nossos projetos, nossos atuais parceiros e benefícios! Para mais informações, acesse a nossa página e nossas redes sociais. 

Artigo escrito por Lucas Cassimiro, Revisado por Prof. Tiago Carneiro."
Por que devemos ensinar programação para crianças?,http://www2.decom.ufop.br/terralab/por-que-devemos-ensinar-programacao-para-criancas/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/08/Arte-em-acao-17.png,"Segundo especialistas, não saber programar é considerado o “analfabetismo do futuro”. Os computadores estão cada vez mais presentes em nosso cotidiano e muitas vezes cumprem tarefas que seriam impossíveis para nós realizarmos. Estamos prontos para viver em um mundo cercado por tamanha tecnologia?

A nova geração, composta pelos chamados “nativos digitais”, está constantemente exposta a telas de computadores, televisores, tablets e smartphones, mas será que eles entendem as máquinas e têm a capacidade de dominá-las? A resposta para a  maioria é não! Apesar do contato constante com a tecnologia, jovens e crianças são expostos a séries, vídeos, filmes, redes sociais e videogames na maior parte do tempo, conteúdos esses empobrecedores que fazem dessa geração apenas consumidora e não produtora de tecnologia. E é aí que mora o problema. 

No livro A Fábrica de Cretinos Digitais¹, o neurocientista francês Michel Desmurget, diretor de pesquisa do Instituto Nacional de Saúde da França, faz um alerta à sociedade sobre como o uso indevido da tecnologia pode afetar o desenvolvimento cognitivo das crianças. Desmurget observou que o tempo gasto em frente a uma tela para fins recreativos atrasa a maturação anatômica e funcional do cérebro em várias redes cognitivas relacionadas à linguagem e à atenção.

Em entrevista à BBC News Mundo, Desmurget foi questionado se os alunos devem aprender habilidades e ferramentas básicas de informática por meio de tecnologia digital; o neurocientista respondeu que se o uso de um determinado software promove efetivamente a transmissão do conhecimento, então sim, a tecnologia digital pode contribuir para o desenvolvimento dos jovens e crianças.

Mas afinal, existe idade certa para começar a aprender a programar?

A britânica Stephanie Shirley, pioneira, empresária e filantropa britânica do ramo da tecnologia da informação, defende que quanto mais cedo as crianças forem introduzidas ao mundo da programação, mais fácil será de assimilar os seus conceitos. Shirley acredita que crianças podem ter o primeiro contato com os fundamentos da lógica de programação a partir dos dois anos de idade, por meio de plataformas voltadas ao ensino para crianças. 

Por meio de conceitos relacionados à sequência, condição e repetição, ligados à uma estrutura lógica disposta em comandos, a programação é capaz de desenvolver a criatividade, o raciocínio lógico, a capacidade de resolver problemas das crianças e adolescentes, além de impactar no desempenho acadêmico de áreas relacionadas à matemática. Apesar de ser promissor, o mercado da indústria de tecnologia da informação sofre e sofrerá com muitos problemas vinculados à falta dessas habilidades (leia mais) que devem ser desenvolvidas em crianças, mesmo que elas não sigam carreira no ramo da tecnologia.

Considerações finais 

Diante da dificuldade que muitos pais têm em proibir o uso das telas ou até mesmo reduzi-lo, nós trouxemos algumas plataformas de programação online e gratuitas que podem, de forma lúdica e atrativa, fazer com que o tempo passado em frente às telas contribua de forma significativa para essa geração, são elas:

 Scratch: O Scratch é uma linguagem de programação que te permite “programar seus próprios jogos, animações e histórias interativas — e compartilhar suas criações com outras pessoas na comunidade on-line. O Scratch ajuda os jovens a aprender a pensar criativamente, raciocinar sistematicamente, e trabalhar em grupo — habilidades essenciais para a vida no século 21.”
Code: “A Code.org® é uma organização sem fins lucrativos dedicada a expandir o acesso à ciência da computação em escolas e aumentar a participação das mulheres e das minorias não representadas. A visão dessa organização é de que todo estudante em toda escola tenha a oportunidade de aprender ciência da computação, assim como aprende biologia, química ou álgebra. Essa organização oferece o currículo mais utilizado para o ensino de ciência da computação nas escolas de ensino fundamental e médio. Além de organizar a campanha anual Hora do Código, que envolveu 10% de todos os alunos do mundo. A Code.org é apoiada por doadores generosos, incluindo a Amazon, o Facebook, o Google, a Infosys Foundation, a Microsoft e muitos outros.”
MIT App Inventor: “o MIT App Inventor é um ambiente de programação visual intuitivo que permite a todos – até crianças – criar aplicativos totalmente funcionais para smartphones e tablets Android e iOS. Aqueles que são novos no MIT App Inventor podem ter um primeiro aplicativo simples instalado e funcionando em menos de 30 minutos. E mais, nossa ferramenta baseada em blocos facilita a criação de aplicativos complexos e de alto impacto em muito menos tempo do que os ambientes de programação tradicionais. O projeto MIT App Inventor visa democratizar o desenvolvimento de software ao capacitar todas as pessoas, especialmente os jovens, para passar do consumo de tecnologia à criação de tecnologia.”

Gostou do assunto? Quer saber mais sobre o mundo da tecnologia? Conheça o objetivo do TerraLAB em nosso site, siga-nos nas nossas redes sociais e acompanhe este blog para ficar por dentro de tudo o que acontece no lab.

¹ DESMURGUET, Michel. A fábrica de cretinos digitais: os perigos das telas para nossos filhos. Península, 2020.

Artigo escrito por Paloma Bento, revisado por Prof. Rodrigo Pedrosa."
Resultados do processo seletivo 2021/2,http://www2.decom.ufop.br/terralab/resultados-do-processo-seletivo-2021-2/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/08/Arte-em-acao-15.png,"Alcançamos a linha de chegada, pois se encerra, nesta semana, o processo seletivo do Programa Trainee TerraLab 2021/2. Abordaremos neste artigo como foi conduzido o processo seletivo, os resultados alcançados e as dificuldades vivenciadas pelos candidatos durante as 5 semanas que duraram o processo.

O processo seletivo, que hoje se conclui, é um programa que vem capacitando estudantes de graduação e pós-graduação de qualquer curso da Universidade Federal de Ouro Preto (UFOP) para atuar nas empresas de Tecnologia da Informação, e é composto por diversas equipes, cada uma dedicada a uma área de trabalho segundo o modelo organizacional de empresa adotado pelo TerraLAB. Após 5 sprints – nome pelo qual são chamados os ciclos semanais de tarefas – os trainees tiveram bastante a aprender sobre as áreas em que se inseriram durante o processo, sobre cooperação em ambiente profissional, e tiveram também a oportunidade de pôr em prática os conhecimentos que adquiriram nesse período. 

Os trainees foram questionados acerca do motivo que os fizeram se interessar no processo seletivo, e as principais respostas falam da oportunidade que o lab oferece de capacitar com eficiência seus membros para atuar no mercado de trabalho, e o desejo de se conhecer melhor as áreas de atuação disponíveis no mercado de Tecnologia da Informação, para quem ainda não tem familiaridade com as áreas ou ainda não tenha definido qual área  seguir. Apresentaremos alguns de seus depoimentos em seguida.

O processo seletivo e seus resultados
User Experience (UX)

Primeiramente a equipe de UX realizou alguns cursos na plataforma da Alura e se reuniu com o cliente para conhecer o produto. Depois elaborou o briefing e o benchmarking, mostrando os pontos fracos e fortes dos concorrentes. Então, se reuniu novamente com o cliente a fim de obter informações dos seus possíveis usuários e projetaram algumas personas para o produto. Por fim, a equipe elaborou o wireframe da interface gráfica (GUI). Desta maneira, desenvolveram um protótipo de um aplicativo para auxiliar profissionais de engenharia no levantamento de dados de campo.

Desenvolvimento de Software

Todos os squads de desenvolvimento de software foram desafiados a desenvolver um aplicativo móvel que permite aos usuários registrar pontos  geográficos de interesse e registrar caminhamentos. Para completar o desafio, os candidatos foram instruídos a utilizar as tecnologias já empregadas dentro do laboratório (React, React Native e NodeJS) e a aproveitar tutoriais e artigos já escritos em nosso blog. Ensinamos aos candidatos como utilizar o GitLab de acordo com os nossos processos e, com base nisso, planejar backlogs, executar  sprints e coletar métricas de produtividade e qualidade  do projeto.

Data Analytics

Durante o processo seletivo os inscritos tiveram dois desafios. O primeiro era um problema de classificação no qual os participantes deveriam gerar análises e insights em cima da tão famosa base de dados IRIS. Após essa etapa deveriam propor e treinar um modelo de Machine Learning capaz de separar corretamente as classes.O segundo desafio foi um pouco mais complexo e demandou dos participantes a habilidade de extrair informações  de um site, gerar análises e, então, criar um modelo de regressão para  de predizer a coluna ‘PTS’. O vídeo apresenta algumas das análises feitas, os modelos e resultados que os trainees produziram. Ao final, apresenta-se  algumas opiniões dos trainees acerca de todo o processo.  

Infraestrutura

Primeiramente os trainees de Infraestrutura hospedaram uma API (previamente desenvolvida) em Node.Js, no Ec2 da AWS, se familiarizando com servidores e com o sistema Linux, Depois, percorreram um caminho de desafios, que a cada sprint exigia tecnologias novas para ser concluído, sempre atentos à arquitetura e fluxo de execução de seu projeto. As tecnologias mais utilizadas foram aquelas que estão mais presentes nos projetos da infraestrutura no TerraLAB, são elas: Docker, Serverless Framework e AWS Lambda, Ec2, TerraForm, PostGreSQL e CI/CD.

Marketing

Para integrar os trainees aos conceitos do marketing, foram disponibilizados dois cursos: marketing de conteúdo e inbound marketing, após a conclusão dos cursos, foi dada aos trainees a tarefa de elaborar um calendário editorial para futuras publicações do lab, com base nos conceitos recém aprendidos e direcionado às personas criadas por eles. Além disso, os trainees produziram conteúdo digital para as nossas redes sociais e, por fim, escreveram este artigo para divulgar todo o trabalho realizado pelos times durante o processo.

De acordo com a opinião dos trainees, as propostas do TerraLAB são diversas, como: Mostrar como é o ambiente de desenvolvimento de projetos numa empresa de software, dar espaço aos erros de iniciante na área, proporcionar uma vivência de multiplicação de conhecimento ensinar os trainees a fazer networking, permitir a vivência do mercado de trabalho ainda durante a graduação e, ainda, melhorar as habilidades técnicas e de trabalho em equipe dos trainees. Logo, percebe-se que o processo seletivo do lab traz uma grande chance para o desenvolvimento pessoal e profissional do candidato. Esse fato é percebido pelos mesmos desde o primeiro momento.

Considerações finais

Ao longo do processo de Trainee, muita coisa foi trabalhada, desenvolvida e melhorada nos participantes, desde conhecimentos de TI até às habilidades de socialização e comunicação. O processo insere o candidato numa realidade em que metas precisam ser cumpridas, para ver como o mesmo trabalha sob pressão e lidando com mais de uma tarefa ao mesmo tempo. Para isso, o planejamento é fundamental, e somado a ele o feedback e o trabalho em equipe são fundamentais para a avaliação necessária. Ademais, o TerraLAB também proporcionou aos estudantes o reconhecimento de que ser autodidata é necessário e, ao mesmo tempo, aperfeiçoou suas habilidades sociais, ao colocar as equipes para trabalharem juntas com foco num mesmo objetivo.

Agora que o processo chega ao fim, resta uma dúvida: O que esperar agora em diante? Questionamos os participantes sobre suas expectativas para a vida profissional agora que experimentaram um pouco do TerraLAB, e como acham que o projeto os ajudará nesse quesito. Veja algumas respostas:

“A estrutura que o Lab possui é algo bem diferente de tudo que existe dentro da UFOP. A proximidade do Lab com empresas e como a estrutura do Lab funciona (igual as principais empresas do mercado de TI) faz com que as pessoas que participam do projeto já saibam como o mundo fora da universidade funciona. Isso ajuda a ter uma maior facilidade de ingresso no mercado.”  — Os depoimentos dos participantes nos dão contexto para entender como o processo seletivo pode redefinir as expectativas de como é trabalhar numa empresa de tecnologia moderna;

“O Lab mostra a realidade de um projeto real e nos capacita para trabalhar neles.”  — Nos elucidam quanto ao quê os trainees consideram ser frutos do processo;

“Através das parcerias formadas – com empresas da área de tecnologia –  e, acima de tudo, devido ao laboratório utilizar toda a tecnologia para qualificação de contratações de recém formados que as empresas gastam tempo e dinheiro para desenvolver.” — Sobre o quê é visto como um diferencial do projeto TerraLab por aqueles que estão chegando agora; 

“Com o conhecimento adquirido enquanto participamos do projeto, já dá pra notar que iremos aprender muito.” — E suas novas expectativas para o futuro dentro do TerraLab.

Por tudo que foi mencionado, é perceptível que o processo seletivo do TerraLAB foi de grande importância para os participantes e o laboratório como um todo. Após todas essas semanas, finaliza-se a seleção e se inserem no projeto novas pessoas trazendo novas ideias, e buscando sempre manter o alto nível das atividades desenvolvidas dentro do lab. 

Assim, o lab inicia mais uma jornada para capacitar mais pessoas para o mercado de trabalho de Tecnologia da Informação, uma das áreas mais cresce e e carece de bons profissionais.  Você sabe como a escassez de recursos humanos tem impactado negativamente as empresas de software? Você sabia que uma parceria conosco tem auxiliado grandes empresas  a reduzir esses impactos? Leia mais sobre estas duas questões neste artigo escrito pelo time TerraLAB.

Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab! 

Artigo escrito por Luka Menin e Vinícius Alvarenga. Revisado pelo Prof. Tiago Carneiro."
Resultados do TerraLAB no mês de Julho,http://www2.decom.ufop.br/terralab/resultados-do-terralab-no-mes-de-julho/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/08/Arte-em-acao-14.png,"Esta é uma sequência de artigos que tem como objetivo mostrar a evolução dos integrantes do TerraLAB ao longo de cada mês. No artigo anterior mostramos os três projetos que estão em desenvolvimento no TerraLAB envolvendo as nossas equipes de UX, Engenharia, Análise de Dados, Infraestrutura, Gestão de Projetos e Marketing, são eles: o Serviço de Geocodificação em Massa (SGM), o aplicativo Segurança da Mulher e o BatCaverna. 

SGM:  Na área de logística e de pesquisa de mercado, a geocodificação criou certas facilidades, que até então não existiam, e proporcionou uma transformação nos métodos de planejamento estratégico, organizacional e distributivo de mercadorias. Um serviço de geocodificação em massa na nuvem, deve ser desenvolvido pela grande aplicabilidade em diversas empresas de diferentes ramos, pois garante perspectiva de crescimento logístico, financeiro e tecnológico. Além disso, na grande maioria das vezes, dados sobre os sistemas sociais são vinculados a um endereço de logradouro ao invés de a uma coordenada geográfica. Este é comumente os casos dos eventos de saúde ou criminais. Consequentemente, o serviço de geocodificação é útil para a espacialização destes eventos e para o entendimento das maioria dos sistemas sociais. Com essas informações o projeto AWS SGM: Crawler de Geocodificação, busca trabalhar no problema: Como conhecer a localização de um logradouro, sem ir a campo?
Segurança da Mulher: O aplicativo tem como objetivo apontar o nível de segurança, com relação à mulher, de diversos locais da cidade com base em denúncias anônimas feitas pelas usuárias sobre qualquer tipo de importunação sexual. Essas denúncias serão exibidas em um mapa, bem como o nível de segurança dos locais, para que qualquer pessoa possa consultá-las. O aplicativo também conta com um espaço para que o importunador tenha direito de resposta e para que os representantes dos locais denunciados exponham o seu lado da história. Ao mostrar um mapa detalhado com indicadores de zonas seguras e de perigo, espera-se poder contribuir para a livre circulação com segurança das usuárias do aplicativo.
BatCaverna: O aplicativo tem como objetivo fazer a coleta detalhada do patrimônio espeleológico da área delimitada no estado de Minas Gerais e busca contribuir para o incremento do banco de dados das cavernas presentes. A principal ferramenta será a de cadastro de pontos de controle ou de feições com as suas respectivas descrições e imagens associadas. Aliado a esse cadastro de pontos, o aplicativo irá coletar os caminhos feitos pelo o usuário e, durante esse caminho, ele terá a liberdade de criar quantos pontos forem necessários. Outra funcionalidade importante que está atribuída a esse aplicativo é a de visualizar os caminhamentos, tanto de uma forma individual, clicando sobre o caminho realizado, quanto de forma geral que será exibida em seu menu principal. O sistema também contará com um gerador de relatórios baseado nos pontos e fotografias tiradas em campo.

Na última semana do mês de julho, ocorreu a validação mensal do trabalho realizado pelas equipes do TerraLAB, acompanhe nos vídeos abaixo o que foi produzido e quais são as principais ferramentas e tecnologias que estão sendo utilizadas e estudadas nos projetos do lab.

User Experience

No vídeo a seguir, o gerente de UX, João Pedro Siqueira nos mostra o que o time realizou no mês de julho, como a criação de telas de alta fidelidade, os fluxos das telas, o guia de estilo e algumas pesquisas internas realizadas pela equipe.

Engenharia

O nosso gerente de engenharia, Vinícius de Paula, mostra no vídeo a seguir o Code Review aplicado nos times de desenvolvimento. Time de inteligência geográfica: Utilizando kubernets e Kong API Gateway para instanciar e atualizar as APIs de geocodificação, realização de testes de stress em cada API com a ferramenta jmeter. E, por fim, fala um pouco sobre os projetos: Serviço de Geocodificação em Massa, framework e componentização, BatCaverna.

Data Analytics

No próximo vídeo o nosso gerente de análise de dados, Diego Matos, faz uma breve apresentação de algumas funcionalidades desenvolvidas pela sua equipe dentro do projeto SGM. Já é possível validar se um ponto de endereço está dentro da malha da cidade, através da malha do IBGE, e visualizar o grau de confiança de um endereço no mapa, utilizando da dispersão espacial dos pontos de diferentes APIs de geocodificação. 

Infraestrutura

Agora o Guilherme Carolino, gerente de infraestrutura do TerraLAB, nos mostra a arquitetura da implementação do projeto SGM na nuvem AWS, utilizando o framework serverless (AWS Lmabda) e sistema de mensageria (AWS MQS).

Gestão de projetos

Quanto à gerência de projetos, o vídeo conduzido por Emanuel Xavier e Bernardo Santos procura compreender os deveres de um Gerente de Projetos, aplicando os pilares conceituais nos processos mensais do Laboratório. Tendo isso em base, os gerentes analisam as falhas e conquistas do Processo Seletivo, sugerindo novas possibilidades e metodologias em sua execução, além de comentar as medidas e ações tomadas em relação a execução dos projetos em desenvolvimento dentro da instituição. Por fim, detalham a implantação da nova plataforma de gestão denominada Jira e também falam do que está por vir dentro do Lab.

Marketing

O marketing tem como objetivo divulgar todo esse trabalho realizado pela equipe do TerraLAB. No blog do laboratório ocorre a publicação semanal de artigos de alta qualidade, escritos pelos próprios estudantes em sua maioria. Produtos, eventos e parcerias também são publicados em diversas redes sociais. Dados destas plataformas mostram que temos crescido enquanto marca e atraído o nosso público alvo. Vale ressaltar que se estivéssemos investindo no Google Ads para ter o mesmo número de acessos que temos em nosso blog, isto nos custaria cerca de 13 mil reais mensais. Os nossos artigos mais visualizados e as publicações que mais nos trouxeram resultados também são mostrados no vídeo a seguir pela nossa gerente de marketing, Paloma Bento.

Nós estamos há três meses trabalhando nos projetos aqui citados, caso você represente uma empresa, gostaríamos do seu feedback para que possamos melhorar continuamente o nosso trabalho. 

Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab! 
Você sabe como a escassez de recursos humanos tem impactado negativamente as empresas de software? Você sabia que uma parceria conosco lhe ajudaria a reduzir esses impactos? Leia mais sobre estas duas questões neste artigo escrito pelo time TerraLAB.

Artigo escrito por Paloma Bento. Revisado por Prof. Tiago Carneiro."
Como a escassez de mão de obra qualificada afeta as empresas de TI,http://www2.decom.ufop.br/terralab/como-a-escassez-de-mao-de-obra-qualificada-afeta-as-empresas-de-ti/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/07/Arte-em-acao-13.png,"Este artigo traz um resumo sobre a problemática gerada pela alta demanda por recursos humanos na indústria de Tecnologia da Informação (TI), discutindo a escassez de  mão de obra qualificada no mercado brasileiro e a alta rotatividade de profissionais percebida pelas empresas de software, agravada pelo acelerado crescimento do número de startups criadas nos recentes anos. 

Para caracterizar quantitativamente estes problemas e seus impactos negativos, o artigo apresenta estatísticas levantadas por consultorias e empresas especializadas no tema, assim como por associações representativas da indústria de TI. 

Finalmente, este artigo apresenta uma solução viável para resolver os problemas citados ou, pelo menos, reduzir seus impactos negativos.

O desafio da indústria de TI no Brasil

Apesar da alta oferta de vagas, atualmente, o setor de TI sofre com a falta de mão de obra especializada. A qualificação dos recursos humanos de uma empresa é um dos principais fatores que impactam no seu sucesso ou fracasso. Segundo a empresa de recursos humanos Korn Ferry,  a falta de capacitação dos profissionais de TI terá impacto significativo na produtividade das empresas, além de diminuir consideravelmente o tempo do trabalhador no mesmo emprego.  Em 2019, o Brasil perdeu duas posições no Global Competitiveness Index (GCI), divulgado pelo Fórum Econômico Mundial, caindo do 94º para o 96º lugar dentre os 141 países avaliados. O cenário do país piora se analisarmos o quesito facilidade de encontrar profissionais qualificados, ficando na 129º posição. Ficamos ainda abaixo da média da região da América Latina e Caribe, como mostra o gráfico:


Fonte: The World Bank – GovData360

Jornais e Revistas destacam o problema que as empresas de TI enfrentam na busca por profissionais qualificados. Além da escassez da mão de obra, o setor corre o risco de sofrer um apagão e dificultar o crescimento do país.

Fonte: Exame
Fonte: Exame

A escassez e profissionais de TI em números

O gargalo produtivo, ou ponto de estrangulamento, é utilizado no jargão administrativo para descrever o processo limitador de crescimento da capacidade produtiva da empresa. 

Uma das pautas de estrangulamento significativo encontrada na indústria brasileira reside na falta de mão de obra qualificada. Segundo estudo da Confederação Nacional da Indústria (CNI), em 2020, 50% das indústrias afirmaram enfrentar esse problema (SondEsp 76). 

Segundo a Brasscom, Associação Brasileira das Empresas de Tecnologia da Informação e Comunicação, o mercado brasileiro de TIC (Tecnologia da Informação e Comunicação), que atualmente é o 7º no mundo, atrás de Estados Unidos, China, Japão, Reino Unido, Alemanha e França, aponta que o setor deve gerar 420 mil vagas no Brasil até 2024. No entanto, mesmo diante da enorme oferta de vagas, a Brasscom considera que o déficit profissionais de TI no mercado de trabalho chegará a 264 mil, em agosto de 2020, e que o Brasil apresenta um déficit anual de 24 mil profissionais na área de TI.

A Associação Brasileira de Startups também considera a oferta de desenvolvedores de software (Dev) insuficiente. Segundo dados do IDC Brasil, o país tem cerca de 150 mil a 200 mil vagas sem candidatos no setor de tecnologia, em um país com grandes índices de desemprego, cerca de 13 milhões de desempregados, a contrariedade centra-se na formação deficiente dos profissionais.

Para agravar a escassez de profissionais qualificados, há um acelerado surgimento de startups e alta rotatividade de profissionais nas empresas de TI. Segundo a consultoria Accenture, há um surgimento marcante de startups brasileiras, 12.700 em 2019, o triplo de 2016, e uma acelerada expansão da economia digital, que já corresponde a 24,3% do PIB brasileiro. Estatísticas do Linkedin mostram que este é o mercado com maior rotatividade de profissionais (13,2%). 

Mão de obra qualificada gera mais produtividade e menos custos

A capacitação da mão de obra na área de TI confere todo o portfólio de conhecimento técnico e normativo para que o colaborador possa desempenhar as suas funções de forma produtiva e com qualidade apurada, além de prolongar a permanência deste dentro da empresa ao ver suas habilidades sendo valorizadas. 

A falta dessa qualificação acarreta prejuízos financeiros às empresas de TI, afinal há uma elevação de custos por causa de retrabalhos, baixa produtividade e desperdício de tempo no desenvolvimento.  

Além disso, a problemática da mão de obra inadequada faz com que muitas empresas reduzam as suas exigências, consideradas básicas para exercer plenamente as funções necessárias para a corporação. Ao mesmo tempo, o custo para manter uma indústria funcionando aumenta, pois torna-se necessário oferecer inúmeros benefícios como atrativo e incentivo, além de em alguns casos, optarem por capacitar os profissionais posteriormente à contratação para conseguir preencher as suas vagas disponíveis. Segundo estudos da Confederação Nacional da Indústria (CNI)(2017), 81% das organizações brasileiras utilizam essa estratégia.

O futuro: Parcerias entre empresas e universidade 

Uma das soluções para reduzir os impactos negativos destas ameaças ao crescimento da indústria brasileira de software tem sido o estabelecimento de parcerias entre empresas e instituições de ensino. Dos anos 2000 em diante, vários avanços promovem essa parceria, desde a criação dos Fundos Setoriais, até a aprovação das Leis da Inovação e do Bem e a recente revisão da legislação, consolidando o Marco Legal de Ciência, Tecnologia e Inovação (Lei nº 13.243/2016).

Há claros estímulos à constituição de alianças estratégicas e o desenvolvimento de projetos de cooperação que envolvam empresas, instituições de ciência e tecnologia (ICTs) e entidades privadas sem fins lucrativos. Entre este estímulos, destacamos: 

Facilidades para a transferência de tecnologia de ICT pública para o setor privado.
Os direitos de propriedade intelectual podem ser negociados e transferidos da instituição de ciência e tecnologia (ICT) para os parceiros privados, nos projetos de cooperação para a geração de produtos inovadores.
Autorização para a administração pública direta, as agências de fomento e as ICTs apoiarem a criação, a implantação e a consolidação de ambientes promotores da inovação.
Documentação exigida para contratação de produto para pesquisa e desenvolvimento poderá ser dispensada, no todo ou em parte, desde que para pronta entrega ou até o valor de R$ 80 mil.
O aperfeiçoamento de instrumentos para estímulo à inovação nas empresas, como a permissão de uso de despesas de capital na subvenção econômica, regulamentação de encomenda tecnológica e criação de bônus tecnológico.
Regulamentação dos instrumentos jurídicos de parcerias para a pesquisa, o desenvolvimento e a inovação: termo de outorga, acordo de parceria para pesquisa, desenvolvimento e inovação, convênio para pesquisa, desenvolvimento e inovação.
Prestação de contas simplificada, privilegiando os resultados obtidos nos acordos de parceria e convênios para pesquisa, desenvolvimento e inovação.
Considerações Finais

Este artigo mostrou que mesmo diante a uma enorme oferta de vagas no mercado de TI, muitas das vagas não são preenchidas devido a escassez de profissionais qualificados. O déficit de profissionais ameaça o crescimento desta indústria e seus impactos negativos vêm sendo noticiados pela mídia especializada, por consultorias e pelas associações de empresas. Da mesma forma, o avolumado crescimento de startups nos últimos anos e a alta rotatividade de profissionais agravam estas ameaças. 

Para evitar colapsos e reduzir impactos negativos, é preciso estreitar as parcerias entre universidades e empresas, investir na capacitação conjunta de recursos humanos como um caminho para aumentar os lucros e reduzir as perdas. Para isso, há claros incentivos para a construção conjunta de um ambiente de inovação aberta que é fortalecido pelo Marco Legal de Ciência, Tecnologia e Inovação.

Diante deste cenário, o TerraLAB oferece capacitação e recrutamento de profissionais de TI para empresas, visando a redução de custo e de tempo consumidos na seleção e treinamento de colaboradores. Formamos profissionais com vivência nos diversos papéis existentes no ecossistema de desenvolvimento e operação de software, entre eles:  Gerente de Projeto, Product Owner (PO), Designer de Experiência do Usuário (UX), Desenvolvedor de Aplicativos WEB, Desenvolvedor de Backend, Desenvolvedor de Aplicativos Móveis, Engenheiro de Qualidade (Testers), Arquitetos de Infraestrutura,  Cientista de Dados, Analista de Marketing e Analista Jurídico. 

Nossos colaboradores também ganham experiência nos mais modernos processos e ferramentas por meio das atividades práticas que desempenham em projetos reais e simulados. Nós adotamos as metodologias ágeis e mantemos nosso o foco na produtividade de nossos times e na qualidade dos produtos e serviços computacionais que desenvolvemos. 

Se você representa uma empresa, entre em conosco e conheça mais sobre os benefícios mútuos de uma parceria. Conheça nossos atuais parceiros! Para saber mais sobre esses assuntos, acesse a nossa página e veja os resultados atingidos em nossas redes sociais. 

Artigo escrito por Gustavo Moreira. Revisado por Prof. Tiago Carneiro."
Como realizar o empacotamento de scripts Python com o NodeJS,http://www2.decom.ufop.br/terralab/como-realizar-o-empacotamento-de-scripts-python-com-o-nodejs/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/07/Arte-em-acao-11.png,"Caso você tenha familiaridade com o software Node.js, você deve saber das facilidades que ele oferece para o desenvolvimento de serviços WEB. Além de ser uma plataforma considerada leve e de código aberto, é um interpretador de Javascript no lado do servidor, muito utilizado pelos desenvolvedores  comunicação e integração entre backend e frontend.

Por outro lado, Python é uma linguagem largamente utilizada pela comunidade de Ciência de Dados e de Visualização Científica. Esta plataforma de desenvolvimento oferece diversas bibliotecas para a análise e a visualização de dados. 

Neste artigo vamos aprender como realizar o empacotamento de scripts PyQGIS em um servidor Node.js. PyQGIS é uma biblioteca Python para a manipulação, análise e visualização de dados geoespaciais em diversos formatos. 

Para isso,  vamos desenvolver o servidor Node.js em um container docker, a fim de avaliá-lo em um ambiente similar ao ambiente de produção de um serviço WEB executando na nuvem. Caso ainda não tenha utilizado o docker, sugerimos a leitura da seção “Instalação do Docker” do artigo: “Entendendo o funcionamento do CICD dentro do Git flow”.

Antes de implementar nosso serviço geográfico, iremos conhecer alguns componentes, estruturas e softwares básicos que irão nos ajudar no processo de “mão no código”.

Python para geotecnologias: O que são QGIS e PyQGIS?

O QGIS é um Sistema de Informações Geográficas open source, que permite a visualização, análise e edição de dados geoespaciais. Ele possui uma interface de programação na linguagem Python que,  através da biblioteca  de funções chamada PyQGIS, permite que sejam escritos scripts para o desenvolvimento de plugins e para a automação de tarefas rotineiras.

O download e instalação do QGIS é simples e pode ser feita pelo link abaixo:

Link de instalação do QGIS 

Entendendo sobre o componente “multipart/form-data”

Executada sobre o ambiente Node.js, a estrutura “framework Express”, implementa uma rota raiz que responde a uma requisição POST com o tipo de conteúdo multipart/form-data que contém dois arquivos: um script PyQGIS e um arquivo shapefile. Este último arquivo armazena dados geoespaciais em formato vetorial. Estes dados serão utilizados pelo script PyQGIS. 

No backend, estes dois arquivos serão tratados pelo middleware multer que é capaz de manipular formulários do tipo multipart/form-data. Middleware é uma função com acesso a qualquer requisição recebida por um servidor, ela é executada entre o momento em que o servidor recebe a requisição HTTP e o momento em que o servidor responde.  No nosso estudo de caso, o multer é utilizado para salvar os dois arquivos recebidos em uma pasta de nome “tmp” . Após o tratamento realizado pelo multer, precisamos executar o script PyQGIS recebido. Utilizamos o método spawn do módulo child_process, que vem no Node.js por padrão, para executar um bash script que exporta as variáveis de ambiente do QGIS e executar o script PyQGIS. O módulo child_process fornece métodos para manipulação de processos filhos no sistema operacional da máquina em que é executado. O método spawn executa um processo e nos permite escutar os streams de dados: stdout e stderr. No nosso estudo de caso, o script PyQGIS lê o arquivo shapefile e gera um arquivo do tipo geojson como saída. Então, após sua execução, apenas retornamos o arquivo geojson para o cliente. Em outras palavras, nosso serviço geográfico apenas converte o arquivo shapefile recebido de um cliente HTTP para o formato geojson.

Mão no código

Na pasta src/config em nosso projeto, criamos um arquivo com o nome multer.js, onde configuramos o multer. Através da função diskStorage do multer, passamos como argumento uma função que determina onde os arquivos recebidos serão armazenados.  Também passamos como argumento outra função que define qual será o nome dos arquivos quando armazenados. E por fim criamos uma função com o nome fileFilter para permitir o recebimento somente de arquivos com a extensão .py e .shp.

const multer = require('multer');
const path = require('path');

module.exports = {
   storage: multer.diskStorage({
       destination: (req, file, cb) => {
           cb(null, path.resolve(__dirname, '..', '..', 'tmp'));
       },
       filename: (req, file, cb) => {
           const scriptName = 'script-qgis.py';

           if(path.extname(file.originalname) == '.py'){
               cb(null, scriptName)
           }
           else if(path.extname(file.originalname) == '.shp'){
               cb(null, file.originalname)
           }
       }
   }),
   fileFilter: (req, file, cb) =>{
       if(path.extname(file.originalname) == '.py' || path.extname(file.originalname) == '.shp'){
           cb(null, true);
       }
       else{
           cb(new Error('Invalid file type'));
       }
   }
};

Após a criação do arquivo de configuração do multer, criamos uma rota raiz  para responder a uma requisição do tipo POST e que define o multer como seu middleware. Após os arquivos serem processados pelo multer, a requisição é tratada e respondida através da função abaixo. Para criar um processo que executa um shell script, o método spawn do módulo child_process é utilizado, onde passamos como argumento o script que necessitamos executar.  Ao final da execução do script o arquivo gerado é retornado ao cliente da requisição.

routes.post('/', multer(multerConfig).array('file', 2), (req, res) => {
   var spawn = require('child_process').spawn;
   var batch = spawn('/app/launch.sh');

   batch.stdout.on('data', function (data) {
       console.log('stdout: ' + data);
   });

   batch.stderr.on('data', function (data) {
       console.log('stderr: ' + data);
   });

   batch.on('exit', function (code) {
       console.log('child process exited with code ' + code);

       res.download('tmp/rj_state_geometries.geojson');
   });   
});

O arquivo shell script executado pelo método spawn precisa importar as variáveis de ambiente do QGIS e executar o script recebido através da requisição e processado pelo multer. 

export PYTHONPATH=/usr/share/qgis/python
export LD_LIBRARY_PATH=/usr/lib
python3 ""tmp/script-qgis.py""

Para fins de execução em nuvem, utilizamos o ambiente Docker, o qual precisa de um arquivo com o nome Dockerfile na raiz do projeto. Este arquivo define os comandos utilizados para a criação de uma imagem Docker. Neste arquivo definimos como imagem base a última versão do Node.js. Utilizamos a instrução RUN para executar alguns comandos a fim de instalar o QGIS e configurar o projeto e suas dependências. Utilizando a instrução EXPOSE, expomos a porta 3333, que será escutada pelo projeto. E por fim utilizamos a instrução CMD que executará o projeto.

FROM node:latest

WORKDIR /app
COPY . /app
RUN apt update
RUN apt -y install gnupg software-properties-common
RUN wget -qO - https://qgis.org/downloads/qgis-2020.gpg.key | gpg --no-default-keyring --keyring gnupg-ring:/etc/apt/trusted.gpg.d/qgis-archive.gpg --import
RUN chmod a+r /etc/apt/trusted.gpg.d/qgis-archive.gpg
RUN add-apt-repository ""deb https://qgis.org/ubuntu $(lsb_release -c -s) main""
RUN apt update
RUN apt -y install qgis qgis-plugin-grass
RUN npm install
EXPOSE 3333
CMD [""npm"", ""start""]

Considerações finais

Neste artigo, ilustramos de forma prática como podemos empacotar scripts PyQGIS em um servidor Node.js utilizando alguns componentes importantes mencionados acima. 

Uma aplicação muito interessante para esse conhecimento é quando, em ambientes de desenvolvimento Web ou Mobile, se deseja exibir visualizações de dados resultantes de alguma análise realizada por scripts Python. 

Assim, esperamos que este artigo seja útil a todos que estejam iniciando no desenvolvimento de software WEB ou Mobile voltados para aplicações em Ciência da Dados, Visualização Científica ou Geoprocessamento.  

Gostaria de aprender mais sobre outros tópicos na área da tecnologia? Acompanhe as nossas redes sociais e comente, compartilhe essa publicação e não deixe de curtir! 

Sabia que este artigo foi completamente desenvolvido por estudantes de graduação da UFOP? Se você representa uma empresa e deseja contribuir ou se tornar parceira do TerraLAB, entre em contato conosco e conheça tudo que esta parceria pode lhe acrescentar! "
Dicas sobre como fazer uma boa apresentação,http://www2.decom.ufop.br/terralab/dicas-sobre-como-fazer-uma-boa-apresentacao/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/07/Arte-em-acao-9.png,"Tanto na indústria quanto na academia, possuir somente habilidades técnicas não é o suficiente. Tanto quanto fazer um bom trabalho, é importante saber apresentar o seu trabalho. A seguir, apresento dicas importantes para uma boa apresentação.

1 – Conecte-se com a sua audiência

Para fazer uma boa apresentação é fundamental conhecer a sua audiência. Qual o nível técnico da sua audiência? Quanto a sua audiência já sabe sobre o assunto que você está apresentando? Sem ter clareza das respostas para estas perguntas é impossível fazer uma apresentação efetiva. Por isso é importante sempre adequar o conteúdo da sua apresentação à sua audiência.


Photo by takje from FreeImages

2 – Foque no que a audiência precisa

Sua apresentação precisa ser construída em torno do que o seu público vai tirar dela. Foque no que a audiência quer e no que ela precisa saber, não no que você quer contar.

3 – Keep it simple

Defina a mensagem que você quer passar com a apresentação e não desvie dela. Se o que você está planejando dizer não contribui diretamente com a mensagem principal, não diga.

4 – Conte uma estória

A sua apresentação deve funcionar como uma estória. Comece contextualizando os assunto, apresente as personagens (pessoas envolvidas, artefatos construídos, hipóteses), desenvolva estas personagens, crie um clímax e apresente o desfecho das personagens. 

5 – Conclua

No fim da apresentação, exponha um resumo dos principais pontos e uma conclusão dos principais resultados atingidos.

6 – Minimize o texto

Uma apresentação não é o formato correto para a leitura. Você quer que a audiência preste atenção no que você está falando e não no que está escrito no slide. Sempre dê preferência para imagens e use-as para ilustrar o que você planeja falar. Um slide não deve ter muito mais do que 12 palavras.   

7 – Use a regra 10-20-30

– Mantenha o número de slides por volta de 10 ou menos;

– Mantenha o tempo de apresentação por volta de 20 minutos ou menos;

– Use fonte de tamanho 30, no MÍNIMO. Esta regra te protege de colocar texto demais nos slides, um erro fatal.

O vídeo abaixo é um bom exemplo de como fazer uma ótima apresentação. Note como logo no início a apresentadora já se conecta com a audiência. Nos primeiros segundos de apresentação ela já define, em linguagem acessível, o tema e mostra como os conceitos que serão explicados são vistos cotidianamente.

Obviamente, o vídeo acima teve uma equipe de produção profissional e não tem o padrão das nossas apresentações do dia a dia. Mesmo assim, a forma com a apresentação foi feita pode servir de inspiração. 

8 – Divirta-se e aproveite 

A apresentação é uma grande chance de mostrar os frutos de seu trabalho duro e colher opiniões para futuras melhorias. Aproveite esta oportunidade.

Artigo escrito por Prof. Rodrigo Cesar Pedrosa Silva.

Gostou deste artigo? Deixe-nos um comentário! Quer saber mais sobre o TerraLAB? Visite o nosso site e siga-nos nas redes sociais para ficar por dentro de todas as dicas, notícias, artigos e tutoriais do lab."
Resultados do TerraLAB no mês de Junho,http://www2.decom.ufop.br/terralab/resultados-do-terralab-no-mes-de-junho/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/07/Arte-em-acao-8.png,"Atualmente, três projetos estão em desenvolvimento no TerraLAB, envolvendo as nossas equipes de UX, Engenharia, Análise de Dados, Infraestrutura, Gestão de Projetos e Marketing, são eles: o Serviço de Geocodificação em Massa (SGM), o aplicativo Segurança da Mulher e o BatCaverna. 

SGM:  Na área de logística e de pesquisa de mercado, a geocodificação criou certas facilidades, que até então não existiam, e proporcionou uma transformação nos métodos de planejamento estratégico, organizacional e distributivo de mercadorias. Um serviço de geocodificação em massa na nuvem, deve ser desenvolvido pela grande aplicabilidade em diversas empresas de diferentes ramos, pois garante perspectiva de crescimento logístico, financeiro e tecnológico. Além disso, na grande maioria das vezes, dados sobre os sistemas sociais são vinculados a um endereço de logradouro ao invés de a uma coordenada geográfica. Este é comumente os casos dos eventos de saúde ou criminais. Consequentemente, o serviço de geocodificação é útil para a espacialização destes eventos e para o entendimento das maioria dos sistemas sociais. Com essas informações o projeto AWS SGM: Crawler de Geocodificação, busca trabalhar no problema: Como conhecer a localização de um logradouro, sem ir a campo?
Segurança da Mulher: O aplicativo tem como objetivo apontar o nível de segurança, com relação à mulher, de diversos locais na cidade com base em denúncias feitas pelas usuárias.
BatCaverna: O aplicativo tem como objetivo fazer a coleta detalhada do patrimônio espeleológico da área delimitada no estado de Minas Gerais e busca contribuir para o incremento do banco de dados das cavernas presentes.

Na última semana do mês de junho ocorreu a validação mensal do trabalho realizado pelas equipes do TerraLAB, acompanhe nos vídeos abaixo o que foi produzido e quais são as principais ferramentas e tecnologias que estão sendo utilizadas e estudadas nos projetos do lab.

User Experience

No vídeo a seguir, o gerente de UX, João Pedro Siqueira, nos mostra como são as demandas de sua área no lab, exibe um relatório analisando algumas possíveis soluções para a resolução do problema demandado, análise de APIs de serviços geográficos na WEB e, por fim, mostra a renderização do aplicativo BatCaverna, entre outras coisas.

Engenharia

O nosso gerente de engenharia, Vinícius de Paula, mostra no vídeo a seguir a definição e manuseio das APIs de geocodificação utilizadas dentro do lab. Apresenta os artigos produzidos pela equipe de desenvolvimento e o vídeo de lançamento do aplicativo Segurança da Mulher.

Data Analytics

No próximo vídeo o nosso gerente de análise de dados, Diego Matos, faz uma breve apresentação dos artefatos gerados e descobertos pela equipe, passando pela arquitetura do atual projeto, pelas decisões tomadas em atividades exploratórias utilizando-se do google colab e pelo novo backlog da equipe.

Infraestrutura

Agora o Guilherme Carolino, gerente de infraestrutura do TerraLAB nos mostra a implementação de um projeto teste baseado na arquitetura serverless que simula a nova arquitetura de nuvem do Terralab.

Gestão de Projetos

As principais demandas da gerência de projetos desse mês foram remanejar as equipes levando em consideração as atividades que cada um iria fazer e estudar o Jira para melhorar a coleta de métricas, mostra Emanuel, gerente de projetos.

Marketing

O marketing tem como objetivo divulgar todo esse trabalho realizado pela equipe do TerraLAB, com publicações semanais de artigos, de alta qualidade e escritos pelos próprios estudantes em sua maioria, em nosso blog e divulgação dos produtos e parcerias também nas redes sociais, temos crescido enquanto marca e atraído o nosso público alvo. Vale ressaltar que se estivéssemos investindo no Google Ads para ter o mesmo número de acessos que temos em nosso blog, seria cerca de 13 mil reais mensais. Os nossos artigos mais visualizados e as publicações que mais nos trouxeram resultados são mostrados no vídeo a seguir pela nossa gerente de marketing, Paloma Bento.

Gostaríamos de agradecer carinhosamente às empresas parceiras que acreditam TerraLAB e vêm nos apoiando: Gerencianet, GS Ciência do Consumo, Memory, Stilingue e Usemobile.

Nós estamos há dois meses trabalhando nos projetos aqui citados, caso você represente uma empresa, gostaríamos do seu feedback para que possamos melhorar continuamente o nosso trabalho. Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab!

Artigo escrito por Paloma Bento. Revisado por Prof. Tiago Carneiro."
Tipos de aprendizado de máquina e algumas aplicações,http://www2.decom.ufop.br/terralab/tipos-de-aprendizado-de-maquina-e-algumas-aplicacoes/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/06/Arte-em-acao-6.png,"Estamos em 2021 e quando falamos em tendências de mercado podemos incluir o aprendizado de máquina no topo da lista, segundo uma pesquisa feita pela Glassdoor, o cargo de Cientista de Dados é o segundo colocado da lista de melhores empregos na América para 2021, além disso, em pesquisa feita pela Algorithmia, 76% das empresas priorizam a inteligência artificial e o aprendizado de máquina nos orçamentos de TI de 2021. Parece atrativo, não é?

Porém, não é de hoje que essa subárea da inteligência artificial está em alta, ao longo dos anos ela vem mostrando sua importância na medida em que possibilita realizar tarefas árduas e/ou impossíveis para o ser humano além de contribuir no aprimoramento das mais diversas técnicas e processos existentes.

Mesmo tendo grande influência na vida da maior parte das pessoas, pouquíssimas dessas sabem de fato do que se trata, como ela está inserida no nosso dia a dia e o quão poderosa é. 

Mas se você é uma das pessoas interessadas em descobrir sobre essa área, leia esse artigo até o final porque nele você vai descobrir o que é o aprendizado de máquina, métodos de aprendizado, aplicações e tudo o que você precisa para conhecer um pouco mais desse mundo.

1. O que é o Aprendizado de Máquina? 

Em 1959, Arthur Lee Samuel, engenheiro do MIT definiu o aprendizado de máquina como “um campo de estudo que dá aos computadores a habilidade de aprender sem terem sido programados para tal”. Vamos entender o que isso quer dizer a seguir.

Quando estamos criando um modelo de aprendizado de máquina não informamos ao computador os passos a seguir para que ele aprenda o que precisa, isso porque o conhecimento é adquirido, no geral, a partir de modelos estatístico/matemáticos que reconhecem padrões em dados, criando a possibilidade que eles aprendam com seus erros e façam previsões em cima do que foi aprendido. Esses modelos podem ser definidos por diferentes formas de aprendizado e elas serão tratadas na seção a seguir.  

2. Tipos de Aprendizado

Cada um dos itens abaixo vai explicar detalhadamente as principais formas que uma máquina pode aprender, elas irão compor o ecossistema do aprendizado de máquina, de forma que seja possível resolver diferentes problemas baseado nas abordagens que mais se encaixem a elas. 

2.1. Aprendizado Supervisionado

O aprendizado supervisionado é um paradigma de aprendizado de máquina, que tem como objetivo adquirir informações de relacionamento entre entrada e saída de um sistema, baseado em um conjunto de amostras de treinamento. 

Um algoritmo de aprendizado supervisionado analisa os dados de treino e produz uma função inferida que será utilizada para mapear novos exemplos. Para deixar menos abstrato, vamos considerar um exemplo, a classificação de e-mails como spam.

Provavelmente, você utiliza e-mail e sabe que conteúdos maliciosos são quase sempre enviados para uma pasta específica, com o objetivo de te proteger. Mas como isso acontece? 

A experiência que permite você não precisar classificar quais e-mails são maliciosos ou não, é proporcionada por um modelo de classificação que é baseado em entradas rotuladas. Nessas entradas, possuímos e-mails classificados como confiáveis ou não, dessa forma, o modelo irá aprender a reconhecer a classe que um novo dado pertence baseado no que já aprendeu sobre esses dados rotulados.

Porém, pode te bater aquela curiosidade, por que então o meu provedor de e-mail ainda me pergunta se a mensagem que eu recebi é ou não um spam? Apesar de já existirem modelos confiáveis treinados em cima de enormes conjuntos de dados, a sua validação permite que esse modelo seja aprimorado cada vez mais, permitindo que essa não seja uma preocupação sua.

O exemplo acima demonstra como seria um problema de Classificação, mas vale lembrar que existe outra gama de problemas que podem ser denominados como problemas de Regressão. Para que não sobrem dúvidas do que se trata cada um desses tipos de problemas, eles estão definidos abaixo:

Classificação 

A classificação é o processo de categorizar um determinado conjunto de dados em classes. No exemplo da classificação de e-mails como spam, teríamos um exemplo de classificação binária, no qual o modelo através dos dados fornecidos, precisaria gerar como resposta se o e-mail é spam ou não.

Alguns dos algoritmos mais famosos são:

KNN
Naive Bayes
Logistic Regression
Support Vector Machines
Decision Trees

Regressão

Os modelos de regressão são utilizados quando queremos prever valores, por exemplo, prever o preço de uma casa ou o número de produtos que serão vendidos em determinado mês.

Os modelos de regressão são dos mais diversos e suas possibilidades são descritas pela imagem abaixo:


Imagem 1-Introdução a regressão e suas possibilidades. Link de acesso.

Analisando a imagem acima podemos perceber que a primeira subdivisão dos modelos de regressão diz respeito ao número de variáveis envolvidas, modelos de regressão simples envolvem apenas uma variável e os múltiplos duas ou mais. Em seguida, para cada um dos tipos descritos ainda existe outra ramificação que divide esses modelos em lineares ou não lineares. 

Alguns modelos são famosos para realizar regressão, são eles:

Linear Regression
Polynomial Regression
Logistic Regression
Principal Components Regression (PCR)
2.2. Aprendizado Não Supervisionado

O aprendizado não supervisionado consiste em treinar uma máquina a partir de dados que não estão rotulados e/ou classificados. Os algoritmos que fazem isso buscam descobrir padrões ocultos que agrupam as informações de acordo com semelhanças ou diferenças, por exemplo. 

Para que isso fique mais claro, vamos imaginar um algoritmo de aprendizado não supervisionado, que receba uma imagem contendo cachorros e gatos.

Ao receber essa imagem nada se sabe sobre as características que cada animal possui, ou seja, não é possível categorizá-los. Porém, esse algoritmo será responsável por descobrir semelhanças, padrões e/ou diferenças que permitam diferenciar cães e gatos.  

No exemplo citado anteriormente utilizamos uma técnica chamada de agrupamento (Clustering), porém existem outras técnicas como regras de associação (Association Rules) e redução de dimensionalidade (Dimensionality Reduction). Falaremos um pouco de cada uma delas abaixo.

Agrupamento

A técnica de agrupamento como explicado no exemplo anterior, consiste em agrupar dados não rotulados com base em suas semelhanças ou diferenças.  Esses algoritmos de agrupamento ainda podem ser subdivididos em agrupamentos exclusivos, sobrepostos, hierárquicos e probabilísticos.

Regras de Associação

Ao usar as regras de associação, buscamos descobrir relações que descrevem grandes porções dos dados. A associação é muito utilizada em análises de cestas de compras, no qual a empresa pode tentar entender relações de preferências de compras entre os produtos.

Quando falamos de algoritmos para gerar regras de associação os principais são: Apriori, Eclat e FP-Growth.

Redução de dimensionalidade

Existem casos nos quais ao estudar um conjunto de dados, podemos encontrar nele um grande número de recursos (dimensões). Por mais que existam situações onde isso é positivo, o excesso pode impactar o desempenho dos algoritmos causando, por exemplo, o overfitting. 

Utilizando a técnica de redução de dimensionalidade, será feita uma redução no número de recursos, de forma que torne-os gerenciáveis por parte do modelo, além de preservar a integridade dos dados.
E para executar essa tarefa existem algumas técnicas que podem ser utilizadas, como: Missing Values Ratio, Low Variance Filter, High Correlation Filter, Random Forests / Ensemble Trees, Principal Component Analysis (PCA), Backward Feature Elimination e Forward Feature Construction.

2.3. Aprendizado por reforço

Para entendermos melhor como funciona o aprendizado por reforço usaremos a seguinte imagem para ilustrar qual é o princípio.


Imagem 2- Modelo de aprendizado por reforço. Link de acesso.

O primeiro passo é definir os elementos presentes na imagem, o agente (Agent) é aquele que toma as decisões com base nas recompensas e punições, esse agente pode realizar uma ação (Action) que irá variar de acordo com o contexto. O ambiente (Environment) é o mundo físico ou virtual em que o agente opera, a recompensa (reward) é o feedback do ambiente baseado na ação tomada e o estado (state) é a situação atual do agente.

A imagem acima demonstra um exemplo de como o aprendizado por reforço pode ser utilizado. Nesse caso, o robô é o nosso agente e ele está situado no estado inicial do nosso ambiente, que é representado pelo “labirinto” que o robô terá de percorrer. Desta forma, o objetivo é chegar ao diamante evitando os obstáculos (fogueiras). 

Definido o objetivo, o robô deve buscar pelo melhor caminho possível para chegar até o diamante. Dessa forma, a cada ação do robô, ele poderá caminhar em uma determinada direção, caso ele escolha corretamente, ele irá inserir pesos diferentes, para diferentes respostas. Com isso, espera-se que ao final o robô consiga realizar seu objetivo de forma que obtenha a maior recompensa cumulativa.

3. Aplicações

Lembra quando eu mencionei no início do artigo que as diferentes abordagens do aprendizado de máquina contribuem na realização de tarefas árduas e melhoria de processos? Nessa seção iremos conhecer em quais cenários o machine learning está inserido.

Diagnósticos médicos

Na área médica as técnicas de machine learning são utilizadas para fazer o reconhecimento de doenças. Com o crescimento da tecnologia tem sido possível construir modelos 3D que podem prever a posição exata de lesões no cérebro, permitindo a detecção de tumores e outros diagnósticos relacionados muito mais fácil. 

Além disso, muito trabalho vem sendo feito com imagens como, por exemplo, o reconhecimento de padrões que identificam câncer de pulmão, de pele, dentre outros.

Detecção de fraudes online

Se considerarmos uma instituição financeira que lida com milhares de transações por dia, ela está sujeita a fraudes a todo momento e sabendo que avaliar toda essa quantidade de operações seria totalmente exaustivo e ineficiente, modelos de machine learning são criados para que possam ser detectadas anomalias nas transações. 

Para ficar mais claro vamos supor que uma pessoa tenha um cartão de crédito de um banco com limite de 2000 reais, porém, ela tem um histórico de uso mensal de no máximo 800 reais, se por acaso em um determinado dia houver uma compra no seu cartão no valor de 2000 reais, o modelo de detecção de fraudes irá perceber que essa compra não se encaixa no seu padrão e, com isso, o banco será notificado colocando a transação em espera.

Sistemas de recomendação

Presente nos mais diversos tipos de aplicações, os sistemas de recomendação tiram aquela velha necessidade de procurar tudo aquilo que desejamos. No sistema de varejo, por exemplo, se você tiver cadastro na plataforma de algum desses varejistas você terá um sistema de recomendações de produto ao seu dispor, ele cria essas recomendações baseado em compras anteriores, históricos de navegação, dentre outras informações complementares.

Dessa forma, quando você está com o carrinho de compras e percebe que esqueceu mais um item da compra que estava planejando, provavelmente ele estará em uma seção destinada a seus possíveis interesses.

Reconhecimento de fala

Provavelmente o exemplo mais famoso para o reconhecimento de fala são os assistentes de voz. Então, a Siri da Apple, Alexa da Amazon, Cortana da Microsoft, dentre outros assistentes de voz usam machine learning através de técnicas de processamento de linguagem natural (NLP) para reconhecerem a fala, posteriormente transformam essa fala em números para que possam formular uma resposta de acordo.

4. Conclusões

Neste artigo você deu os primeiros passos dentro de uma área gigante, passamos pelos conceitos iniciais, vimos as formas de aprendizado e alguns dos modelos mais famosos de machine learning, além de conhecer diversas aplicações. Dessa forma, agora você pode destinar seus estudos para a forma de aprendizado e aplicações que mais te chamaram atenção.

Portanto, se você quer saber mais sobre machine learning e outros tópicos quentes na área de tecnologia, recomendo acompanhar todas nossas redes sociais. Não se esqueça também de curtir e compartilhar, seu apoio é muito importante!



Artigo escrito por Lucas Natali Magalhães Silva. Revisado por Prof. Rodrigo César Pedrosa Silva."
Introdução à arquitetura serverless com Amazon Lambda,http://www2.decom.ufop.br/terralab/introducao-a-arquitetura-serverless-com-amazon-lambda/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/06/Blog.png,"Neste artigo iremos falar sobre a arquitetura serverless, cuja tradução literal seria arquitetura “sem servidor”. Para um introdução prática, apresentaremos uma visão básica do serviço serverless Amazon lambda, do serviço de armazenamento Amazon S3, e do Serverless framework,  seguida de um exemplo prático em Python utilizando todas essas ferramentas.

Amazon lambda

Podemos dizer que o AWS Lambda é um serviço de computação serverless. Serverless é uma maneira de descrever os serviços, práticas e estratégias que permitem desenvolver aplicações mais ágeis.

As tarefas de gerenciamento de infraestrutura são executadas pela provedora da infraestrutura, neste caso a própria Amazon. Desta forma, você pode se concentrar em  apenas escrever códigos do seu serviço, a partir disso foi criado o nome serverless, em que a ausência da preocupação com os servidores tornou-se uma realidade para os desenvolvedores.

A computação serverless tem algumas vantagens e desvantagens. As vantagens são escalabilidade, facilidade de uso, baixo custo e uma grande quantidade de linguagens de desenvolvimento suportadas. Algumas desvantagens estão relacionadas ao Debug e aos testes que ficam mais complicados. Além disso, processos lambda com longo tempo de execução podem resultar em altos custos.

Amazon S3

O Amazon S3 ou simple storage service, é um serviço de armazenamento em que você pode guardar ou fazer o download de qualquer quantidade de dados de qualquer lugar da web.

O S3 é um serviço simples de ser utilizado, escalável e o próprio provedor do serviço escala ele para você, altamente durável, um custo baixo de $ 0,023 usd por GB + solicitações PUT, COPY, POST, LIST (por 1.000 solicitações) e também possui  integração com a maioria dos outros serviços.

Serverless framework

O Serverless é um framework utilizado para construir aplicações com funções lambda, ele cria e gerencia os servidores necessários para executar suas funções e responder aos eventos desejados, com isso você pode se concentrar melhor na parte da programação da lógica de sua aplicação.

Para iniciar, é preciso instalar o Serverless na sua máquina. Neste tutorial, todo o processo é realizado no Linux. Por ser um framework criado em NodeJS, utilizaremos o gerenciador de pacotes npm para fazer a instalação. (Caso você não tenha  o NodeJS instalado, faça o download)

Abra seu terminal e digite o comando:

npm install -g serverless


Agora precisaremos configurar as credenciais da AWS no seu Serverless

Faça login na AWS Console
Pesquise pelo serviço IAM, que é a forma de controlar acessos aos recursos da AWS.

Vá em Usuários > Adicionar usuário

Coloque um nome para seu usuário e adicione o Acesso Programático, e vá para o próximo passo

Vá até Anexar políticas existentes e adicione o AdministratorAccess, depois vá para o próximo passo

Continue avançando até finalizar a criação do usuário

Sua chave será criada e conterá um ID de acesso e uma Chave Secreta, como mostra a imagem a seguir, e você irá utilizá-la no próximo passo

Voltando a seu terminal, digite o seguinte comando:

serverless config credentials -o --provider aws --key=<ID da chave de acesso> --secret <Chave de acesso secreta>


Agora, iremos gerar o seu primeiro template no Serverless para uma função em Python. No terminal, insira o comando a seguir, que contém qual tipo de template será usado e a pasta em que ele será armazenado (a pasta não pode existir previamente, ela será criada pelo próprio comando):

serverless create --template aws-python3 --path <nome da pasta>


Finalmente, será gerado um arquivo .yml e um .py. Então, poderemos prosseguir para o nosso exemplo prático.

Exemplo prático

Para demonstrar o uso dessas ferramentas, faremos um exemplo prático: Para ser mais simples, invocaremos a função lambda manualmente sem utilizar gatilhos por enquanto. Essa função será encarregada de criar um arquivo de texto escrito ‘Hello World’ e colocá-lo dentro do AWS S3 Bucket, que é o serviço de armazenamento de objetos/arquivos.  Esse é um exercício bem simples que serve apenas para mostrar como criar um certo fluxo de dados na arquitetura serverless.

No arquivo serverless.yml, iremos configurar:
Região (São Paulo)
Tamanho da memória da função (256 MB)
Permissões de usuário (Pegar e colocar objetos no S3)
Função
Recurso Bucket s3.

e dessa forma o arquivo serverless.yml ficará assim:

service: exampleserverless
 
frameworkVersion: '2'
 
provider:
  name: aws
  runtime: python3.8
  lambdaHashingVersion: 20201221
  region: sa-east-1
  memorySize: 256
  iamRoleStatements:
    - Effect: Allow
      Action:
        - s3:GetObject
        - s3:PutObject
      Resource:
        - ""arn:aws:s3:::*""
 
functions:
  hello:
    handler: handler.hello
 
resources:
  Resources:
    BucketUpload:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: exampleterralabserverless

Partindo agora para o arquivo handler.py, criaremos a nossa função:
Importe a biblioteca de gerenciamento da AWS chamada “boto3”
Dê os nomes necessários aos arquivos e pastas
Escreva em um arquivo temporário a frase ‘Hello World’
Insira o código para adicionar esse arquivo temporário no AWS S3

seguindo esses passos, o seu código deverá ficar dessa maneira:

import json
import boto3
 
def hello(event, context):
 
    #Definindo os nomes
    caminho_temporario = ""/tmp/arquivotemp.txt""
    arquivo_S3 = ""hello.txt""
    nome_S3 = ""exampleterralabserverless""
    txt_hello = "" Hello World!""
 
    #Criando o arquivo temporário
    arquivo = open(caminho_temporario, 'w')
    arquivo.write(txt_hello)
    arquivo.close()
 
    #Adicionando o arquivo no AWS S3
    s3 = boto3.client('s3')
    with open(caminho_temporario,'rb') as arq:
        s3.upload_fileobj(arq, nome_S3, arquivo_S3)
 
 
    body = {
        ""message"": ""Go Serverless v1.0! Your function executed successfully!"",
        ""input"": event
    }
 
    response = {
        ""statusCode"": 200,
        ""body"": json.dumps(body)
    }
 
    return response


Agora apenas resta realizar o deploy do Serverless e testar a função. Para realizar um deploy, volte a seu terminal e digite:

serverless deploy -v


Ao finalizar esse processo com sucesso, podemos invocar nossa função, também via terminal com o seguinte comando:

serverless invoke -f hello -l


E pronto, sua função foi executada com sucesso. E para conferirmos se deu certo, podemos ir ao console da AWS, e pesquisar pelo serviço S3.

Se seu processo estiver funcionado, o serviço AWS S3 deverá conter 2 buckets, sendo um o que estará armazenando o seu próprio código lambda feito no tutorial, e o outro será o que definimos como exampleterralabserverless, o qual armazena o arquivo hello.txt gerado pela função. Abra o exampleterralabserverless para conferir se nossa função executou corretamente, ou seja, deverá conter o arquivo hello.txt

Considerações finais

A arquitetura Serverless hospedará seus serviços e funções sem necessitar de configurações de hardware. A sua aplicação será executada e todas suas dependências já estarão instaladas de forma nativa. Com a ascensão do mundo da nuvem, esse tipo de arquitetura vem crescendo bastante e é muito recomendado para execuções rápidas de funções.

Apesar de ser muito recomendado para certos tipos de aplicação, é sempre bom lembrar que analisar o seu problema e a arquitetura do seu produto é essencial para tomar a melhor decisão para a infraestrutura de seu projeto.

Gostaria de aprender mais sobre infraestrutura na nuvem? Você atua profissionalmente na área? Tem algo a acrescentar nessa discussão? Alguma dúvida sobre aquilo que apresentamos? O seu comentário será muito bem-vindo! Comente, compartilhe esta publicação e não deixe de acompanhar nosso blog e redes sociais!

Referências

Serverless com IBM Cloud Functions: como funciona esse tal “Serverless”?

Serverless com IBM Cloud Functions: como funciona esse tal “Serverless”?

Serverless AWS

Computação sem servidor – Amazon Web Services

Serveless (Dicionário do programador)

Serverless // Dicionário do Programador

Código prático Rocket Seat

Serverless com NodeJS e AWS Lambda | Diego Fernandes

Artigo escrito por Guilherme Carolino e Guilherme Ferreira Rocha. Revisado por Prof. Tiago Carneiro."
Como escrever artigos para um Blog?,http://www2.decom.ufop.br/terralab/como-escrever-artigos-para-um-blog/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/06/Arte-em-acao-3.png,"Semanalmente, um artigo é publicado em nosso blog. Estes artigos abordam e divulgam, em linguagem simples e objetiva, os conhecimentos tecnológicos ou científicos que o TerraLAB produz ou utiliza em sua rotina. Estes artigos podem ser um tutorial, uma notícia ou um artigo. Você já parou pra pensar como estes artigos são feitos? Qual estrutura seguem? Como escrever um desses artigos e contribuir?

Esta semana, o TerraLAB responde a essas questões neste tutorial. Ele explica como escrever um artigo para nosso blog e descreve, em cada tópico, as instruções que alguém deve seguir para desenvolver seu artigo. Ele também oferece dicas rápidas que ajudam a memorizar estas instruções.

INSTRUÇÕES GERAIS

Seguem alguma instruções gerais sobre como ler este tutorial e de como escrever um artigo para nosso blog: 

Em cada tópico, você encontrará instruções como desenvolvê-lo da melhor forma, além de dicas rápidas para aproveitá-los melhor;
Ao final, você terá uma estrutura de artigo, sabendo o que deve conter em cada parte: introdução, desenvolvimento e conclusão. Como uma boa redação conta uma história, tem início meio e fim,  o seu artigo não precisa ser separado nesses três tópicos, mas o conteúdo escrito precisa contemplar todos esses tópicos.
A ideia é que a partir dessa estrutura, seja possível desenvolver um conteúdo que transmita o seu conhecimento;
Não se esqueça de que um bom conteúdo é aquele que está alinhado com o interesse e linguagem do seu público alvo (leitor) e com o objetivo do TerraLAB. O TerraLAB tem como público alvo: Estudantes, Pesquisadores, Profissionais de TI e Empresas. Tenha um deles em foco e converse com ele. Dificilmente, um artigo atingirá vários sem parecer desinteressante para pelo menos um. Aqui você encontra algumas dicas sobre o que olhar antes de começar, a escrever um artigo;
Sem ideias de conteúdo? Liste o que você mais gosta, aquilo que você conhece bem, os desafios que superou e mostre que você é capaz de ter ideias incríveis.
AUTORES (ordem alfabética)

Apresente os nomes dos autores em ordem alfabética: 

aaaaaaaaaaaaa
bbbbbbbbbbbb
ccccccccccccccc
PALAVRAS-CHAVE

Apresente as palavras chaves na ordem de importância: 

Escrita;
Artigos;
TerraLab;
Blog.
DESCRIÇÃO GERAL (máximo 220 caracteres)

Forneça um resumo do conteúdo de seu artigo, em no máximo 220 caracteres. Este limite é imposto por algumas redes sociais e deve ser respeitado para evitar que alguém, que conheça menos do assunto abordado, precise retrabalhar seu texto para resumi-lo por você.

Exemplo: Em meio ao isolamento social causado pela pandemia do Coronavírus, muitos trabalhadores estão mantendo suas atividades via home office. Logo, uma lista com apps e ferramentas úteis a essa atividade se faz valiosa nesse cenário. Destas, vale a pena destacar e mencionar: Trello, Evernote, Wunderlist, Google Meet, Github, Pomodoro, Notion, TeamViewer, Pocket e Forest.
Passo 1. Planejamento

Utilize esse espaço para documentar questões estratégicas do artigo..

Público alvo: 
Possível data de publicação: 
Referências: 
Tema: 
Pergunta que guiará a produção de conteúdo: 
Passo 2. Título do artigo

Insira aqui 3 a 5 ideias de título. Não se esqueça de incluir a palavra-chave principal do artigo no título! Este item garante que os sites de busca orientem pesquisas para o seu texto.

 Um título eficiente precisa ter:

Tamanho ideal: Não pode ser muito longo, mas simples e direto, que diga aquilo que é preciso ser dito;
Apontar utilidade: Crie uma frase que aponte a utilidade de seu texto para fornecer uma solução (Ex: “ Passo a passo para implantar a Análise SWOT em sua empresa”);
Aguçar a curiosidade do leitor: Você pode tentar despertar a curiosidade do leitor para que ele abra o link de seu artigo ( Ex:  “segredos de como aumentar a produtividade de seus negócios!”);
Utilizar perguntas: Usar perguntas como forma de indicar que seu texto irá respondê-las (Ex: “Como emitir notas fiscais eletrônicas?”);
Lista com números: Outro tipo de título eficiente é aquele que utiliza números para apontar uma lista    (Ex: “5 coisas que você precisa saber antes de viajar para o exterior”).
Passo 3. Introdução

Descreva aqui o problema que você quer solucionar e de que forma isso impacta o dia a dia das pessoas e das empresas. Sua introdução deve fazê-la da maneira mais atrativa possível, para garantir que o leitor continuará a leitura. Então contar uma história que possa despertar a identificação do leitor com o conteúdo, fazendo uma citação de uma fonte confiável e relevante sobre o assunto, além de usar dados estatísticos, pode fazer com que o leitor aprenda a realizar perguntas, em sua mente, que serão respondidas ao longo do artigo. Então entenda seu público alvo, escolha um assunto que você goste e utilize imagens criativas e didáticas como as abaixo:


Imagem 1- Tente encher os olhos do seu leitor com boas imagens, bem distribuídas e sugestivas no texto

De uma forma muito prática, após apresentar o problema que tratará e apresentar suas motivações que o levaram a isto, em pelo menos um parágrafo (no mínimo), inicie o próximo parágrafo enunciando claramente o objetivo do seu artigo. Por exemplo, “O intuito deste artigo é…” ou “Este artigo tem por objetivo…”. 

Passo 4. Desenvolvimento em tópicos

Pense em 3 ou mais formas de resolver esse problema e as escreva aqui resumidamente, na forma de itens (bullets). Esses itens serão os subtítulos do seu artigo. Este exercício lhe ajudará a organizar seu conhecimento na história mais objetiva possível. Após levantar esses itens, descreva, em tópicos, qual será o conteúdo dentro de cada um.

No mundo online, as pessoas costumam ser multitarefas. Ao mesmo tempo em que visitam seu site, conversam com os amigos em alguma rede social, respondem seus e-mails, procuram alguma informação em mecanismos de busca etc. Diante deste fato,  o que você pode fazer para tornar seu conteúdo mais legal?

Quebre o texto em parágrafos curtos (um parágrafo trata apenas de um único assunto);
Use subtítulos;
Use bullet points (como estes);
Use negrito, itálico, citações e outros estilos (mas cuidado para não poluir o texto);
Incorpore outras mídias (imagens, vídeos, áudios, tweets, posts no Facebook e Instagram);
Uma maneira simples de desenvolver tópicos é escrever artigos em formatos de lista. É um tipo de artigo que costuma gerar bastante resultado.

Tente adicionar um vídeo em seu artigo! Quando é possível, pode-se melhorar a absorção do conteúdo abordado e trazer outras visões e experiências que dificilmente poderiam ser expressas por meio textual .

Vídeo 1- Fizemos um vídeo para acompanhar este artigo e tornar mais intuitiva a absorção do seu conteúdo.

Passo 5. Considerações finais/Conclusão

Esse é o momento de encerrar o texto e recapitular as ideias apresentadas até então. Fale aqui o que o leitor deverá esperar ao aplicar o que foi passado nos tópicos anteriores. Após produzir o conteúdo, é preciso terminá-lo. Não é porque você se empenhou até agora que pode escrever qualquer coisa no final do seu texto, deixando aquela sensação de que faltou alguma coisa, como aquele filme ou livro que não conta bem o final da história.

Por isso, conclua bem o seu conteúdo. É bem comum utilizarmos um capítulo de “conclusão” para fechar o texto, amarrar bem as ideias e sugerir ações a partir dos aprendizados do conteúdo. A ideia é recapitular o que foi dito, reforçar o objetivo principal do artigo e se possível criar links para outros artigos do TerraLAB. Qual a oferta final do conteúdo? O que o leitor pode fazer em seguida? Se o seu artigo for sobre um assunto introdutório, indique algum conteúdo mais profundo. Se for um artigo avançado, indique alguma oferta de meio ou fundo de funil, como um estudo de caso ou conversa com um consultor, para trazer mais credibilidade ao seu trabalho. O mais importante: Divirta-se.

Passo 6. Resumo/Linha fina

Como você diria, em poucas palavras, do que se trata o artigo? O resumo é uma forma de complementar o título usando mais palavras. Dica: Esse resumo também pode ser usado para compor a meta descrição do post. Para isso, siga as instruções a seguir e ele será usado para descrever as peças gráficas publicadas nas redes sociais.

Forneça o resumo na forma de quatro parágrafos, em linguagem simples e objetiva, conforme sugerimos a seguir:

Parágrafo 1 – O que é o artigo; 
Parágrafo 2 – Detalhes do artigo; 
Parágrafo 3 – Relacionamentos e hashtags; e 
Parágrafo 4. Um convite pessoal (CTA – call to action) e link para o artigo no blog.
Considerações Finais

Para que os componentes do TerraLAB construam bons artigos, nossa equipe de marketing buscou informações para orientar e melhorar as técnicas de redação de nossos colaboradores. 

Nós acreditamos que este assunto pode ser de interesse geral, visto que escrever bem é uma necessidade profissional e académica. Por essa razão, tivemos a iniciativa de tornar públicas nossas diretivas de escrita. Esperamos que sejam úteis para aquelas pessoas que necessitem desenvolver textos de divulgação científica, tecnológica em quaisquer áreas do conhecimento. Além disso, acreditamos, que as ideias expostas também possam ajudar na redação de textos em geral.

Finalmente, acreditamos que um bom artigo resulta mais facilmente de assuntos que os autores gostem e que, principalmente, é essencial se expressar de maneira a cativar o interesse do leitor.

A equipe do TerraLAB também acredita que a leitura de assuntos diversos e de textos que utilizem diferentes estilos literários podem contribuir para aumentar a criatividade dos autores na construção de conteúdo, assim como ampliar as técnicas de redação e auxiliar a escolha de estilos de escrita.  

Esperamos que tenha gostado desse artigo. Avalie este conteúdo, entre em contato conosco e compartilhe nossos artigos, para que juntos consigamos disseminar conhecimento. Conheça nossas redes sociais!

Artigo escrito por Josemar Félix. Revisado por Prof. Tiago Carneiro."
Uma introdução ao Git e Gitflow,http://www2.decom.ufop.br/terralab/uma-introducao-ao-git-e-gitflow/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/06/Arte-em-acao-2.png,"Uma das principais ferramentas utilizadas pelos desenvolvedores hoje em dia, são os controladores de versão de código fonte, entre eles temos o Subversion, CVS, e o mais popular deles, o Git. Estas ferramentas permitem que todas as mudanças realizadas sobre o código fonte de um software sejam rastreáveis e armazenadas sem sobrescrever umas às outras. Desta forma, é possível recuperar versões do código e verificar os autores e motivações das mudanças realizadas.

O intuito deste artigo é demonstrar o uso da ferramenta Git na prática, por meio de exemplos, trabalhando localmente e remotamente, seguindo o fluxo de trabalho estabelecido pelo Gitflow.

Conceitos e funcionamento básico

Antes de seguirmos, é  preciso que alguns conceitos básicos sejam entendidos para compreendermos o funcionamento básico do Git. Um repositório de código é um diretório onde os arquivos de código fonte do seu projeto e o histórico completo das revisões ficam armazenados. Ele pode ficar remotamente em um projeto no Git ou localmente em sua máquina.  

Para que a ferramenta de versionamento Git seja eficaz no controle de versões de um projeto de software, ela precisa ser utilizada de uma maneira específica.  O Gitflow é apenas uma ideia abstrata de como o fluxo de trabalho de uma equipe de desenvolvimento de software deve acontecer através do Git. Ele dita que tipos de ramificações devem ser configuradas em um projeto de software, quando as ramificações devem acontecer e como fazer a mesclagem de código. A figura baixo traz um exemplo de um repositório Git seguindo o fluxo de trabalho Gitflow. Podemos enxergar o repositório de código como uma árvore de versões de código. Inicialmente, quando criamos um repositório, há somente um ramo (do inglês, branch), com o nome “master” que representa o ramo principal (versão v 0.0.0). Posteriormente a criação do repositório, seguindo o modelo de fluxo de trabalho Gitflow, devemos criar um ramo com o nome “develop”, a partir do branch master. É este ramo que receberá todas as modificações para correção ou evolução de funcionalidades já existentes, assim como para o desenvolvimento de novas funcionalidades. O ramo “staging” também é criado a partir do ramo “develop” e é utilizado para colocar em testes as modificações de código realizadas pelos   desenvolvedores. Somente quando as modificações são aprovadas em todas as rotinas de testes existentes, isto é, testes regressivos, a versão do ramo “staging” é mesclada à versão “master”, dando origem a uma versão evoluída do software (v 0.0.1). Este processo de verificação por testes regressivo confere qualidade ao software ao garantir que a nova versão do software passa em todos os testes da versão anterior e também em novos testes que verificam as modificações realizadas no código.

Idealmente, um desenvolvedor deve criar um ramo a partir do ramo “develop” para cada correção ou evolução do código que pretende realizar. Desta maneira, ele poderá trabalhar sem a interferência do trabalho realizado por outros desenvolvedores e sem interferir no trabalho dos demais. Aṕos certo período de desenvolvimento, quando um desenvolvedor julgar que seu trabalho está concluído,  ele deverá mesclar seu código com o ramo “staging” para colocar as modificações em teste. Somente quando o ramo “staging” for aprovado em todos os testes regressivos, seu conteúdo dará origem a uma nova versão do software. Para saber mais sobre o Gitflow e como ele ajuda a automação do processo de desenvolvimento de software, leia o artigo “Entendendo o funcionamento do CICD dentro do Git flow”. Para conhecer mais sobre as regras de versionamento e atribuição de tag ou número de versões, leia o artigo “Como Funciona o Versionamento de Software e como criar Tags no Git”

Antes de demonstrarmos o uso do Git por meio de um exemplo prático, é preciso que você conheça alguns conceitos básicos:

Staging área – Local temporário onde são armazenadas todas as alterações que serão adicionadas no próximo commit;
Working directory – Todos os arquivos que estão sendo trabalhados no momento;
Commit – É um instantâneo (do inglês, snapshot) das modificações adicionadas na staging area, persistindo-as no repositório local;
Branch – É uma ramificação do código, apenas um ponteiro que aponta para um commit e tudo anterior a esse commit. O branch padrão é o master;
Head – É um ponteiro que aponta para algum commit ou alguma branch; e
Merge – Ação de juntar os commits de dois branches.

No Git um arquivo pode estar em um desses 4 estados:

untracked – Arquivos que não estavam no último commit;
unmodified – Arquivos não modificados desde o último commit;
modified – Arquivos modificados desde o último commit; e
staged – Arquivos preparados para comitar.

A figura abaixo mostra como os arquivos de código em um determinado repositório migram de um estado para o outro. Quando inseridos manualmente em um repositório local, isto é, no working directory, os arquivos são criados no estado untracked. Podemos alterar o estado de um arquivo para staged, através do comando git add <nome_do_arquivo>, isto informa ao Git que este arquivo deve ser versionado futuramente. Para removermos um arquivo da staging area, utilizamos o comando git rm –cached <nome_do_arquivo>, evitando seu versionamento. Quando editarmos qualquer arquivo em nosso working directory, o mesmo passará do estado unmodified, para o estado modified. Quando modificações em um arquivo de código são persistidas no repositório local pelo comando git commit, este arquivo transita do estado staged para unmodified.

Crie um repositório local para seu software

Agora que já entendemos um pouco sobre o git, vamos partir para a prática. A primeira coisa que devemos fazer quando queremos trabalhar com git, é criar um repositório, fazemos isso com o comando git init. Crie um diretório local para o código fonte do seu software e, em seguida, execute este comando. Esse comando cria um subdiretório .git com todos os metadados necessários. No exemplo abaixo, criamos o diretório “hello_world”.

Vamos criar um arquivo README.md em nosso projeto. Este arquivo é apenas um arquivo texto que, em geral, acompanha projetos de software explicando questões iniciais para seu uso ou instalação  Depois, vamos adicionar este arquivo à staging area para que faça parte do próximo commit. Fazemos isso com o comando git add. O comando touch do Linux é usado para criar arquivos vazios, além de alterar o registro de data e hora (timestamp) de arquivos ou pastas.

O comando git status pode ser utilizado para verificar que o arquivo README.md foi adicionado a staging area.

Agora vamos realizar o commit de nossas alterações, para que os arquivos na staging area passem a ser versionados. Só arquivos que sofreram commit terão suas versões controladas pelo Git. Para isso utilizamos o comando   git commit -m “Mensagem do commit”. A mensagem passada como parâmetro deve ser utilizada com inteligência para que seja útil ao time de desenvolvimento, ela deve justificar e explicar as alterações pelas quais o código fonte passou.

Utilizando o comando git log podemos ver o histórico de commits realizados em nosso repositório.



Ignore os arquivos que não lhe interessa versionar

Muitas vezes, em um projeto de software, haverá arquivos binários que não devem ser adicionados a um repositório de código, evitando que a ferramenta Git tente realizar o controle de versão dos mesmos. Para exemplificar, vamos criar um arquivo main.cpp que imprime a mensagem “Hello World!” na tela e, em seguida, compilá-lo.

Após compilarmos, podemos verificar que foi gerado o arquivo main que é nosso executável. 

Porém, nós não queremos realizar um commit com esse arquivo. Para resolver isso, vamos criar o arquivo .gitignore onde vamos colocar todos os arquivos que queremos ignorar ao realizar um commit.  O comando echo é utilizado apenas para adicionar a nova linha “main” ao arquivo .gitignore.

Agora vamos salvar nossas alterações adicionando os arquivos main.cpp e .gitignore no staging. Podemos fazer isso através do comando “git add .“, que adiciona todos os arquivos modificados no staging.

Se checarmos o estado do nosso repositório, podemos observar que todas as modificações foram adicionadas ao staging.

Se tentarmos agora adicionar o executável main no staging, recebemos um aviso. 

Se quisermos adicionar um arquivo ignorado ao staging, devemos utilizar a opção -f no comando git add, para forçar a adição. 

Agora podemos realizar o commit das nossas alterações

Utilizando o comando git log, podemos observar que agora temos dois commits. 

Restaurando versões anteriores do código

Mas e se nos arrependermos de algum commit e desejarmos desfazê-lo? Bom, para isso temos dois comandos, git reset e git revert. 

No comando git reset existem três opções, soft, mixed e hard. A opção soft, move o HEAD para o commit indicado, mas mantém o staging e o working directory inalterados. A opção mixed, move o HEAD para o commit indicado, altera o staging e mantém o working directory. A opção hard faz com que o HEAD aponte para algum commit anterior, mas também altera a staging area e o working directory para o estado do commit indicado, ou seja, todas as alterações realizadas após o commit ao qual retornamos serão perdidas. Isso não é recomendável se as alterações já tiverem sido enviadas para o repositório remoto. Nesse caso devemos utilizar o git revert.

Vamos supor que queremos desfazer as alterações do último commit em nosso exemplo, utilizando o comando git reset com a opção hard. Dessa forma precisamos indicar ao comando git reset o código SHA-1 do commit ao qual queremos que o HEAD aponte. Outra forma de utilizar o comando git reset, é indicando quantos commits queremos retornar o HEAD, fazemos isso com o comando git reset HEAD~n. O parâmetro HEAD~n, nos indica que queremos posicionar o HEAD para n commits atrás. Por exemplo, para retornar para o commit anterior usamos HEAD~1.

Na última linha da  figura acima, podemos observar que o arquivo main.cpp que havíamos adicionado no segundo commit, não está mais em nosso working directory.

Por outro lado, o comando git revert cria um novo commit com as alterações do commit indicado. Utilizando git revert em nosso exemplo, nosso repositório ficaria assim

Como podemos verificar, um novo commit foi criado revertendo para as alterações do commit f861569.

Entendendo o uso de um repositório remoto

Agora que já aprendemos como utilizar o Git no repositório local, vamos entender como utilizamos o Git com um repositório remoto seguindo o fluxo de trabalho Gitflow.

Vamos supor que tenhamos um repositório em um projeto do Gitlab. Nosso primeiro passo é utilizar o comando git clone para baixar o repositório em nossa máquina. 

Nosso repositório está vazio, então agora vamos realizar nosso primeiro commit. 

Vamos subir as alterações para o repositório remoto no Gitlab. Fazemos isso com o comando git push -u origin master.

Podemos observar que o ramo master foi criado em nosso repositório no Gitlab. Agora precisamos criar os ramos develop e staging. Para isso podemos utilizar o comando git checkout -b develop master, que cria o ramo develop a partir do ramo master. Realizamos o mesmo procedimento para o ramo staging com o comando git checkout -b staging develop. Após criarmos os ramos, utilizamos os comandos git push -u origin develop e git push -u origin staging para subir as alterações no repositório remoto.

Utilizando o comando git branch podemos verificar os ramos em nosso repositório local. Para verificarmos os ramos remotos, utilizamos o comando git branch -r.

Resolvendo issues no GitLAB

Agora vamos supor que durante as reuniões realizadas com seu time, seu líder (em geral, um Product Owner) definiu e atribuiu a você uma funcionalidade que deverá ser desenvolvida no próximo ciclo de desenvolvimento (sprint), que em nosso exemplo é a funcionalidade  “Add function to print hello world”. Para isso, no GitLab, ele adicionou essa issue ao board “Sprint backlog”, conforme ilustra a figura a abaixo.

Antes de começar a implementar esta funcionalidade, precisamos a nova issue para o board “Doing”, conforme ilustra a figura abaixo.

Agora, precisamos criar um novo ramo onde para essa issue será resolvida, no jargão da área chamamos esse ramo de feature branch. Para isso, o criamos a partir do ramo develop e começamos a desenvolver a funcionalidade. Faremos isso clicando com o mouse no nome da issue, então seremos redirecionados para a página com a definição da issue, conforme ilustra a figura abaixo. Depois, vamos clicar no botão “Create merge request” e digitar no campo “Source (branch ou tag)” o nome da branch na qual iremos trabalhar, em nosso exemplo o branch “develop”, e por fim clicar em “Create merge request”.

Então, seremos redirecionados para a tela de definição do merge request, apresentada abaixo. Perceba que não é possível realizar o merge, pois ainda não existem alterações na nova branch criada.

Ao entrar na página Active branches do repositório remoto, podemos observar que foi criado um ramo cujo nome é definido pelo número e nome da issue que deu origem ao ramo.

Agora, é preciso atualizar nosso repositório local baixando o novo branch criado. Fazemos isso com os comandos git fetch origin nome_branch_nova que irá buscar o branch criado do repositório remotoe  git checkout nome_branch_nova, que cria um branch local que rastreia o branch remoto que acabamos de criar.

Finalmente, após implementar a funcionalidade, devemos realizar o commit e enviar para o repositório remoto através do comando git push.

Como podemos ver na página principal do projeto, apresentada abaixo, o repositório remoto no Gitlab foi atualizado. 

Como terminamos de implementar a funcionalidade, precisamos mover a issue para o board “Waiting Acceptance” e aguardar que alguém aprove as mudanças realizadas.

Após atualizar o repositório no Gitlab, podemos voltar na página da definição do merge request (abaixo) e após uma alguém (uma liderança) garantir que a issue foi resolvida, podemos realizar o merge dessas alterações clicando em “Mark as ready” e depois em “Merge”. Neste exemplo, apenas para sermos breves evitaremos o passar pelo ramo staging e código será mesclado diretamente com ramo develop.

No entanto, o procedimento descrito anteriormente, também é válido para o branch staging. Além disso, em uma situação crítica, supondo que tenha ocorrido uma falha de software no ambiente de produção, é preciso corrigi-la o mais rapidamente possível. Para isso, deve-se seguir o mesmo procedimento descrito anteriormente. Porém, desta vez, deve-se criar um branch hotfix a partir do ramo master e fazer o merge de volta no ramo master e também no ramo develop para propagar as correções.

Considerações finais

Neste artigo, mostramos de forma prática como utilizar o Git tanto localmente quanto remotamente. Para isso, apresentamos alguns conceitos básicos do Git e discutimos o funcionamento esperado para o fluxo de trabalho de uma equipe de desenvolvimento de software conforme preconiza o Gitflow. Nós mostramos como criar um repositório local e ignorar arquivos que não devem ser versionados, como realizar commits e desfazê-los, como criar ramos e enviar alterações para o repositório remoto. Finalmente, mostramos como os desenvolvedores devem resolver as issues que lhes são atribuídas. 

Assim, esperamos que este artigo seja útil a todos que estejam se iniciando no uso do Git. Porém, se você tiver interesse em entender aprender mais sobre o Git, de uma forma completamente interativa, recomendamos acessar Learn Git Branching

Gostou do texto? Ele foi útil para você? Nos diga nos comentários, se você teve alguma dúvida que gostaria que fosse sanada! 

Artigo escrito por Higor Duarte. Revisado por Prof. Tiago Carneiro e Vinícius de Paula."
O TerraLAB e as parcerias em inovação e extensão tecnológica,http://www2.decom.ufop.br/terralab/o-terralab-e-as-parcerias-em-inovacao-e-extensao-tecnologica/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/06/Untitled-design-6.png,"Édada a largada para a convocação para o processo seletivo do Programa de Trainee TerraLAB, edição 2021.2. Programa que vem capacitando estudantes de graduação e pós-graduação de qualquer curso da UFOP para atuar nas empresas de Tecnologia da Informação. Para se inscrever, preencha este formulário até o dia 11 de junho, o processo seletivo terá início no dia 14 de junho. O Programa de Trainee é realizado totalmente online.

Os esforços do TerraLAB vem rendendo a seus voluntários um leque de conhecimentos práticos, vivências e experiências em funções, processos e tecnologias para a produção de software. A evidência maior do sucesso do Programa de Trainee TerraLAB é o fato de 100% dos trainees que concluíram o treinamento já foram absorvidos pelo mercado de trabalho, antes mesmo de se formarem, totalizando 23 estudantes em menos de 1 ano e meio. Atualmente, as empresas parceiras do TerraLAB oferecem um total de 10 estágios remunerados para que os estudantes que tiveram melhor desempenho no processo seletivo de 2021.1 continuem seu treinamento no laboratório. Este é um enorme incentivo e um tremendo investimento no aperfeiçoamento profissional e pessoal dos estudantes da UFOP.

Este artigo tem o objetivo de apresentar aos estudantes da UFOP, aos pesquisadores, profissionais e empresas de TI os resultados e as parcerias que o TerraLAB vem desenvolvendo ao longo de anos para levar à frente seus projetos de extensão e inovação tecnológica. Desta maneira, esperamos justificar e agradecer o apoio que recebemos até o momento e, sobretudo, despertar o interesse de outras pessoas e instituições em fazer parte do nosso time. Vem com a gente!

Breve histórico em pesquisa, produção de software e geração de startups 

O TerraLAB – Laboratório para Pesquisa e Capacitação em Desenvolvimento de Software, teve sua origem em 2007, fruto de uma parceria entre a UFOP e o Instituto Nacional de Pesquisas Espaciais (INPE). A sua principal missão é realizar pesquisas científicas neste tema, além de manter e evoluir a plataforma de software livre e de código aberto denominada TerraME (www.terrame.org). Atualmente, o TerraME serve como base tecnológica para dois produtos operacionais do INPE utilizados para definição e análise de políticas públicas para as regiões do Cerrado e da Floresta Amazônica, dois dos biomas brasileiros mais ameaçados. Este produtos são: (1) Arcabouço para Modelagem e Simulação de Mudanças de Uso e Cobertura  do Solo – LUCCME (luccme.ccst.inpe.br); e (2) Modelo de Estimativa de Emissão de Gases de Efeito Estufa por Mudanças de Cobertura da Terra – INPE-EM (inpe-em.ccst.inpe.br). O TerraME também é base tecnológica para pesquisas institucionais em diversos programas de pós-graduação do INPE, nos níveis de mestrado e doutorado, entre eles o curso de Ciência do Sistema Terrestre, o curso de Sensoriamento Remoto e o curso de Computação Aplicada. Ele também serviu como base tecnológica para cooperações científicas com Programa de Computação Científica (PROCC) da FIOCRUZ (ver projeto DengueME), com o Instituto de Geoinformática da Universidade de Munster, Alemanha e com o Environmental Change Institute (ECI) da Universidade de Oxford, Inglaterra. Os artigos científicos que resultam destas pesquisas podem ser acessados a partir do currículo lattes do Prof. Tiago Garcia de Senna Carneiro, coordenador do laboratório. Por meio destas parcerias, o TerraLAB estabelece uma forte conexão entre instituições de pesquisa de destaque e as pesquisas realizadas nas diversas graduações e na pós-graduação em Ciência da Computação  da UFOP. 

Para levar à frente estas iniciativas, o TerraLAB adquiriu e desenvolveu técnicas, métodos e processos para garantir qualidade aos produtos de software e velocidade aos seus times de desenvolvimento. Para isso, estreitou laços com a indústria de software e co-evoluiu com várias empresas parceiras. Neste sentido, o TerraLAB também se fez um ateliê de software que oferece aos estudantes da UFOP um ambiente de inovação que alinha o método/conhecimento científico ao saber produtivo, transformando os resultados de pesquisa em produtos e serviços de software. Nosso entendimento é de que a inovação nasce quando a ciência toca a vida! Neste contexto, o TerraLAB vem gerando startups como a Usemobile, Cachaça Gestor e MineInside (que já não existe por ter sido comprada).  Os trainees do processo seletivo de 2019.2 e 2020.1 produziram ao todo 7 aplicativos com versões WEB, Android e iOS. Alguns deles já estão disponíveis na Play Store, como é o caso do aplicativo da Orquestra Ouro Preto, acessível a partir de dispositivos Android e iOS. Todos os outros aplicativos já foram concluídos e se encontram em fase de testes alpha e beta.

Além destes esforços, para promover apropriação, por parte da sociedade, do conhecimento científico e tecnológico gerado e utilizado por nossas equipes, o TerraLAB mantém diversos canais de comunicações e redes sociais, todos acessíveis pelo link:  https://linktr.ee/terralab.ufop.

As empresas parceiras do TerraLAB

Desde 2019, o laboratório conduz um Programa de Trainee que permite aos estudantes vivenciar os cargos existentes no ecossistema de produção de software e ganhar experiência nos processos e tecnologias que representam o estado-da-arte na indústria de desenvolvimento de software. Para isso, recebe mentoria de profissionais de reconhecida autoridade e conta com a cooperação de diversas empresas, entre elas  destacamos: 

Usemobile: Especialista no desenvolvimento de aplicativos mobile para Android e iOS e aplicações web e desktop.
Stilingue: Plataforma multicanal para melhores experiências entre marcas e consumidores.  
Gerencianet: Conta digital focada em negócios para que empreendedores possam emitir e gerenciar recebimentos.
Memory: Desenvolve soluções tecnológicas, líder no desenvolvimento de sistemas para órgãos públicos. 
GS Ciência do Consumo: Empresa de inteligência com foco no aumento do faturamento para o varejo e a indústria.
Cachaça Gestor: Fornece sistema computacional que visa tornar a vida do produtor de cachaça mais eficiente.

Outras empresas nos oferecem apoio por meio de palestras e treinamentos pontuais, mas algumas  já negociam conosco modelos de colaboração mais abrangentes: 

Take.Blip: Especialista em facilitar a comunicação entre empresas e pessoas. Gestão e evolução de chatbots e contatos inteligentes. 
Tembici: Empresa líder da América Latina de tecnologia para micromobilidade que cria soluções para inspirar uma revolução do espaço urbano.
Nubank: Fintech que desenvolve soluções simples, seguras e 100% digitais, maior banco digital independente do mundo. 
iFood: Maior foodtech da América Latina. 
Como a indústria enxerga o TerraLAB

No final da página principal do site do TerraLAB, é possível ler o depoimento de nossos parceiros e dos trainees que já concluíram o treinamento. Além desses depoimentos, selecionamos os áudios de duas empresas parceiras que já avaliaram trainees do TerraLAB em seus processos seletivos. 
No primeiro áudio, o diretor de tecnologia (CTO) da empresa Usemobile, Patrick Brunoro, relata sua experiência com trainee do TerraLAB, após o primeiro mês de chegada dele na empresa. Durante um ano de treinamento,Arilton Aguilarfoi gerente de engenharia e product owner do TerraLAB e Koda Gabriel foi desenvolvedor de aplicativos móveis do TerraLAB. Atualmente, Aril (como é carinhosamente conhecido por nós) é Analista de Requisitos e Koda é Desenvolvedor iOs Jr na Usemobile.

No segundo áudio, o Gerente de Engenharia de Software da Tembici, Francisco Daniel Costa, relata suas impressões após entrevistar 18 trainees que passaram pelo nosso Programa de Trainee.

Captação de recursos e estágios remunerados 

Além da transferência de know-how, de nos oferecer mentorias, treinamentos e palestras, as empresas parceiras apoiam o TerraLAB nos oferecendo plataformas de aprendizado online e estágios remunerados; estimamos que, ao todo, estes investimentos representam o valor de 240 mil reais/ano provenientes da iniciativa privada.

 Sempre que possível, os trainees que se destacam recebem incentivos, na forma de remuneração direta e indireta (auxílio alimentação, plano de saúde e plano odontológico), para permanecerem no laboratório desempenhando seu treinamento e atividades de desenvolvimento e pesquisa. À medida que ganham experiência e vivência, cerca de 4 a 6 meses dependendo unicamente do estudante, esses trainees migram gradativamente para os ambientes produtivos das empresas. Aqueles que permanecem no laboratório por mais tempo, recebem treinamento em liderança e gestão de pessoas, desenvolvendo habilidades que vão além das habilidades técnicas.

Atualmente a empresa Gerencianet nos oferece três licenças da plataforma de aprendizado online Alura. As empresas Usemobilie e Cachaça Gestor nos oferecem uma licença da plataforma Udemy. Por meio destas licenças, os estudantes têm acesso a certificações e a cursos de elevada qualidade em diversas tecnologias e áreas do conhecimento.

A empresa Usemobile nos oferece dois estágios remunerados pelo segundo ano consecutivo. As empresas Stilingue e Memory nos oferecem quatro estágios remunerados, cada. As negociações com as empresas GS Ciência do Consumo e Tembici já estão em fase avançada. A Memory já sinalizou o desejo de ampliar a parceria. As negociações com a empresa iFood já estão em andamento. Neste contexto, esperamos, brevemente, poder oferecer mais estágios remunerados aos trainees do laboratório. 

Para que possamos ampliar a captação de recursos, desonerando e beneficiando as empresas parceiras, estamos em constante colaboração com a Coordenadoria de Convênios e o Nucleo de Inovação Tecnológica e Empreendedorismo da UFOP, para tirar proveito de tudo da fomenta e permite o Novo Marco Legal de Ciência, Tecnologia e Inovação (Lei nº 13.243/2016). Neste contexto, estamos preparando instrumentos legais e institucionais para facilitar e agilizar a captação de recursos financeiros, equipamentos, serviços de computação em nuvem e, é claro, viabilizar o pagamento de bolsas de estudos com menos burocracia. Um de nossos principais objetivos é viabilizar que as empresas parceiras possam reverter parte dos impostos que pagam em prol desta iniciativa. 

O número de trainees só cresce

O número de estudantes que se inscrevem no processo seletivo e  que realizam e concluem  o Programa de Trainee do TerraLAB é cada vez maior. Na edição de 2019.2 tivemos 17 inscritos, 9 selecionados e apenas 7 concluintes. Na edição de 2020.1, tivemos 39 inscritos, 29 selecionados e 16 concluintes. Na edição 2021.1, tivemos 38 inscritos e 22 selecionados que permanecem em treinamento. Atualmente, 54 pessoas permanecem ligadas ao laboratório e 34 desempenham atividades rotineiras. Queremos ampliar todos estes números, por isso, estamos reforçando nosso convite a estudantes, profissionais, pesquisadores e empresas para somarem forças na construção desse sonho conjunto.  

Venha fazer parte do nosso time

Venha fazer parte desse ambiente onde há espaço para todos aqueles que são automotivados, perseverantes e comprometidos com o próprio sucesso e com o sucesso dos projetos em que se envolvem. Convidamos os estudantes a vivenciar as diversas áreas de atuação envolvidas na produção de software, passando pelo desenvolvimento de  produtos, operação de serviços em nuvem, gestão de projetos e de processos produtivos. Aproveite tudo o que temos a oferecer em uma via de mão dupla. Aprenda as ferramentas, tecnologias e processos que estão em pauta na indústria e no mercado, assim como também tenha a chance de instaurar o novo e trazer suas próprias construções em uma contínua melhoria conjunta!

Se você é estudante da UFOP e se considera uma pessoa automotivada, autodidata, comprometida com o sucesso, proativa, descontraída e generosa, você tem o perfil que buscamos. Você poderá colher benefícios que só o ambiente profissionalizante do TerraLAB oferece para completar o conhecimento acadêmico oferecido pela UFOP, entre este benefício citamos: 

Contato com empresas de destaque na indústria;
Mentorias com profissionais experientes;
Estágios remunerados sem sair da universidade;
Aumento da sua empregabilidade;
Vivência dos diferentes cargos existentes no ecossistema de desenvolvimento de software;
Treinamento e ganho de experiência prática nos mais atuais processos e tecnologias para produção de software ;
Cumprimento da carga horária em atividades extra-curriculares (ATV100)  exigida pelos cursos de graduação.

Para isso, basta que você possa dedicar 20 horas semanais às atividades do TerraLAB e faça logo sua inscrição preenchendo este formulário. As inscrições estarão abertas até o dia 11 de junho e o processo seletivo terá início no próximo dia 14. Tanto o processo seletivo quanto o treinamento são realizados completamente online. Vem com a gente, faça parte do nosso time!
Se você representa uma empresa e se interessou por tudo o que viu aqui, entre em contato direto pelo e-mail do professor Tiago Carneiro.

Artigo escrito por Prof. Tiago Carneiro. Revisado por Paloma Bento."
A influências das cores em projetos de UX/UI,http://www2.decom.ufop.br/terralab/a-influencias-das-cores-em-projetos-de-ux-ui/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/05/UX-4-730x350.png,"Apsicologia é uma ciência que estuda a mente humana, os fatores que provocam diferentes decisões, atitudes, aprendizagem, inteligência, emoções e sensações. Isso faz com que seja interessante entendermos um pouco sobre, pois, assim podemos melhorar nossos produtos ou serviços pensando no comportamento de seu público alvo. Por isso, neste artigo abordaremos a psicologia das cores e sua aplicação na experiência do usuário ou, simplesmente, UX (de User eXperience).

A psicologia das cores é o estudo que busca compreender como as cores influenciam nas sensações e no comportamento. As cores podem conter diferentes significados que são influenciados por questões fisiológicas, culturais, de grupo e individuais.

Ela pode ajudar a transmitir uma mensagem mais clara e alavancar os negócios através das sensações que provocam nas pessoas. Pensando assim, podemos citar como exemplo as empresas do ramo alimentício que geralmente utilizam o vermelho e o amarelo que são cores que remetem à fome.

O que é necessário saber para aplicar a psicologia das cores?

Para aplicarmos a psicologia das cores em nosso projeto é importante termos em mente que apesar de haver alguns guias que podem ser bastante úteis para entendermos como cada cor influencia nosso comportamento. Precisamos levar em consideração o contexto, fatores culturais e experiências individuais, pois podem afetar a percepção das pessoas em relação a cada cor.

Um ótimo exemplo, é o caso de uma das franquias do McDonald ‘s na Turquia, que tiveram de abrir mão do seu clássico vermelho e amarelo devido à pressão causada pela rivalidade entre dois times de futebol. A loja da rede estava localizada próxima ao estádio dos Besiktas e a torcida não aceitou as cores, pois representava o time rival. Essa pode ser uma lição valiosa, de que precisamos estar atentos a todas as variáveis que podem influenciar um determinado público alvo.

Figura 1 – Loja da rede na região de Besiktas, Turquia. Fonte: Página da ESPN, A rivalidade que obrigou o McDonald’s a esconder suas cores¹

Como os profissionais que trabalham com UX podem se beneficiar deste conhecimento?

O profissional de UX é responsável por pensar em toda a experiência que os usuários terão durante o uso de um produto ou serviço e pode utilizar da psicologia das cores como parte do processo.  

Neste sentido, as cores podem servir como uma grande aliada nas sensações que os clientes irão experimentar com aquilo que lhe estiver sendo oferecido. Assim como também irão impactar na imagem de uma empresa perante seu público.

Tendo em mente o contexto, podemos utilizar alguns guias sobre as cores que serão bem úteis na hora de pensarmos o design e a experiência do usuário.

Figura 2: Guia emocional das cores.  Fonte: Acervo publicitário²


Alguns especialistas do site “No Film School” elaboraram um compilado contendo todos os significados ligados a determinadas cores, para facilitar a vida dos profissionais de diversas áreas:

Vermelho: raiva, paixão, fúria, ira, desejo, excitação, energia, velocidade, força, poder, calor, amor, agressão, perigo, fogo, sangue, guerra e violência.
Rosa: amor, inocência, saúde, felicidade, satisfação, romantismo, charme, brincadeira, leveza, delicadeza e feminilidade.
Amarelo: sabedoria, conhecimento, relaxamento, alegria, felicidade, otimismo, idealismo, imaginação, esperança, claridade, radiosidade, verão, desonestidade, covardia, traição, inveja, cobiça, engano, doença e perigo.
Laranja: humor, energia, equilíbrio, calor, entusiasmo, vibração, expansão, extravagância, excessivo e flamejante.
Verde: cura, calma, perseverança, tenacidade, autoconsciência, orgulho, imutabilidade natureza, meio ambiente, saudável, boa sorte, renovação, juventude, vigor, primavera, generosidade, fertilidade, ciúme, inexperiência, inveja, imaturidade e destruição.
Azul: fé, espiritualidade, contentamento, lealdade, paz, tranquilidade, calma, estabilidade, harmonia, unidade, confiança, verdade, confiança, conservadorismo, segurança, limpeza, ordem, céu, água, frio, tecnologia e depressão.
Roxo/Violeta: erotismo, realeza, nobreza, espiritualidade, cerimônia, misterioso, transformação, sabedoria, conhecimento, iluminação, crueldade, arrogância, luto, poder, sensibilidade e intimidade.
Marrom: materialismo, excitação, terra, casa, ar livre, confiabilidade, conforto, resistência, estabilidade e simplicidade.
Preto: não, poder, sexualidade, sofisticação, formalidade, elegância, riqueza, mistério, medo, anonimato, infelicidade, profundidade, estilo, mal, tristeza, remorso e raiva.
Branco: sim, proteção, amor, respeito, mesura, pureza, simplicidade, limpeza, paz, humildade, precisão, inocência, juventude, nascimento, inverno, neve, bom, esterilidade, casamento (culturas ocidentais), morte (culturas orientais), frio, clínico e estéril.
Prata: riqueza, glamour, fascínio, diferença, natural, liso, suave e macio.
Considerações finais

Compreender a psicologia das cores e aplicá-la na hora de escolher uma paleta de cores que transmita bem a mensagem do design, traz consigo um componente-chave do impacto psicológico que os produtos ou serviços devem ter sobre os usuários e, consequentemente, sua experiência.

As cores bem definidas e bem escolhidas elevam o design de “bom” a “ótimo”, enquanto a escolha de cores incorretas ou fora de harmonia pode prejudicar a experiência geral do usuário e como consequência interferir em sua capacidade de compreender as funcionalidades de uma aplicação. 

Embora tenhamos cores que sejam consideradas universais no design UX (como o preto, branco e cinza, utilizadas na maioria dos designs existentes) as cores com as quais são combinadas podem ter grande impacto sobre a compreensão do usuário de uma aplicação.

Para isso é necessário compreender o contexto social do usuário, por mais que a combinação esteja harmoniosa e bem definida, se o contexto cultural não for conhecido, todo o projeto poderá fracassar. Por isso, lembre-se: o que agrada o designer pode não agradar ao público alvo. Assim, estudar o usuário para atingir os melhores resultados nos negócios é essencial.  

Gostaria de aprender mais sobre UX? Você atua profissionalmente como designer de UX ou de UI? Gostaria de contribuir para essa discussão? Ou quem sabe conhecer mais sobre o TerraLAB? Comente, compartilhe esta publicação e não deixe de acompanhar nosso blog e redes sociais!

Artigo escrito por Eudes Rodrigues e Gustavo Lucas Moreira. Revisado por Walisson Farias, Daniel Keoma e Prof. Tiago Carneiro.

Referências

Como a Psicologia é Essencial em Seus Projetos de UX Design 

Entenda o que é Psicologia das Cores e descubra o significado de cada cor.

psicologia cores um guia avancado para profissionais 

harmonia-das-cores

Psicologia da Cores – Aulão completo e atualizado sobre Significado das Cores

Psicologia das Cores – Significado das cores no design

Psicologia das Cores no Marketing

A PSICOLOGIA DAS CORES – Significado das Cores no Marketing

How Colors Affect Conversion Rate

A Psicologia das Cores no Marketing

Psicologia das Cores – @Curso em Vídeo HTML5 e CSS3

Hack de carreira: como se tornar um UX designer (e por que você deveria)

A rivalidade que obrigou o McDonald’s a esconder suas cores | Blogs"
Resultados do Processo Seletivo 2021.1,http://www2.decom.ufop.br/terralab/resultados-do-processo-seletivo-2021-1/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/05/imagemdecabecalho-730x350.png,"Há três semanas atrás encerramos o nosso processo seletivo. No artigo Encerramento do processo seletivo 2021.1, explicamos como o processo foi conduzido, suas premissas e discutimos os principais desafios vividos por nosso time e pelos candidatos a trainees. Foram cinco semanas de muito trabalho que resultaram na aquisição de 22 novos colaboradores para o time TerraLAB. Eles atuaram nas áreas de Data Analytics, Geoinformatics, Marketing, Infrastructure, User Experience, Quality Assurance, Front-end Web, Front-end Mobile e Backend.

Nesse artigo, iremos mostrar os resultados concretos obtidos em cada uma dessas áreas ao longo do processo seletivo. Para isso, apresentamos vídeos gravados pelos gerentes responsáveis por essas áreas. Os gerentes são integrantes mais experientes do TerraLAB, isto é, são estudantes que já passaram pelo treinamento de nível operacional e atualmente realizam treinamento em atividades de gestão e ensino. Nos vídeos são apresentados os resultados alcançados pelos trainees em cada área de conhecimento do laboratório, esperamos que esses vídeos também evidenciem a maturidade de nosso processos de desenvolvimento de software, evidenciem nosso domínio sobre as práticas e ferramentas utilizadas na cultura DevOps e nosso domínio sobre as tecnologias de desenvolvimento mais atuais.

Os vídeos estão em nosso canal do YouTube e, para sua comodidade, listamos o link a seguir. 

Vídeo gravado por João Pedro Mendes – Gerente de User Experience.

Vídeo gravado por Carlos Magalhães – Gerente de Marketing.

Vídeo gravado por Emanuel Xavier – Gerente de projetos.

Vídeo gravado por Vinícius de Paula- Gerente de Desenvolvimento de Software.

Vídeo gravado por Diego Henrique – Gerente de Data Analytics.

Vídeo gravado por Guilherme Carolino – Gerente de Infraestrutura.

Considerações finais

Os resultados apresentados no final do processo seletivo confirmam o êxito dos trainees ao realizarem os desafios das suas respectivas áreas. Esse é apenas o começo de suas trajetórias no TerraLAB, que serão sempre marcadas por mais desafios e muito aprendizado. Durante seu período de capacitação dentro do laboratório, os estudantes experimentarão na prática a vivência do mercado de desenvolvimento de software e contarão com a colaboração de todo nosso time e das empresas parceiras.

Você ficou curioso sobre o processo seletivo? Deu aquela vontade gostosa de fazer parte do nosso time? Gostaria de conhecer mais sobre o TerraLAB? Comente, compartilhe esta publicação e não deixe de acompanhar nosso blog e redes sociais!"
Aplicativos para o Controle das Finanças Pessoais,http://www2.decom.ufop.br/terralab/aplicativos-para-o-controle-das-financas-pessoais/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/05/Aplicativos-para-o-Controle-das-Financas-Pessoais-2-730x350.png,"Este artigo sai um pouco do tema “Desenvolvimento de Software” e mostra como eles podem ser utilizados para a melhoria de vida, impactando o bem estar de famílias. Você pensou em utilizar aplicativos para controlar suas finanças pessoais? Neste artigo, tratamos desta questão como um exemplo vívido e muito simples dos benefícios que a tecnologia pode trazer. Escolhemos um exemplo de aplicativo que fosse útil a todos, sem exceções. A possibilidade de impactar positivamente a vida das pessoas justifica muito, ou quase tudo, daquilo que fazemos no TerraLAB. Vem com a gente!

A educação financeira nada mais é do que uma forma de entender a importância do dinheiro no dia-a-dia e conseguir monitorá-lo e controlá-lo adequadamente. Não é simplesmente economizar, mas é ter consciência dos riscos e oportunidades que envolvem este tema. Neste contexto, aplicativos para smartphones ou tablets podem lhe auxiliar a  decidir o que fazer com o seu salário.  Abaixo estão algumas sugestões de aplicativos para este fim:

O aplicativo Minhas Economias é muito bem avaliado na playstore da Google  (4.4) e teve cerca de 38 mil avaliações. Ele permite organizar entradas e saídas, além de categorizar os grupos que influenciam a renda mensal. Ele disponibiliza histórico de transações cadastradas, extrato mensal e gráficos de movimentação financeira. Também é possível estabelecer metas específicas para determinados objetivos e acompanhá-los ao longo do tempo, além de ser uma opção gratuita de apoio na administração financeira.

O aplicativo Mobiles Controle Financeiro também é muito bem avaliado (4.6) em aproximadamente 241 mil avaliações. Ele permite todos os itens que o aplicativo anterior oferece, além de gerar gráficos e relatórios personalizados com simples comandos. Este aplicativo permite o gerenciamento de cartões de crédito, controle de investimentos, leitura de SMS e notificações de bancos e geolocalização de despesas. Destacamos que é possível ter a versão gratuita com algumas limitações, porque só é possível usufruir na versão paga.

Acima de tudo, você deve compreender os conceitos que giram entorno da economia pessoal, para perceber que existem diversas opções para lhe direcionar a um controle de gastos. Para verificar este pressuposto, o TerraLAB buscou a Educadora e Consultora Financeira Kelly Ribeiro, para comentar sobre o uso de recursos digitais no controle pessoal financeiro.

“Para facilitar a organização financeira, podemos contar com os recursos computacionais, porém é preciso analisar bem quais utilizar. O desconhecimento de conceitos básicos sobre finanças pode ter o efeito contrário e acabar prejudicando o direcionamento de gastos, como é o caso dos aplicativos financeiros dos bancos.  Muitos deles possuem vínculos com instituições financeiras, que os utilizam para saber da sua situação e então te oferecem as “soluções”, que nem sempre são as melhores. Ressalto que em muitos casos eles criam um hábito desnecessário de “anotar sempre os seus gastos”. Diagnósticos de direcionamento financeiro, ou seja, entender quais gastos essenciais e não essenciais, uma vez ao ano ou quando o cenário muda, podem ser o suficiente para se preparar para os imprevistos econômicos. 

Basicamente, o orçamento mensal precisa conter os seguintes itens:

Se pagar – valor que você tem para gastar como quiser, ou seja passar o mês;
Reserva financeira – para casos de emergência e de oportunidades;
Objetivos – valor para seus objetivos de curto, médio e longo prazo;
Despesas – são as despesas mensais, que podem ser categorizadas por valor, Tipo(mensal ou bimestral, por exemplo) e formas de pagamento.

Neste caso uma boa e velha agenda ou então uma planilha de Excel simples, é suficiente. 

Veja abaixo o exemplo de uma planilha:

“Resumindo, não importa se é em agenda, caderno, planilha, papel de pão…o importante é que você escolha uma ferramenta simples, que você tenha familiaridade e que coloque em prática. Só melhoramos algo quando de fato conhecemos a situação que nos encontramos atualmente.“

Kelly Ribeiro é membro da Associação Brasileira de Educadores Financeiros (ABEFIN), Pós-Graduanda em Educação Financeira Metodologia (UNOESTE) e graduada em Engenharia de Produção – UNIPAC. Linkedin: kelly-ribeiro.

Artigo escrito por Josemar Coelho Felix. Revisado por Prof. Tiago Carneiro.

REFERÊNCIAS

[1]Pravaler. Educação financeira, qual sua importância de saber sobre finanças. Disponível em: https://www.pravaler.com.br/educacao-financeira-qual-a-importancia-de-saber-sobre-financas . Acessado: 27/04/2021

[2]ParanáBanco. Os 5 aplicativos de controle financeiro que você precisa conhecer. Disponível em: https://paranabanco.com.br/blog/educacao-financeira/os-5-aplicativos-de-controle-financeiro-que-voce-precisa-conhecer. Acessado: 27/04/2021"
Encerramento do processo seletivo 2021/1,http://www2.decom.ufop.br/terralab/encerramento-do-processo-seletivo-2021-1/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/05/WhatsApp-Image-2021-05-05-at-18.16.20-1-730x350.jpeg,"Após cinco semanas de muitos desafios, treinamento, aprendizado e trabalho em equipe, finda o processo seletivo do Programa de Trainee do TerraLAB, para o primeiro período letivo de 2021. Neste artigo, abordamos como o processo seletivo foi conduzido e apresentamos os resultados alcançados e as dificuldades vivenciadas pelos candidatos a trainees durante este processo. Desta forma, esperamos informar as empresas parceiras que apoiam o TerraLAB e encorajar estudantes que avaliam a possibilidade de fazer parte do nosso time. 

Ao invés de um processo seletivo que filtra ou de um processo seletivo que promova a competição entre os participantes, o TerraLAB possui um processo seletivo inclusivo, que promove a colaboração e o espírito de equipe. Uma vez que não temos limites de vagas, buscamos um processo seletivo que gere oportunidades para o máximo de pessoas, que respeite a diversidade e promova a inclusão etária, de gênero, de raça, de credo e de formação (precisamos de pessoas dos mais diversos cursos e habilidades). Com a ajuda das empresas parceiras, estamos cientes de que temos a capacidade de oferecer capacitação técnica de alta qualidade. Portanto, alinhado com nossos valores, o processo seletivo busca por pessoas automotivadas, comprometidas com o sucesso dos projetos nos quais se envolvem, dedicadas, autodidatas, generosas, gentis, descontraídas e proativas. 

Desta forma, o TerraLAB constrói e mantém um ambiente direcionado ao crescimento em conjunto da equipe, mostrando que é possível trabalhar e desenvolver o ser humano de maneira empática e amigável. Por isso, mesmo durante o processo seletivo, ninguém, em momento algum, trabalhou sozinho. Nossa preocupação é inserir cada participante dentro dessa cultura, deixando claro que nossas métricas de avaliação não se pautam apenas em produção, mas também privilegia o engajamento e a capacidade colaborativa de cada um. Esse é o pilar que garante a satisfação das nossas parceiras e da nossa equipe, orientando todos ao sucesso.

Trabalho em equipe

Conforme a cultura Agile – integrada a todos os processos do laboratório – cada participante foi alocado a um chapter específico, de acordo com as habilidades que gostaria de desenvolver. Chapter ou capítulo é termo organizacional da cultura Agile que agrupa pessoas com habilidades e responsabilidades similares, para que se fortaleçam tecnicamente ao colaborarem e possam também suprir ausências umas das outras. Nas equipes assim organizadas, é comum que haja um chapter para as diferentes áreas de conhecimento, desenvolvimento e produção da tecnologia da informação. Atualmente, a equipe do TerraLAB se estrutura nos seguintes chapters: Data Analytics, Geoinformatics, Marketing, Infrastructure, User Experience, Quality Assurance, Front-end Web, Front-end Mobile e Backend . 

Os participantes do chapters diretamente relacionado às atividades de desenvolvimento de software, Quality Assurance, Front-end Web, Front-end Mobile e Backend, também foram divididos em pequenas equipes denominadas squads. Cada squad é um pequeno time de desenvolvimento de software cujo objetivo é produzir aplicações ou serviços (SaaS – Software as a Service) dentro do foco de interesse e conhecimento do laboratório. Neste contexto, todo squad foi formado por, pelo menos, um indivíduo de cada um destes chapters. Alguns participantes decidiram em comum acordo com o TerraLAB e se voluntariaram para participar do processo seletivo em mais de um chapter. Diante desta decisão, estes participantes passaram a ser avaliados em uma função principal e foram bonificados por seu desempenho na função secundária.

Para fins de mentoria, treinamento e gestão do próprio processo seletivo, cada um dos chapters ou squads contou com um tutor. Este tutor, em geral, é um integrante experiente do TerraLAB que, do início do processo até o final do treinamento dos candidatos aprovados, passa a vivenciar atividades de gerência e ensino, responsabilizando-se pelo bem estar e desempenho de um chapter ou squad.

A seguir, descrevemos como foram conduzidos os desafios dentro de cada um destes grupos de pessoas, seus resultados e suas dificuldades.

Área de Desenvolvimento

“Como gerente técnico fiquei responsável por ajudar os trainees dos squads de desenvolvimento, esclarecendo dúvidas em relação a implementação dos desafios propostos pelo processo seletivo. A minha responsabilidade foi ajudar e adaptar os squads durante todo o processo seletivo.  Para o sucesso do trabalho foi utilizado a ferramenta GitLab, que criando pequenos treinamentos para formar os profissionais, tornou possível coletar métricas, além de organizar o fluxo de tarefas e entregáveis. Para conseguir auxiliar adequadamente os trainees no seu desenvolvimento, em conjunto com outros gerentes, foram elaborados tutoriais que ensinavam como utilizar bibliotecas e implementar as funcionalidades dos sistemas, exigido no desafio”, destaca Vinícius de Paula, gerente de engenharia, responsável pela área de desenvolvimento.

Todos os squads de desenvolvimento receberam o mesmo desafio de montar um sistema, descrito pelo cliente e coordenador e do TerraLAB. Cada um teve sua interpretação da descrição proposta e a partir desta, os times tiveram que buscar conhecimento para elaborar um backlog, criar protótipos de interface e validá-los com o cliente. Para conseguir realizar com sucesso esses feitos, o gerente de engenharia propôs prioridades de tarefas para cada sprint, ocasionando boas práticas no aprendizado do desenvolvimento do produto. 

Tela de login do aplicativo Web denominado GeoRef, desenvolvido pelo Squad 4 durante o processo seletivo.


Durante o processo foi possível notar que houve necessidade de incentivar o trabalho em equipe dos desenvolvedores Backend, principalmente no desenvolvimento de uma API unificada para atender a todos os front-ends, além de promover a interação dos desenvolvedores na construção da base de código. Para os desenvolvedores Front-end, a maior dificuldade foi a curva de conhecimento dos conceitos intrínsecos da linguagem e Frameworks utilizados. Nesta sessão de desenvolvimento destacou-se a necessidade de maior orientação e discussão de tópicos específicos da área de engenharia de software. Dessa maneira, o desafio de implementação dos mapas junto às funcionalidades do sistema tornou-se menos desafiador e mais compreensível. Em relação aos Product Owners, as principais dificuldades encontradas foram a adaptação à rotina de reuniões, e a utilização do GitLab para gerir os desenvolvedores e conseguir se adequar à cultura Agile do laboratório, mas com a iniciativa e vontade dos participantes, esse pequeno incômodo foi sendo superado aos poucos.

Tela de marcação de caminhamentos e pontos de interesse do aplicativo Android denominado GeoMap, desenvolvido pelo Squad 6 durante o processo seletivo.

Área de User Experience 

Os desafios da equipe de UX (Experiência do Usuário) foram divididos em cinco sprints de uma semana, onde a equipe trabalhou a todo momento de forma colaborativa em um desafio principal e um secundário.

O principal seria um projeto onde os candidatos estudaram todo o contexto do produto, realizaram o briefing com o usuário interno, pesquisa e benchmarking, elaboração de personas e por último a criação de um wireframe. A solução a ser desenvolvida seria criar um protótipo de um aplicativo para auxiliar profissionais de engenharia no levantamento de dados de campo.

Já o desafio secundário exigia que a equipe de UX auxiliasse os squads no desenvolvimento de UI (Interface do Usuário) por meio de validações e utilização das melhores práticas de UX. Ao final do processo seletivo todos os candidatos tiveram a oportunidade de aplicar na prática todo o conhecimento adquirido por meio dos cursos realizados na primeira semana do processo seletivo, o que trouxe grandes resultados em pouco tempo.

“Minha experiência durante o processo foi muito positiva, além de me preparar profissional e pessoalmente, tive contato com uma metodologia de mercado dentro da universidade, sendo única dentre as oferecidas e extremamente enriquecedoras. Espero que toda a bagagem adquirida possa me auxiliar futuramente na busca de aprimoramento de minhas habilidades sociais e técnicas.” –  disse Gustavo Lucas, trainee na área de UX.

Área de Quality Assurance

Na área de Quality Assurance, os participantes estudaram as técnicas de desenvolvimento ágil BDD – Behavior Driven Development, ou seja, Desenvolvimento Orientado por Comportamento, visando a melhora da qualidade dos produtos, ao elucidar os critérios de aceitação dos clientes e ao especificar cenários de testes equilibrados (muito robustos e tão baratos quanto possível). Estas técnicas foram utilizadas no desenvolvimento de testes funcionais e testes de aceitação para as aplicações Web e Mobile, assim como para a API do backend.

A maior dificuldade foi a de desenvolver a visão de cenários de testes capazes de prever cada necessidade do usuário, de forma a entregar uma aplicação robusta desde sua primeira iteração. Seguindo as práticas do BDD, houve também a necessidade de se criar cenários que não contavam necessariamente com um componente pronto ou funcional, mas que já previssem seu comportamento, suas falhas e a melhor forma de conduzir sua execução.

“O processo seletivo de 2021 foi algo muito gratificante para minha carreira, pois tive oportunidade de compartilhar meus conhecimentos com os trainees e vê-los se tornarem aptos a realizar a função de engenheiro de testes. O maior desafio que enfrentei na gerência foi a falta de conhecimento sobre atividades gerenciais, principalmente na organização e no trato com as pessoas, porém com a ajuda da equipe TerraLab pude desenvolver essas habilidades e cumprir as tarefas de gerente. Com pouco mais de 1 ano de TerraLAB já pude obter muito conhecimento, mas espero continuar desenvolvendo novas habilidades e compartilhando também” – depoimento de Bruno Augusto, gerente de Quality Assurance.

Área de Infrastructure

O processo seletivo da equipe de Infraestrutura consistiu em cinco sprints com cinco desafios, onde habilidades para fazer o deploy automático de componentes de software e manter um serviço em nuvem disponível foram aprendidas e exercitadas. Durante as sprints os trainees utilizaram ferramentas como docker e dockerfile, serviços de nuvem como o AWS Ec2, servidores Apache 2 e Ngnix e  implementaram pipelines CI/CD via GitLab.

  Os desafios foram realizados de forma individual, com intuito de gerar conhecimento para os trainees e simular de forma mais verossímil o trabalho em um sistema real. Todos foram realizados com dedicação e excelência pelo time, onde a maior dificuldade foi não estar trabalhando em um sistema real, havendo necessidade de criar simulações para todos os desafios.

“Participei do processo seletivo do TerraLAB nas áreas de UX e Infraestrutura e foi uma experiência bastante enriquecedora, pois tive a oportunidade de aprender coisas novas e colocar em prática através dos desafios ao lado de pessoas fantásticas interessadas em crescer e se desenvolver cada vez mais, só tenho a agradecer pela oportunidade.” – destaca Eudes Rodrigues, trainee nas áreas de UX e Infraestrutura.

Área de Data Analytics

O processo seletivo da equipe de Data Analytics consistiu em cinco sprints e três desafios onde foram introduzidos algumas técnicas da área da análise de dados. Os trainees foram incentivados a trabalharem em equipe, porém o último desafio foi individual com o objetivo de serem melhor avaliados.

Durante as sprints os trainees utilizaram técnicas como o web scraping para extrair dados necessários para um dos desafios, análise de séries temporais para prever uma determinada situação e uma análise livre para gerar diferentes tipos de insights a respeito de uma determinada base de dados.

Todos os desafios foram superados com excelência pelo time, ficou claro como o processo trainee tem a capacidade de ensinar, incentivar e os engaja-los a fazerem parte do TerraLAB.

“Participar do processo seletivo do Terralab foi uma experiência que possibilitou conhecer diferentes assuntos e ferramentas na área da Ciência de Dados, através de desafios práticos. Além disso, durante o processo a vivência de metodologias ágeis e da cultura organizacional laboratório permitiu criar uma visão de como é o mercado de trabalho.”- relata Lucas Natali, trainee na área de Data Analytics.

Área de Marketing

Durante o processo seletivo, os trainees da área de marketing realizaram cursos relacionados a área e foram apresentados ao atual plano de marketing do TerraLAB. A partir dos conhecimentos obtidos, os trainees produziram conteúdos para as redes sociais do lab e montaram uma nova estratégia de marketing de conteúdo.

Para a realização dessas atividades, os trainees se organizaram de maneira que fossem feitas colaborativamente e assim, todos puderam contribuir, o que foi determinante para o resultado final do processo.

“Aprender sobre uma área que faz parte da realidade de todas as empresas e poder desenvolver uma estratégia, que pode trazer impactos positivos, para o TerraLAB foi muito proveitoso e gratificante. Acredito que o aprendizado absorvido no processo seletivo vai ser um diferencial tanto na vida pessoal quanto na profissional daqui pra frente.” – depoimento de Paloma Bento, trainee na equipe de marketing.

Sequência de stories para Instagram feita pelos trainees de marketing, durante o processo seletivo. 

Considerações finais

Parabenizamos e agradecemos a cada participante que fez parte desse processo seletivo. A troca de conhecimentos e experiências promovida foi enriquecedora para o TerraLAB. A partir desse momento, os 22 aprovados entre os 38 candidatos iniciam sua jornada no lab, composta por mais desafios, que serão superados conjuntamente por todos nós.

Para o professor Tiago Garcia de Senna Carneiro, coordenador do TerraLAB, “Os principais desafios que o processo seletivo são: Projetar um processo seletivo inclusivo que crie oportunidades e amadureça pessoal ao invés de simplesmente filtrá-las; Manter os candidatos engajados aos longo do processo mesmo diante de desafios de nível elevado e de uma curva de aprendizado muito acentuada verificadas em algumas tecnologias; Mudar o mindset dos candidatos – de estudante para profissional – trazendo-os tão cedo quanto possível para uma cultura empresarial. Finalmente, de maneira muito prática foi preciso avaliar não só desempenho e qualidade técnica, mas também considerar com muito carinho a dedicação e as habilidades de comunicação e colaboração dos trainees.” 

Você ficou curioso sobre o processo seletivo? Gostaria de ficar atento aos acontecimentos do TerraLAB? Comente, compartilhe esta publicação e não deixe de acompanhar nosso blog e redes sociais para saber mais!"
"S.O.L.I.D: Conceitos e exemplos em Typescript – Parte 5, o “D”",http://www2.decom.ufop.br/terralab/s-o-l-i-d-conceitos-e-exemplos-em-typescript-parte-5-o-d/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/04/SOLID-13-730x350.png,"Um bom engenheiro de software precisa conhecer os princípios designados pelas letras que formam o acrônimo S.O.L.I.D:

S – Single Responsibility Principle 
O – Open-Closed Principle 
L – Liskov Substitution Principle 
I – Interface Segregation Principle 
D – Dependency Inversion Principle 

Este é o quinto e último artigo de uma série inteiramente dedicada a estes princípios. Caso ainda não tenha lido os artigos anteriores da série, sobre as letras “S”, “O” “L” e “I”, sugerimos a leitura antes que continue: Princípio da Responsabilidade Única, Princípio Aberto/Fechado,  Princípio da Substituição de Liskov e Princípio da Segregação de Interface respectivamente. 

A seguir iremos aprofundar em nossa explicações sobre a letra “D” – Princípio da Inversão de Dependência (The Dependency Inversion Principle).

DIP – Princípio da Inversão de Dependência

O Princípio da Inversão de Dependência estabelece que um sistema flexível é aquele cujo as dependências no código fonte sempre apontam para abstrações ao invés de componentes concretos. Estas dependências no código fonte são criadas através de palavras chaves como “include”, “require” ou “import”, como é o caso do Typescript.

Antes de nos aprofundarmos mais no princípio, é importante categorizar os dois tipos de componentes que podem existir em uma aplicação: 

Componentes de alto nível: São implementações que refletem as regras de negócio da aplicação. São sequências de passos que existem independente do tipo de tecnologia que é utilizada em um processamento. São a finalidade de um processo e não o meio.
Componentes de baixo nível: Também conhecido como componentes voláteis, são implementações que estão sujeitas a modificações frequentes. Geralmente, são códigos que realizam integração com bancos de dados (BD), com interfaces de programação de aplicações (APIs) ou com bibliotecas de código. É comum que estas tecnologias mudem e precisem ser atualizadas ou substituídas, com mais frequência do que a regra de negócio que uma aplicação implementa. São o meio para atingir o objetivo de um processo e não a finalidade. 

A ideia do DIP é que ao invés das dependências do código fonte apontarem para componentes de baixo nível, as mesmas sejam direcionadas para classes abstratas ou interfaces, que certamente são bem menos voláteis do que implementações concretas.

Estas implicações convergem em um conjunto bem específico de práticas de codificação:

Não faça referência a classes concretas que sejam voláteis: Ao invés disso dependa de classes abstratas ou interfaces;
Não implemente herança de classes concretas que sejam voláteis: Em linguagens estaticamente tipadas, herança é tipo de relacionamento mais forte e rígido; assim, deve ser usado com bastante cautela
Não sobrescreva (override) funções concretas: Ao utilizar ou chamar uma função concreta, seu código também está dependendo de um elemento concreto. O fato de você sobrescrever esta função não tira a dependência do elemento. Ao invés disso, você está criando um relacionamento de herança com ele. Uma boa solução para este cenário seria definir a função como abstrata e criar múltiplas implementações para a mesma. 
Nunca mencione o nome de qualquer classe ou componente concreto que seja volátil: Basicamente é o resumo do princípio em si.

Vamos entender melhor este princípio através de um exemplo prático em Typescript, discutindo como o mesmo pode ser violado:

import { Sequelize, QueryTypes } from 'sequelize';
 
interface User {
  nickname: string,
  password: string
};
 
export class Login {
  private _conn: Sequelize;
 
  private _createConnection() {
    this._conn = new Sequelize(
      'database',
      'username',
      'password',
      {
        host: 'localhost',
        dialect: 'mysql'
      }
    );
  }
  async execute(nickname: string, password: string) {
	    this._createConnection();
 
    if (!this._conn) {
      throw new Error(""DB connection not initialized"");
    }
 
    const users = await this._conn.query(`SELECT * FROM users WHERE nickname = ${nickname}`, {
      type: QueryTypes.SELECT,
      raw: true
    }) as User[];
 
    const user = users[0];
 
    if (user.password !== password) {
      throw new Error(""The given password is wrong. Try again"");
    }
 
    return ""Successfully logged"";
  }
}

A classe acima representa uma regra de negócio para realização de login de usuário em um sistema. 

Basicamente, o método principal recebe um nome de usuário e senha, tenta recuperar o registro do usuário no BD através do nickname informado, e verifica se o password recuperado é igual ao informado pelo usuário; se não for, um erro de falha de autenticação é retornado. O problema é que esta classe está indo contra grande parte dos princípios do S.O.L.I.D, a começar pelo número excessivo de responsabilidades que ela tem. Ao invés de implementar somente a regra de login, a classe também é responsável por implementar a integração com o BD através do pacote do Sequelize. Por essa razão, devemos dividir essas responsabilidades como é mostrado abaixo:

import { Sequelize, QueryTypes } from 'sequelize';
 
interface User {
  nickname: string,
  password: string
};
 
export default class SQLUserGateway {
  private _conn: Sequelize;
 
  constructor() {
    this._conn = new Sequelize(
      'database',
      'username',
      'password',
      {
        host: 'localhost',
        dialect: 'mysql'
      }
    );
  }
 
  async findUserByNickname(nickname: string): Promise<User> {
    const users = await this._conn.query(`SELECT * FROM users WHERE nickname = ${nickname}`, {
      type: QueryTypes.SELECT,
      raw: true
    }) as User[];
 
    return users[0];
  }
}

Acima, mostramos como a integração com o BD foi isolada em uma classe específica. Veja agora como fica a classe de Login:

import SQLUserGateway from './sql-user-gateway';
 
export class Login {
  async execute(nickname: string, password: string) {
 
    const userGateway = new SQLUserGateway();
 
    const user = await userGateway.findUserByNickname(nickname);
 
    if (user.password !== password) {
      throw new Error(""The given password is wrong. Try again"");
    }
 
    return ""Successfully logged"";
  }
} 

Apesar de cada classe possuir sua própria responsabilidade, ainda estamos ferindo o DIP, uma vez que a classe Login, um componente de alto nível,  possui uma dependência direta com a classe SQLUserGateway, um componente de baixo nível. Desta forma, qualquer mudança na integração com o BD, fato muito comum em projetos de software de longa duração, pode impactar na regra de negócio principal do login .
Para corrigir este problema, basta definir uma interface específica para a classe Login, contendo todos os métodos necessários para a execução do login (lembre do Princípio da Segregação de Interface). Assim, a classe Login não estará mais dependente de uma implementação concreta que poderia ser alterada a qualquer momento. Veja o código abaixo:

export interface User {
  nickname: string,
  password: string
};
 
export interface UserGateway {
  findUserByNickname(nickname: string): Promise<User>
}
 
export class Login {
  private _userGateway: UserGateway;
 
  constructor(userGateway: UserGateway) {
    this._userGateway = userGateway;
  }
 
  async execute(nickname: string, password: string) {
    const user = await this._userGateway.findUserByNickname(nickname);
 
    if (user.password !== password) {
      throw new Error(""The given password is wrong. Try again"");
    }
 
    return ""Successfully logged"";
  }
}

Repare que além de definirmos a interface para o gateway a ser utilizado na implementação da regra de negócio, não existe nenhuma relação de dependência do código com a classe que realiza a integração com o BD. Ao invés disso, o construtor da classe recebe um gateway do tipo UserGateway já instanciado. A classe SQLUserGateway, responsável por fazer a integração com BD, fica da seguinte forma:

import { Sequelize, QueryTypes } from 'sequelize';
import { User, UserGateway } from './login';
 
export default class SQLUserGateway implements UserGateway {
  private _conn: Sequelize;
 
  constructor() {
    this._conn = new Sequelize(
      'database',
      'username',
      'password',
      {
        host: 'localhost',
        dialect: 'mysql'
      }
    );
  }
 
  async findUserByNickname(nickname: string): Promise<User> {
    const users = await this._conn.query(`SELECT * FROM users WHERE nickname = ${nickname}`, {
      type: QueryTypes.SELECT,
      raw: true
    }) as User[];
 
    return users[0];
  }
} 

Agora é a classe SQLUserGateway que cria uma relação de dependência ao implementar a interface fornecida pela classe Login. O Princípio da Inversão de Dependência foi aplicado com sucesso, uma vez que a classe de mais baixo nível, SQLUserGateway, passou a depender da classe de mais alto nível, Login.

Para ilustrar como DIP foi aplicado no nosso exemplo, observe os dois diagramas abaixo, que representam o antes e o depois da relação de dependência entre os componentes:

Antes:

Depois:

Antes, a seta, que representa uma relação de dependência, sai do componente de alto nível, representado pela cor amarela, em direção ao componente de baixo nível, representado pela cor azul. Depois, após a aplicação do DIP, a seta pontilhada, que representa um relacionamento de implementação, sai do componente de baixo nível em direção ao componente de alto nível, invertendo a relação de dependência. A dependência do componente de alto nível com uma interface é um contrato com um componente abstrato, isto é, uma abstração. Por isso, mudanças nos componentes de baixo nível que implementam esse contrato não implicarão em necessidade de mudanças nos componentes de alto nível. 

Não foi mostrado no exemplo, mas em sistemas que aplicam o Princípio da Inversão de Dependência é necessária a criação de um componente cuja única responsabilidade é instanciar classes e injetá-las nos construtores dos componentes de alto nível. Geralmente são as classes “Main” do sistema, que ficam a cargo de gerar os containers da aplicação. O node.js possui um pacote muito bom para implementar injeção de dependência de forma automática. Caso tenha interesse, o nome dele é Awilix.

Considerações Finais

Como vimos, o DIP nos orienta a depender de abstrações e não de implementações, pois as abstrações mudam com menos frequência, além de facilitar a mudança de comportamentos e tecnologias e proporcionar uma melhor (mais fácil, mais rápida e mais suave) evolução do código.

O princípio se faz mais necessário quando estamos desenvolvendo aplicações com componentes com diferentes níveis de volatilidade. A aplicação do princípio não só irá manter o código mais limpo como também auxiliará em refatorações futuras.

Assim, finalizamos a série sobre S.O.L.I.D. Nos diga se gostou deste texto e se ele lhe foi útil? Você aplica os princípios S.O.L.I.D em seus projetos ou eles são novidades para você? Esperamos que tenha gostado da série. Até a próxima! 

Artigo escrito por Filipe Mata (Tech Lead na empresa Gerencianet). Revisado por Prof. Tiago Carneiro."
"S.O.L.I.D: Conceitos e exemplos em Typescript – Parte 4, o “I”",http://www2.decom.ufop.br/terralab/s-o-l-i-d-conceitos-e-exemplos-em-typescript-parte-4-o-i/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/04/SOLID-11-730x350.png,"Um bom engenheiro de software precisa conhecer os princípios designados pelas letras que formam o acrônimo S.O.L.I.D:

S – Single Responsibility Principle 
O – Open-Closed Principle 
L – Liskov Substitution Principle 
I – Interface Segregation Principle 
D – Dependency Inversion Principle 

Este é o quarto artigo de uma série inteiramente dedicada a estes princípios. Caso ainda não tenha lido os artigos anteriores da série, sobre as letras “S”, “O” e “L”, sugerimos a leitura antes que continue: Princípio da Responsabilidade Única, Princípio Aberto/Fechado e  Princípio da Substituição de Liskov, respectivamente. 

A seguir iremos aprofundar em nossa explicações sobre a letra “I” – Princípio da Segregação de Interface (The Interface Segregation Principle).

ISP – Princípio da Segregação de Interface

Este princípio foi proposto por Robert C. Martin, mais conhecido Uncle Bob, enquanto ele realizava uma consultoria para a empresa Xerox, algumas décadas atrás. Na época, a Xerox possuía um sistema de impressora que executava uma série de tarefas. O problema com esse sistema estava concentrado em uma única classe que continha uma série de responsabilidades, sendo assim utilizada por uma infinidade de tarefas. Dá pra ter uma ideia do receio que qualquer desenvolvedor desse sistema tinha ao realizar uma manutenção nessa classe. Qualquer bug no código, por menor que fosse, comprometeria uma boa parte do sistema.

Para demonstrar melhor este cenário, veja o diagrama abaixo:

Imagine que a classe OPS representa a superclasse da Xerox que fornecia uma série de operações para tarefas distintas. Neste cenário, a tarefa representada pelo componente Task1 precisaria somente da operação op1(), a tarefa Task2 somente da operação op2() e a tarefa Task3 somente da operação op3().

Agora, imagine que a classe OPS foi implementada em uma linguagem estaticamente tipada, como Java ou C++. O código da Task1 fica dependente das operações op2() e op3(), mesmo que estas operações não sejam executadas no fluxo da tarefa Task1. Assim, qualquer mudança no código do método op2() irá forçar as tarefas Task1 e Task3 a serem compiladas e reimplantadas, mesmo que os métodos utilizados por estas tarefas não tenham sido alterados.

Este era o cenário da Xerox na época, e a solução proposta por Robert Martin foi a aplicação de uma camada de interfaces entre a superclasse, exemplificada pela classe OPS, e as tarefas que executavam as operações fornecidas por esta classe, conforme ilustra a figura abaixo. Mais tarde essa solução ficou conhecida como o Princípio da Segregação de Interface.

Repare que agora cada tarefa do sistema possui sua própria interface, contendo somente os métodos que serão utilizados pela tarefa. Assim, a tarefa Task1 irá depender somente da interface Task1Ops, e consequentemente da operação op1(), que é a operação que realmente importa para a tarefa Task1. Portanto, qualquer mudança nos métodos op2() e/ou op3() na classe OPS não irá forçar uma recompilação e uma nova implantação (deploy) da classe Task1.

Exemplos Práticos

Imagine a seguinte interface:

export interface Carro {
  acelerar();
  frear();
}

Agora temos duas classes para implementar a interface Carro: 

export class Fiat implements Carro {
  acelerar() {
    console.log('O Fiat está sendo acelerada');
  }
 
  frear() {
    console.log('Você acabou de frear o Fiat');
  }
}
 
export class Ford implements Carro {
  acelerar() {
    console.log('O Ford está sendo acelerada');
  }
 
  frear() {
    console.log('Você acabou de frear o Ford');
  }
}

Até aqui tudo certo. Temos as classes Ford e Fiat implementando todos os métodos da interface Carro. Obrigatoriamente todo carro deve acelerar e frear. Nesse sentido, o ISP está sendo aplicado da maneira correta. 

Agora imagine que algum desenvolvedor teve a brilhante ideia de implementar um carro bem específico com funções bem peculiares. A primeira ação dele foi alterar a interface carro, como pode ser visto:

export interface Carro {
  acelerar();
  frear();
  viajarNoTempo();
}
 
export class DeLorean implements Carro {
  acelerar() {
    console.log('O DeLorean está sendo acelerado');
  }
 
  frear() {
    console.log('Você acabou de frear o DeLorean');
  }
 
  viajarNoTempo() {
    console.log('Vamos de volta para o futuro com o DeLorean');
  }
}

Aparentemente está tudo certo com a nova classe implementando o clássico DeLorean de “De volta para o futuro”, mas veja o que o Typescript nos alerta com relação às outras classes:

Aqui o compilador está nos forçando a implementar o método viajarNoTempo() nas classes Fiat e Ford, por mais que este comportamento não exista nestes tipos de carro. Assim, nosso inexperiente desenvolvedor teve outra brilhante (ou não) ideia de implementar este método de uma maneira bem estranha:

export class Fiat implements Carro {
  acelerar() {
    console.log('O Fiat está sendo acelerada');
  }
 
  frear() {
    console.log('Você acabou de frear o Fiat');
  }
 
  viajarNoTempo() {
    throw new Error(""Este comportamento não funciona para este carro"");
  }
}
 
export class Ford implements Carro {
  acelerar() {
    console.log('O Ford está sendo acelerada');
  }
 
  frear() {
    console.log('Você acabou de frear o Ford');
  }
 
  viajarNoTempo() {
    throw new Error(""Este comportamento não funciona para este carro"");
  }
}

 Nosso desenvolvedor não sabe, mas ele acabou de ferir o Princípio da Segregação de Interface. Ele fez isso no momento em que ele forçou as classes Fiat e Ford a implementar um método que elas não precisam. Pior, a implementação não faz sentido algum, já que ela só lança uma exceção.

O grande problema dessa implementação é que, ao longo do tempo, pode ser que o método viajarNoTempo() sofra algumas alterações como, por exemplo, receber um argumento referente ao ano para o qual o dono do carro deseja viajar. Todas as classes que implementam a classe Carro precisarão ser alteradas para satisfazer esta mudança, quando na verdade somente a classe DeLorean implementa, de fato, o método de viagem no tempo. Somado a isso, existe o fato de que, se este código estivesse escrito em linguagens como C++ ou Java, todas as classes que implementam a interface Carro deveriam ser re-compiladas. Tudo isso parece um trabalho bobo, mas imagine que ao invés de apenas 3 classes implementando a interface Carro, tenhamos 1000 ou 10000 classes.

Podemos resolver este problema segregando as interfaces do nosso exemplo, de modo que a interface Carro contenha somente os métodos acelerar() e frear(), e tenhamos mais uma interface responsável por disponibilizar somente comportamentos referentes a viagem no tempo. Veja o exemplo:

export interface Carro {
  acelerar();
  frear();
}
 
export interface MaquinaDoTempo {
  viajarNoTempo();
}

Agora, as classes Fiat e Ford não são obrigadas a implementar o método viajarNoTempo() ou qualquer outro que não esteja definido no contrato da interface Carro. No entanto, como o DeLorean é ao mesmo  tempo um carro e uma máquina do tempo, podemos fazer algo como a seguinte implementação:

export class DeLorean implements Carro, MaquinaDoTempo {
  acelerar() {
    console.log('O DeLorean está sendo acelerado');
  }
 
  frear() {
    console.log('Você acabou de frear o DeLorean');
  }
 
  viajarNoTempo() {
    console.log('Vamos de volta para o futuro com o DeLorean');
  }
}

 Ao realizar estas alterações voltamos a respeitar o ISP, já que isolamos as interfaces da aplicação através das diferenças de responsabilidade e comportamentos, e, portanto, passamos a ter interfaces específicas ao invés de interfaces genéricas.

Considerações Finais

Vimos que o Princípio da Segregação de Interface está relacionado a coesão de interfaces, definindo que classes ou componentes clientes não devem ser forçados a depender de métodos que eles não precisam. 

Vale ressaltar, também, que esse princípio está restrito ao tipo de linguagem que você está utilizando no desenvolvimento das suas aplicações. Se você está desenvolvendo em linguagens tipadas dinamicamente como Python ou Ruby, declarações de tipo e dependências de interface não existem no código fonte. Ao invés disso, elas são inferidas em tempo de execução e, portanto, não existem dependências de código que forcem a recompilação e um novo deploy. Por isso é que linguagens dinamicamente tipadas criam sistemas mais flexíveis e menos acoplados do que quando desenvolvemos em linguagens estaticamente tipadas como o Java. Assim, o ISP é mais vinculado a uma questão de linguagem do que de design.

Nos diga se gostou deste texto e se ele lhe foi útil? Você aplica os princípios S.O.L.I.D em seus projetos ou eles são novidades para você? Fique de olho em nossas redes sociais, o próximo artigo desta série irá se aprofundar no quinto e último princípio do acrônimo, o “D”, ou mais especificamente, o Princípio da Inversão de Dependência. Aguardamos você lá! 

Artigo escrito por Filipe Mata (Tech Lead na empresa Gerencianet). Revisado por Prof. Tiago Carneiro."
"S.O.L.I.D: Conceitos e exemplos em Typescript – Parte 3, o “L”",http://www2.decom.ufop.br/terralab/s-o-l-i-d-conceitos-e-exemplos-em-typescript-parte-3-o-l/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/04/SOLID-9-730x350.png,"Se seu objetivo atual é tornar-se um bom engenheiro de software, é essencial que você conheça os princípios designados pelas letras que formam o acrônimo S.O.L.I.D:

S – Single Responsibility Principle
O – Open-Closed Principle
L – Liskov Substitution Principle
I – Interface Segregation Principle
D – Dependency Inversion Principle

Este é o terceiro artigo de uma série inteiramente dedicada a estes princípios. Caso ainda não tenha lido os artigos anteriores da série, sobre as letras “S” e “O”, sugerimos a leitura antes que continue: Princípio da Responsabilidade Única e Princípio Aberto/Fechado, respectivamente.

A seguir iremos aprofundar em nossa explicações sobre a letra “L” – Princípio da Substituição de Liskov (Liskov Substitution Principle).

LSP – Princípio da Substituição de Liskov

Este pode ser considerado um dos princípios chaves do S.O.L.I.D, já que o mesmo define o conceito de tipos abstratos e interfaces, sendo, assim, crucial para a aplicação dos outros princípios do S.O.L.I.D.

A criadora do princípio é uma das maiores referência da Ciência da Computação no mundo, a doutora Barbara Liskov, a primeira a mulher a obter um PhD em Ciência da Computação nos Estados Unidos, e também conhecida por inventar o conceito de Tipo Abstrato de Dados (TAD), que é um intimamente ligado com o princípio LSP.

Em 1988, Liskov escreveu o seguinte para definir subtipos:

“O que é desejado aqui é algo como a seguinte propriedade de substituição: Se para cada objeto O1 do tipo S existe um objeto O2 do tipo T, de modo que, para todos os programas P definidos em termos de T, o comportamento de P fica imutável quando O1 é substituído por O2, então S é um subtipo de T.”

Apesar de complexo, o que este princípio diz é que, em uma aplicação um objeto de um dado tipo pode ser facilmente substituído por outro objeto de um tipo derivado sem qualquer tipo de impacto. Claramente estamos falando dos conceitos de abstração e definição de contratos por interface. Para demonstrar de uma melhor maneira este princípio, vamos imaginar um programa de um banco que exibe um relatório completo das tarifas de seus clientes. No relatório temos as informações de documento e tarifa do cliente. Veja o exemplo abaixo:

export interface Cliente {
  recuperarDocumento();
  calcularTarifa();
}

export class Program {
  private _clientes: Cliente[];

  constructor(clientes: Cliente[]) {
    this._clientes = clientes;
  }

  exibirRelatorio() {
    console.log('Documento | Tarifa');
    this._clientes.forEach((cliente) => {
      const documento = cliente.recuperarDocumento();
      const tarifa = cliente.calcularTarifa();

      console.log(`${documento} | ${tarifa}`);
    })
  }
}


Repare que o programa interage com um tipo que representa o cliente do banco. É esperado que o programa possa extrair o documento e calcular a tarifa desse cliente, independente de quais características específicas este cliente possua. Ao fazer isto, estamos fechando um contrato do programa que emite o relatório com a interface do tipo cliente. Assim, o programa consegue lidar com quaisquer tipos que respeitem o contrato da interface do cliente. A seguir podemos ver alguns exemplos de classe que representam tipos específicos de cliente, o cliente pessoa física e o cliente pessoa jurídica:

export class ClientePF implements Cliente {
  private _cpf: string;

  constructor(cpf: string) {
    if (cpf.length !== 11) {
      throw new Error(""O CPF de um Cliente pessoa física deve conter exatamente 11 dígitos"");
    }

    this._cpf = cpf;
  }

  recuperarDocumento() {
    return this._cpf;
  }

  calcularTarifa() {
    /** regras para calcular tarifa de um cliente PF */
  }
}

export class ClientePJ implements Cliente {
  private _cnpj: string;

  constructor(cnpj: string) {
    if (cnpj.length !== 14) {
      throw new Error(""O CNPJ de um Cliente pessoa jurídica deve conter exatamente 14 dígitos"");
    }

    this._cnpj = cnpj;
  }

  recuperarDocumento() {
    return this._cnpj;
  }

  calcularTarifa() {
    /** regras para calcular tarifa de um cliente PJ */
  }
}


Temos duas classes que representam tipos diferentes de clientes, cada uma com suas regras de negócio específicas, porém ambas respeitam o contrato da interface Cliente. Assim, o programa está preparado para receber em seu construtor qualquer uma das duas implementações.

Neste exemplo utilizamos o conceito de interfaces para demonstrar o LSP, no entanto poderíamos facilmente tê-lo demonstrado através do uso de tipo abstrato de dados, como foi feito no artigo anterior.

Violação do LSP – Problema do Quadrado/Retângulo

Um quadrado é um retângulo? Este é um caso clássico de violação do LSP. Um programa interage com um tipo abstrato de dados, Retângulo, que fornece uma interface com métodos para editar a altura e a largura do retângulo. Bem, um quadrado também possui largura e altura, então poderíamos definir um tipo Quadrado como subtipo de Retângulo, como é mostrado no diagrama abaixo. Mas seria que poderíamos mesmo?

Na verdade, Quadrado não é um subtipo de Retângulo, pois a altura e largura de um retângulo podem mudar de forma independente. O mesmo não acontece com um quadrado, já que, obrigatoriamente, a largura e altura de um quadrado devem ter sempre o mesmo valor. Uma vez que o programa irá interagir com o tipo Retângulo, algumas coisas podem dar errado:

class Programa {
  private _retangulo: Retangulo;

  constructor(retangulo: Retangulo) {
    this._retangulo = retangulo;
  }

  exec() {
    this._retangulo.setLargura(2);
    this._retangulo.setAltura(5);

    console.log('Resultado: ', this._retangulo.getArea() === 10); // Verdadeiro ou Falso?
  }
}


Se passarmos uma instância de Quadrado no construtor do programa, como mostra o código abaixo, o resultado a ser exibido na tela será falso.

let retangulo: Retangulo = new Quadrado();
let p = new Programa(retangulo);
p.exec();


Algo que poderia ser feito, neste caso, é adição de uma condição no programa para verificar se o retângulo recebido no construtor é um quadrado ou não. Porém, uma vez que o comportamento do programa depende do tipo que ele está recebendo no construtor, estes tipos não são substituíveis e, portanto, é uma violação clara do LSP.

Você pode estar pensando que uma alternativa válida seria sobrescrever o método getArea() na classe Quadrado. No entanto, ainda que este método fosse sobrescrito, isto não impediria que o programa definisse a altura e a largura do quadrado de forma independente. Pior, se o método que getArea() levasse em consideração somente uma propriedade do retângulo, altura ou largura, não faria sentido criar um setter para a propriedade não aproveitada. Nos dois casos, o programa imprimiria falso, pois a área do quadrado não seria igual a 10, se utilizasse a largura como parâmetro válido a área calculada seria 4 e se utilizasse a altura a área seria 25.  Isso deixa mais evidente que Quadrado não é um subtipo de Retângulo.

Considerações Finais

O Princípio da Substituição de Liskov é um dos mais fantásticos do S.O.L.I.D, pois, como vimos, é através dele que conseguimos alcançar maior flexibilidade na construção de aplicações, através da substituição de tipos, com uso de tipos abstratos de dados ou contratos por interface. É um princípio de suma importância para entendimento e aplicação dos outros princípios do S.O.L.I.D.

Nos diga se gostou deste texto e se ele lhe foi útil? Você aplica os princípios S.O.L.I.D em seus projetos ou eles são novidades para você? Fique de olho em nossas redes sociais, o próximo artigo desta série irá se aprofundar no terceiro princípio do acrônimo, o “I”, ou mais especificamente, o Princípio da Segregação de Interfaces. Aguardamos você lá!

Artigo escrito por Filipe Mata (Tech Lead na empresa Gerencianet). Revisado por Prof. Tiago Carneiro."
"S.O.L.I.D: Conceitos e exemplos em Typescript – Parte 2, o “O”",http://www2.decom.ufop.br/terralab/s-o-l-i-d-conceitos-e-exemplos-em-typescript-parte-2-o-o/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/04/SOLID-6-730x350.png,"Se você ainda não ouviu falar do acrônimo S.O.L.I.D e, principalmente, se o seu objetivo atual é tornar-se um bom engenheiro de software, é essencial que você conheça cada um dos princípios designados pelas letras que formam esse acrônimo:

S – Single Responsibility Principle 
O – Open-Closed Principle 
L – Liskov Substitution Principle 
I – Interface Segregation Principle 
D – Dependency Inversion Principle 

Este é o segundo artigo de uma série inteiramente dedicada aos princípios S.O.L.I.D. Caso ainda não tenha lido o primeiro artigo da série, sobre a letra “S”, sugerimos  a leitura antes que continue: Princípio da Responsabilidade Única 

A seguir iremos aprofundar em nossa explicações sobre a letra “O” – Princípio Aberto/Fechado (Open/Closed Principle).

OCP – Princípio Aberto/Fechado

Este princípio foi criado em 1988 pelo acadêmico francês Bertrand Meyer, criador da linguagem de programação Eiffel e do conceito de Design por contrato. Ele diz:

“Um artefato de software deve ser aberto para extensões, mas fechado para modificações”

Na rotina de trabalho de um engenheiro de software, construir aplicações novas é quase sempre visto como algo muito bom, mas nem sempre esse olhar se aplica quando se fala em manutenção de código.

Provavelmente, em algum momento, você já deve ter ouvido algum relato sobre um grupo de desenvolvedores que já esteve em apuros para dar manutenção em códigos de aplicações legadas. Isto é, código de aplicações que foram desenvolvidas no passado (às vezes muito distante) e que continuam a atender as necessidades de seus usuários. 

A manutenção de aplicações legadas acontece porque, muitas vezes, seu código deveria manter suas regras de negócio intactas ao longo do tempo, mas acaba sofrendo uma série de modificações para atender as inúmeras mudanças de requisitos e escopos que comumente ocorrem no ciclo de vida de um software. E aí, o que era mil maravilhas no início do projeto, acaba virando um inferno depois de alguns anos ou até mesmo meses de trabalho.

O problema não está nas mudanças de requisitos ou escopo, até porque este tipo de situação é mais do que comum no mundo do desenvolvimento de software. O problema em geral reside na maneira como estas alterações são refletidas no código. Se o seu sistema possui uma classe com única responsabilidade (lembre o SRP), que já funciona perfeitamente bem, entregando exatamente aquilo que ela foi proposta a entregar, no momento em que você acrescenta um função nessa classe para atender a outro requisito que nada tem a ver com o propósito da classe, você não só atribui mais uma responsabilidade a classe como também corre o risco de comprometer o funcionamento das regras de negócio originais da classe. É aí que o OCP, ou Princípio Aberto/Fechado, deve entrar em ação.

Um bom design de aplicação deve oferecer padrões que possibilitem que o desenvolvedor realize o menor número de manutenções possíveis. Isto pode ser feito através de dois conceitos chave: Extensão e Abstração.

A ideia é que, se a aplicação possui uma classe, por exemplo, que já tem suas regras bem definidas, ao receber novas demandas de requisitos que fogem do escopo da classe, porém estão intimamente interligados com o que a classe faz, nós iremos estender o comportamento dessa classe em classe mais específicas, de modo que a classe estendida abstraia as funcionalidades que serão implementadas nas classes concretas. Para ficar mais claro, observe o código abaixo:

import { promises as fs } from 'fs';
 
interface ReportRegister {
  workedHours: number;
  employeeName: string;
}
 
export class HourReport {
  private reportData: Array<ReportRegister>;
 
  constructor(data: Array<ReportRegister>) {
    this.reportData = data;
  };
 
  async export() {
    let content: string = 'Name | Worked Hours\n';
 
    this.reportData.forEach((reportReg) => {
        content = content + `${reportReg.employeeName} | ${reportReg.workedHours}\n`;
    });
 
    const currentDateTime = new Date().getTime();
 
    await fs.writeFile(`./report-${currentDateTime}.txt`, content, 'utf8');
  }
}


Neste código temos uma classe que representa um relatório de horas trabalhadas por um funcionário (lembre-se da classe HourReporter do artigo anterior).

Nesta classe é possível observar um método para exportação do relatório no formato de um arquivo de texto. Isto aconteceu porque, provavelmente, o Departamento Pessoal da empresa solicitou essa funcionalidade. Porém, depois de um tempo, a Diretoria de Operações percebe que o relatório no formato de texto não é o mais adequado para os padrões da empresa. Será necessário gerar relatórios no formato PDF.

Nesse sentido, a primeira ideia que pode vir na cabeça do desenvolvedor é alterar o método export() da classe HourReport, de modo que o mesmo passe a manipular e escrever arquivos PDF ao invés de arquivos em formato de texto. 

A ideia parece simples e até poderia resolver o problema da Diretoria de Operações, porém alguns fatores devem ser levados em conta:

O desenvolvedor precisou fazer alterações em um código que já funciona para atender a um requisito que pode mudar a qualquer momento. Assim, se na hora de implementar a integração com a biblioteca de manipulação de PDF o desenvolvedor deixou algum erro passar, todas as regras de negócio envolvendo a estrutura e os dados do relatório estariam comprometidas também.
Hoje a Diretoria de Operações deseja um relatório no formato PDF, mas pode ser que amanhã ou depois ela queira retomar os relatórios no formato TXT bem como solicitar relatórios no formato DOC. A cada vez que um novo formato de documento for solicitado pela Diretoria de Operações, o desenvolvedor terá que fazer alterações na classe HourReport, correndo o risco de inserir uma falha na mesma.

Para resolver este problema, podemos aplicar os conceitos de Extensão e Abstração para atender o OCP. Veja qual seria a melhor maneira de escrevermos a classe HourReport.

export abstract class HourReport {
  protected reportData: Array<ReportRegister>;
 
  constructor(data: Array<ReportRegister>) {
    this.reportData = data;
  };
 
  toString() {
    let content: string = 'Name | Worked Hours\n';
 
    this.reportData.forEach((reportReg) => {
        content = content + `${reportReg.employeeName} | ${reportReg.workedHours}\n`;
    });
 
    return content;
  }
 
  /** 
   * ...
   * Aqui podem ter outros métodos que implementam regras de negócio específicas de relatórios de horas
   * ...
   */
 
  abstract export();
}


Repare que isolamos as regras de negócio mais críticas referente a criação e manipulação de relatórios dentro da classe HourReport, que agora é uma classe abstrata. Assim, definimos que o método export, que representa um requisito que tende a mudar com mais frequência, passa a ser um método abstrato.

Tudo que precisamos fazer agora é criar extensões para a nossa abstração, ou seja, plugins que implementem o método export() de maneiras diferentes, atendendo as demandas da Diretoria de Operações. Veja como pode ser implementado o plugin para gerar relatórios de horas no formato TXT:

import { promises as fs } from 'fs';
 
export class TxtHourReport extends HourReport {
 
  async export() {
    const currentDateTime = new Date().getTime();
    await fs.writeFile(`./report-${currentDateTime}.txt`, this.toString(), 'utf8');
  }
}


O mesmo pode ser feito para qualquer outro formato que a Diretoria de Operações queira. Veja os exemplos abaixo:

export class DocHourReport extends HourReport {
  
  async export() {
    /** Manipula e escreve um novo arquivo no formato .docx */
  }
}
 
export class PDFHourReport extends HourReport {
  
  async export() {
    /** Manipula e escreve um novo arquivo no formato .pdf */
  }
}


Com os plugins criados, a aplicação irá instanciar a classe específica baseada na escolha de formato do usuário da aplicação. Assim, se o analista do Departamento Pessoal solicitar um relatório no formato PDF, a aplicação irá instanciar a classe PDFHourReport. 

Considerações Finais

Como visto acima, a aplicação do Princípio Aberto/Fechado – OCP nos direciona a um design que separa as regras de negócio mais críticas da aplicação daqueles comportamentos que tendem a sofrer modificações frequentes. Esta separação se dá através da criação de plugins para as regras de negócio da aplicação. Assim, sempre que houver necessidade de implementação de uma funcionalidade que se adeque às estas regras de negócio, como um novo formato de relatório, no exemplo acima, iremos criar um plugin para esta nova funcionalidade, ao invés de alterar um código que já funciona. Isso garante mais qualidade e maior facilidade na manutenção futura de códigos.

Nos diga se gostou deste texto e se ele lhe foi útil? Você aplica os princípios SOLID em seus projetos ou eles são novidades para você? Fique de olho em nossas redes sociais, o próximo artigo desta série irá se aprofundar no terceiro princípio do acrônimo, o “L”, ou mais especificamente, o Princípio da Substituição de Liskov. Aguardamos você lá!

Artigo escrito por Filipe Mata (Tech Lead na empresa Gerencianet). Revisado por Prof. Tiago Carneiro."
"S.O.L.I.D: Conceitos e exemplos em Typescript – Parte 1, o “S”",http://www2.decom.ufop.br/terralab/s-o-l-i-d-conceitos-e-exemplos-em-typescript-parte-1-o-s/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/03/SOLID-3-730x350.png,"Para você que não conhece, S.O.L.I.D é um acrônimo para 5 princípios de Orientação a Objeto que definem o que é, de fato, um bom design de aplicação. 

Estes princípios foram definidos nos anos 2000 por Robert C. Martin e Michael Feathers, no intuito de promover técnicas e conceitos que facilitassem o desenvolvimento e manutenção de sistemas complexos de computação.

Caso você não conheça, Robert C. Martin, mais conhecido como Uncle Bob é uma das referências em engenharia e arquitetura de software no mundo, sendo um dos assinantes do Manifesto Ágil, além de criador de conceitos consagrados na comunidade como Clean Code e Clean Architecture. Não distante, Michael Feathers, também é reconhecido como um dos grandes nomes do mundo do desenvolvimento de software, foi, de fato, o criador do acrônimo S.O.L.I.D.

Se você já ouviu falar de Clean Code e Clean Architecture saiba que estes princípios formam a pedra angular de todos estes conceitos. Assim, se o seu objetivo é tornar-se um bom engenheiro de software, é de suma importância o entendimento destes princípios antes de aprofundar em qualquer modelo arquitetural mais rebuscado. 

Cada letra do acrônimo representa os seguintes conceitos:

S – Single Responsibility Principle (Princípio da Responsabilidade Única)
O – Open-Closed Principle (Princípio Aberto/Fechado)
L – Liskov Substitution Principle (Princípio da Substituição de Liskov)
I – Interface Segregation Principle (Princípio da Segregação de Interface)
D – Dependency Inversion Principle (Princípio da Inversão de Dependência)

Neste artigo, iremos aprofundar no primeiro princípio, fazendo uso de alguns exemplos práticos em Typescript do que deve e do que não deve ser feito em designs que tenham a pretensão de seguir o Princípio da Responsabilidade Única, ou SRP. 

SRP – Princípio da Responsabilidade Única

“Cada módulo de uma aplicação deve ter uma, e somente uma, razão para mudar”

Erroneamente muitos programadores, ao se depararem com este princípio, assumem que cada módulo deve realizar apenas uma ação.Isto ocorre pois existe um conceito que fala que cada função de uma classe deve fazer apenas uma coisa. Do ponto de vista da refatoração de sistemas isto está correto, porém não é o que o Princípio da Responsabilidade Única é.

Primeiramente devemos entender que um módulo nada mais é do que a maneira que os códigos da sua aplicação são armazenados ou agrupados. Pode ser por arquivo ou simplesmente através de funções ou estruturas de dados. 

Cada módulo do seu sistema é implementado para satisfazer as necessidades de um determinado grupo de usuários ou stakeholders (qualquer pessoa interessada no projeto). Essas necessidades geralmente são definidas na forma de requisitos e este grupo específico de usuários são os atores do sistema.

Uma vez que estes conceitos estejam claros, podemos entender que o SRP define que cada módulo da sua aplicação, feito exclusivamente para um ator específico do sistema, deve mudar apenas para atender as necessidades deste ator. Se um determinado módulo se propõe a satisfazer as necessidades de dois atores distintos, o SRP será quebrado, já que ao satisfazer as mudanças de requisitos de um ator os requisitos do outro ator podem ficar comprometidos. O módulo deixa de ter apenas uma responsabilidade.

Abaixo é mostrado um exemplo clássico de violação do SRP:

class Employee {
  public calculatePay() {}
  public reportHours() {}
  public save() {}
}


A classe Employee representa os funcionários de uma determinada empresa. Nela podemos observar 3 métodos atendendo 3 necessidades distintas:

O método calculatePay(), que tem por objetivo calcular o pagamento do funcionário, atende a uma demanda do setor financeiro da empresa, ou seja, neste caso, a classe Employee é responsável por mudanças requisitadas por um ator que representa a diretoria de finanças da da empresa.
O método reportHours(), responsável por gerar relatórios de horas trabalhadas ao departamento pessoal, atende aos requisitos estabelecidos por um ator que representa a diretoria de operações da empresa.
O método save(),  que define como um funcionário é cadastrado na empresa, é especificado por DBAs, já que os mesmos são as pessoas mais hábeis para garantir a persistência dos dados de forma performática. Assim, a classe Employee passa a ser responsável por mudanças requisitadas pela diretoria de tecnologia da empresa.

A classe Employee possui 3 responsabilidades distintas e com isso alguns problemas podem aparecer. Se, por exemplo, a diretoria de finanças especificar uma mudança na forma como as horas trabalhadas são calculadas, para suprir a uma mudança no cálculo do pagamento, o relatório de horas pode ser gerado com erros para a diretoria de operações. 

Algo similar acontece quando, por exemplo, a diretoria de tecnologia resolve definir uma nova forma de armazenamento para os dados do funcionário. Neste cenário, uma mudança no método save(), pode acarretar em modificações indesejadas em toda a classe e, consequentemente, em resultados errados tanto para a diretoria de finanças quanto para a diretoria de operações.

Uma das soluções para este problema é definir uma classe EmployeeData, sem métodos, contendo apenas as propriedades de um funcionário. Tal classe seria compartilhada entre 3 classes distintas, cada uma sendo responsável por apenas um dos diferentes atores do nosso exemplo. Veja o exemplo abaixo:

class EmployeeData { }
 
class PayCalculator {
  public calculatePay(data: EmployeeData) {}
}
 
class HourReporter {
  public reportHours(data: EmployeeData) {}
}
 
class EmployeeRepository {
  public saveEmployee(data: EmployeeData) {}
}


Agora as classes PayCalculator, responsável por atender os requisitos da diretoria de finanças, HourReporter, responsável por atender os requisitos da diretoria de operações, e EmployeeRepository, responsável por atender as especificações da diretoria de tecnologia, são isoladas uma das outras. Assim, mudanças em qualquer uma das classes não impactarão em nada nas outras classes e, portanto, cada classe do nosso exemplo passam a  possuir uma e somente uma responsabilidade.

A única desvantagem dessa solução é a quantidade de instâncias que precisamos criar para iniciar a aplicação do exemplo. Para contornar este problema pode-se utilizar o padrão de projeto de software “Facade” , como mostrado no exemplo abaixo:

class EmployeeData { }
 
class PayCalculator {
  public calculatePay(data: EmployeeData) {}
}
 
class HourReporter {
  public reportHours(data: EmployeeData) {}
}
 
class EmployeeRepository {
  public saveEmployee(data: EmployeeData) {}
}
 
class Employee {
  private data: EmployeeData;
  private payCalculator: PayCalculator;
  private hourReporter: HourReporter;
  private employeeRepository: EmployeeRepository;
 
  constructor(data: EmployeeData) {
    this.data = data;
    this.payCalculator = new PayCalculator();
    this.hourReporter = new HourReporter();
    this.employeeRepository = new EmployeeRepository();
  }
  
  public calculatePay() {
    return this.payCalculator.calculatePay(this.data);
  }
 
  public reportHours() {
    return this.hourReporter.reportHours(this.data);
  }
 
  public save() {
    return this.employeeRepository.saveEmployee(this.data);
  }
}


Repare que voltamos com a classe Employee para o nosso exemplo, com uma estrutura bem similar àquela mostrada no início. No entanto, agora, ela tem o  único propósito de ser uma fachada para as outras classes. A classe Employee possui 3 métodos, mas ela delega as regras de negócio mais críticas, ou seja, suas responsabilidades, para as classes PayCalculator, HourReporter e EmployeeRepository. 

Considerações Finais

Como visto no exemplo, o Princípio da Responsabilidade Única – SRP nada tem a ver com o número de funções ou métodos que um componente ou classe possui, mas sim com o número de responsabilidades que o mesmo tem. Cada uma das classes do nosso exemplo possui uma e somente uma razão para mudar e este é o propósito do SRP.

Nos diga se gostou deste texto e se ele lhe foi útil? Você aplica os princípios SOLID em seus projetos ou eles são novidades para você? Fique de olho em nossas redes sociais, o próximo artigo desta série irá se aprofundar no segundo princípio do acrônimo, o “O”, ou mais especificamente, o Princípio Aberto/Fechado. Aguardamos você lá! 

Artigo escrito por Filipe Mata (Engenheiro de Software na empresa Gerencianet). Revisado por Prof. Tiago Carneiro."
Como simular a AWS no seu computador e utilizar o serviço de filas SQS,http://www2.decom.ufop.br/terralab/como-simular-a-aws-no-seu-computador-e-utilizar-o-servico-de-filas-sqs/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/03/Como-simular-a-AWS-no-seu-computador-e-utilizar-o-servico-de-filas-SQS-9-730x350.png,"Introdução

A AWS (Amazon Web Services) é uma plataforma de serviços de computação em nuvem oferecida pela amazon.com. Seus produtos, oferecidos no sistema de pay-as-you-go, são utilizados globalmente tanto por indivíduos e empresas de pequeno porte, quanto por multinacionais e governos. Atualmente, a AWS fornece mais de 200 diferentes serviços e produtos, incluindo serviços de armazenamento, redes, analytics, mobile e IoT. Alguns dos mais populares são: EC2, S3, lambda e SQS. 

É muito comum que aplicações que executam na AWS sejam divididas em diversos componentes. Assim, cada componente tem uma responsabilidade específica dentro da aplicação. Na verdade, esta é uma boa prática de Engenharia de Software, utilizada na maioria dos projetos de software, pois permite que cada componente seja desenvolvido de forma independente, por uma equipe dedicada, que evolua em seu próprio ritmo e seja fracamente acoplado aos demais componentes que formam a arquitetura desta aplicação. Componentes podem ser entendidos como módulos executáveis de código, no Windows seriam distribuídos arquivos executáveis (.exe) ou como DLL (Dynamic Link Library) e, no sistema operacional Linux, seriam distribuídos como arquivos do tipo “.so” (Shared Object) .

 Em aplicações onde se deseja robustez e alto desempenho é comum que, para promover o fraco acoplamento entre os componentes, a comunicação entre eles ocorra por meio de  filas de mensagens colocadas na entrada e/ou na saída de cada componente.  Os serviços de fila trazem os seguintes benefícios:

Resiliência: A aplicação pode continuar funcionando mesmo que um ou mais componentes falhem, as mensagens continuarão a ser processadas pelos componentes ativos até que os demais sejam restabelecidos;
Escalabilidade: A aplicação pode escalar de acordo com a demanda, isto é, a quantidade de mensagens na fila determina o número de réplicas de componentes que serão utilizadas para consumí-las. Assim, evita-se que as mensagens esperem muito para serem atendidas e o desempenho de toda aplicação degrade;
Segurança: O desacoplamento dos componentes traz segurança, já que não há requisições diretas entre eles; e
Equilíbrio: A assincronicidade no atendimento às mensagens possibilita que um componente muito rápido não sobrecarregue outro mais lento.

Este artigo tem como objetivo introduzir você ao serviço de fila SQS oferecido pela nuvem AWS. No entanto, outras nuvens oferecem serviços similares que operam segundo os mesmo princípios e possuem a mesma missão. Assim, mesmo que você esteja interessado(a) em outras nuvens, também será beneficiado(a) pela leitura deste artigo. Além disso, mostraremos como simular a AWS localmente, em seu computador pessoal, para que você possa aprender a utilizar o SQS em seus projetos.

O serviço Simple Queue Service (SQS) 

O serviço Amazon Simple Queue Service oferece sistema de filas seguras, duráveis e de alta disponibilidade que permite integrar e desacoplar sistemas de software e componentes distribuídos. As principais vantagens da utilização desse serviço são:

Segurança: Você controla quem pode enviar e receber mensagens em uma fila do Amazon SQS; 
Disponibilidade: O uso de infraestrutura redundante fornece acesso altamente simultâneo às mensagens e alta disponibilidade para produzir e consumir mensagens;
Tutorial

A seguir mostramos os passos que devem ser seguidos para a instalação e configuração do Localstack para simulação da nuvem AWS com o serviço SQS. Para isso, partimos do pressuposto de que você está utilizando o sistema operacional Linux.

Instalando o Localstack

Para fazer uma demonstração de uma aplicação com troca de mensagens, usaremos o localstack. Este projeto fornece um framework para teste de aplicações em nuvem (no momento, para as aplicações da AWS). O jeito mais fácil de instalá-lo é utilizando pip (os outros jeitos são detalhados no repositório do github). Os requisitos necessários são python 2 ou 3, pip e Docker.

pip install localstack

Após a instalação, é necessário executar o localstack…

localstack start

Agora, vamos utilizar o aws-cli para criar uma fila pelo terminal. Este cliente pode ser instalado por qualquer gerenciador de pacotes (apt, pip): 

https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html

Obs: Se nunca tiver configurado a AWS no computador, tente chamar aws configure e setar quaisquer valores (com uma region válida), como por exemplo:

AWS Access Key ID: key-id
AWS Secret Access Key: secret-ak
Default region name: us-east-1
Default output format None

Após a instalação do cliente, crie uma fila para teste. Ao passar o endpoint como localhost, direcionamos a chamada para o localstack que, neste momento, já deve estar executando em seu computador.

aws --endpoint=""http://localhost:4566"" sqs create-queue --queue-name=test_queue

O retorno de sucesso é:

{
    ""QueueUrl"": ""http://localhost:4566/000000000000/test_queue""
}
Desenvolvendo uma aplicação simplificada

Vamos criar uma aplicação simples com dois módulos que se comunicam através da nossa fila. O componente 1 recebe uma mensagem através da linha de comando e a envia através da fila.

 
const AWS = require('aws-sdk');
const queueUrl = 'http://localhost:4566/000000000000/test_queue';
 
const sendMessage = async (messageBody = 'empty') => {
   const sqs = new AWS.SQS({region: 'us-east-1'});
   const a = await sqs.sendMessage({
       QueueUrl: queueUrl,
       MessageBody: messageBody
   }).promise();
};
 
sendMessage(process.argv[2]);

O segundo componente fará a leitura da fila esperando alguma mensagem.

const AWS = require('aws-sdk');
const queueUrl = 'http://localhost:4566/000000000000/test_queue';
 
const run = async () => {
   const sqs = new AWS.SQS({region: 'us-east-1'});
 
   while(true) {
       try {
           const a = await sqs.receiveMessage({
               QueueUrl: queueUrl,
               WaitTimeSeconds: 20
           }).promise();
           console.log('Message received: ', a.Messages[0].Body);
 
           await sqs.deleteMessage({
               QueueUrl: queueUrl,
               ReceiptHandle: a.Messages[0].ReceiptHandle
           }).promise();
       } catch (err) {
           console.log(err);
           break;
       }
   }
};
 
run();

Para fazer o teste, iniciar o componente 2 na linha de comando com o comando a seguir:

node second-component.js

Depois, envie uma mensagem  para testá-lo, utilizando o comando a seguir:

node first-component.js ""aqui vai a mensagem""
Usando mais de dois componentes

Em alguns casos, pode haver necessidade de um componente enviar a mesma mensagem para mais de um outro componente. Podemos fazer a chamada do envio mais de uma vez, porém, isso se torna inviável quando o sistema aumenta de tamanho.

Para fazer isso de forma natural, contornando o problema de aumento de tráfego de mensagens, podemos usar outro serviço da AWS: O  Simple Notification Service (SNS). Vamos direcionar a mensagem a um tópico do SNS, que irá entregar para todos os componentes inscritos no mesmo. Esse padrão de envio de um para muitos é muitas vezes chamado de “Fan Out” Design Pattern.

Conheça o serviço de notificação simples (SNS)

O Amazon Simple Notification Service (SNS) é um serviço gerenciado que provê a entrega de mensagens de publicadores (publishers) para subscreventes (subscribers), em acordo com (Publisher-Subscriber Design Pattern). Os publicadores comunicam-se de forma assíncrona com os subscreventes enviando mensagens a um tópico no qual os subscreventes estarão inscritos.
Para  utilizar o SNS, conectando 2 componentes, vamos criar uma fila para o segundo componente e para um novo tópico, no qual as duas filas estarão inscritas.

Criação da segunda fila:

aws --endpoint=""http://localhost:4566"" sqs create-queue --queue-name=another_test_queue

Criação do novo tópico:

aws --endpoint=""http://localhost:4566"" sns create-topic --name=test_topic

O retorno de sucesso deve ser:

{
    ""TopicArn"": ""arn:aws:sns:us-east-1:000000000000:test_topic""
}

Para preparar as filas para receber mensagens enviadas àquele tópico, faça:

aws --endpoint=""http://localhost:4566"" sns subscribe --topic-arn=arn:aws:sns:us-east-1:000000000000:test_topic --protocol=sqs --notification-endpoint=http://localhost:4566/000000000000/test_queue 
&&
aws --endpoint=""http://localhost:4566"" sns subscribe --topic-arn=arn:aws:sns:us-east-1:000000000000:test_topic --protocol=sqs --notification-endpoint=http://localhost:4566/000000000000/another_test_queue

Após estes passos, basta enviar as mensagens no tópico e todas as aplicações inscritas nele receberão a mesma mensagem simultaneamente.

O código fonte do segundo exemplo pode ser visto em  https://github.com/bermr/sqs-demo.

Considerações Finais

Neste artigo, explicamos as motivações e os passos necessários para o uso do sistema de filas da AWS, o serviço SQS, e para o uso do sistema de distribuição de notificações, o SNS. Para ensinar a prática, nós também discutimos o código fonte de uma aplicação exemplo que utiliza esses serviços com o objetivo de obter uma comunicação rápida e resiliente. Convidamos você a deixar um feedback sobre o texto e contar quais são as suas experiências com essa arquitetura de software.

Fontes
https://aws.amazon.com/
https://en.wikipedia.org/wiki/Amazon_Web_Services
https://docs.aws.amazon.com/pt_br/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html
https://docs.aws.amazon.com/sns/latest/dg/welcome.html
https://github.com/localstack/localstack
https://github.com/bermr/sqs-demo

Artigo escrito por Bernardo Marotta de Rezende (Tech Lead na empresa Gerencianet). Revisado por Prof. Tiago Carneiro."
Serviços de software oferecidos de graça para estudantes,http://www2.decom.ufop.br/terralab/servicos-de-software-oferecidos-de-graca-para-estudantes/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/03/Servicos-de-software-oferecidos-de-graca-para-estudantes-730x350.png,"Diversas empresas oferecem alguns de seus produtos de maneira gratuita para estudantes. É uma relação de benefício mútuo. O estudante pode aproveitar um serviço de qualidade e que pode ajudá-lo durante sua graduação, sem ter custos. Já as empresas, por sua vez,  contribuem na formação dos beneficiários, estimulam a reciprocidade e impulsionam suas marcas. Elas gradativamente promovem e ampliam a comunidade de usuários destes produtos.

Pensando nisso, mostraremos neste artigo 4 serviços pagos que podem ser obtidos de maneira gratuita por estudantes. Para cada um deles, apresentaremos quais ferramentas são oferecidas e faremos um breve tutorial de como um estudante pode obtê-las de forma gratuita.

GitHub Student Developer Pack

O GitHub é uma plataforma de hospedagem e versionamento de código amplamente conhecida entre a comunidade de desenvolvedores de software. Buscando oferecer para estudantes um grupo de serviços amplamente utilizados nessa área, a empresa criou o GitHub Student Developer Pack. Neste pacote, são oferecidas assinaturas gratuitas das mais diversas plataformas voltadas para a área de desenvolvimento. Entre elas estão: Heroku, Digital Ocean, Unity, Bootstrap Studio, Canva e GitHub Pro.

Para obter acesso a esse pacote de serviços, você deve ter uma matrícula ativa em alguma instituição de ensino. Para iniciar seu cadastro, você deve possuir uma conta no GitHub. Caso não possua, crie uma utilizando seu email institucional. Caso já possua, mas sua conta não esteja vinculada a seu email institucional, você deve fazer isso acessando as configurações de sua conta e procurando a seção “Emails”, assim como na figura abaixo.

Com um email institucional já vinculado a sua conta, acesse o site GitHub Student Developer Pack. Nesse site, clique no botão “Get Your Pack”. A partir desse momento, você será redirecionado para a seguinte página:

Na parte inferior dela, há um formulário no qual você deverá responder às perguntas, que são referentes à sua instituição de ensino. Após submeter as respostas, você já terá seu acesso garantido. Para resgatar seus benefícios, basta acessar a página Student Offers.

Pacote Office

O Pacote Office é um conjunto de aplicativos criado pela Microsoft. Ele é composto de programas voltados para as mais diversas funções como gerenciamento de banco de dados, manipulação de planilhas e edição de texto. Seus serviços são amplamente utilizados nas instituições de ensino e no mercado.

O que muitos não sabem é que a Microsoft disponibiliza esse pacote de graça para estudantes. O Office Student é a versão do pacote completamente online e gratuita para alunos. 

Para obter essa versão, você deve acessar o site Office Student e inserir seu email institucional no campo destacado na imagem abaixo.

Após realizar esse passo, você será redirecionado para uma página na qual você deve preencher um formulário. Logo após concluí-lo, seu acesso ao pacote Office estará liberado.

Microsoft Azure for Students

O Azure é um dos serviços de computação em nuvem mais conhecidos, e oferece mais de 200 produtos que são distribuídos, em sua maioria, na forma de software como serviço (SaaS). Apesar dele permitir o uso de diferentes tipos de linguagens e sistemas operacionais, o seu principal diferencial é o suporte a sistemas criados com a plataforma .Net e Windows.

O programa Microsoft Azure for Students permite que você utilize alguns dos serviços da nuvem de graça por 1 ano e ainda lhe oferece US$100 (cem dólares) para aprender e testar algumas soluções mais robustas. Além disso, a plataforma te oferece cursos e certificações gratuitas que podem fazer a diferença em qualquer entrevista de emprego na área de TI.

Para fazer parte do programa é necessário ter um e-mail institucional ativo da sua universidade, e se cadastrar passando pela fase de Verificação de Identidade por Telefone e a Verificação Acadêmica.

Depois disso, você só precisa esperar pelo email da Microsoft lhe avisando da liberação da conta. Você também pode participar através do GitHub Student Developer Pack.

AWS Educate

A plataforma AWS é a maior fornecedora de serviço de computação em nuvem disponível, e possui diversos servidores espalhados pelo mundo. Através do AWS Educate a plataforma pretende educar e formar profissionais para atender o mercado de trabalho gerado pelos serviços em nuvem. Estima-se que, ao redor do mundo, este tipo de serviço em nuvem gera cerca de 18 milhões de empregos.

Dentro da plataforma é possível realizar cursos sobre machine learning, cybersecurity, data science e muitos outros, todos com oferecendo certificações ao final. Além disso, após concluir alguns cursos, podemos nos candidatar a muitas vagas de emprego oferecidas dentro da própria AWS Educate.

Esta plataforma não é feita somente para estudantes, existem formas de conseguir acesso como professor, instituição ou recrutador. Para entrar como estudante você deve ter um e-mail institucional e estar devidamente matriculado em alguma universidade.

Depois dessa etapa é só esperar um email da AWS Educate e você poderá utilizar a plataforma.
Neste artigo mostramos para você 4 recursos que podem ser obtidos de graça por estudantes. Gostou do texto? Ele foi útil para você? Conte para gente nos comentários!

Tutorial escrito por Carlos Magalhães Silva e Vinícius de Paula Silva. Revisado por Prof. Tiago Carneiro."
As Revoluções Industriais e a Evolução do Guia SCRUM,http://www2.decom.ufop.br/terralab/as-revolucoes-industriais-e-a-evolucao-do-guia-scrum/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/03/As-Revolucoes-Industriais-e-a-Evolucao-do-Guia-SCRUM-730x350.png,"De fato o planeta passou por várias revoluções, talvez um termo que soa melhor atualmente seria adaptações. Começaremos fazendo um breve histórico das revoluções industriais que estiveram presentes nos momentos mais importantes da humanidade. Esses ocorridos estruturam as empresas no presente e futuro. Vamos iniciar com um breve histórico das principais mudanças que ocorreram da primeira à quarta evolução da indústria.

Indústria 1.0

Ocorreu por volta de 1784 e foi baseada em mecanismo usando água e metal.

Indústria 2.0

Acontece quase 100 anos depois, em torno de 1870, introduzindo produção em massa com a ajuda de energia elétrica.

Indústria 3.0

Um pouco mais tarde, já nas décadas de 60 e 70 inicia-se a revolução digital com o uso de eletrônicos e tecnologia da informação para automatizar a produção.

Indústria 4.0

E nesse exato momento passamos pela quarta revolução industrial com sistemas cibernéticos e físicos, com algoritmos inteligentes, se conectando através da internet.

Existem sinais do ambiente atual que demonstram que precisamos de algum tipo de mudança, adaptabilidade ou flexibilidade. No contexto das revoluções podemos citar 5 deles. 

 Atualmente temos uma alta competitividade na maioria dos setores, causados por empresas startups, geralmente com menos de 20 anos, que são rápidas, possuem um cenário de menor número de colaboradores, com um perfil jovem e engajado. Elas possuem a estrutura organizacional mais horizontal, que muitas vezes supera os sistemas altamente hierárquicos, promovendo a tomada de decisão de forma mais ágil e comunicação mais fluida. 

Depois disso, as novas tecnologias sempre ganham espaço, até mesmo a comunicação e decisões a serem tomadas podem ser revolucionadas com novas ferramentas ou algoritmos auxiliares para uma determinada estratégia. O objetivo é dar ao talento individual uma forma de maximizar a sua produtividade. Um quarto momento é a necessidade de nos mantermos sempre conectados, essa conexão passa por uma alta adaptação de garantia de estarmos sempre ligados uns aos outros, permitindo tomadas de decisão simplesmente pelo uso de alguma plataforma. 

Por fim, o gerenciamento em escala, que abre uma comunicação honesta e sem medo para as futuras lideranças conseguirem “servir” as suas equipes. Com isso um ambiente de bastante transparência, mudança de projetos está sendo preparado para absorver a Indústria 4.0.

Nesse novo contexto, a organização precisa ser robusta, com planejamentos estratégicos bem definidos. O que significa que terá um plano mas poderá adaptá-lo a cada instante, sempre promovendo uma cultura e engajamento das pessoas. 

Tudo isso faz sinergia com a perspetiva do framework Scrum, desde a sua essência e ainda mais agora com sua atualização em novembro de 2020. O que apresentaremos aqui é simplesmente um breve resumo de como o guia Scrum está em termos de princípios, valores, papéis, artefatos e as suas principais adaptações em relação a 2017.

A íntegra do que iremos abordar você pode consultar no próprio site da Scrum.org, onde também é possível consultar várias postagens a respeito das mudanças que ocorreram, sob a perspectiva de várias abordagens. 

O seu propósito e definição mantém a base original de ser um framework leve para ajudar pessoas e organizações a gerenciar suas entregas de valor, através de adaptações para solucionar problemas complexos.

Em termos teóricos ele se baseia em três pilares, transparência, inspeção e adaptação. Ou seja, todos os papéis, eventos, artefatos, propósitos e valores, são para dar visibilidade ao trabalho em questão, permitir uma validação antes, durante e depois do processo e a qualquer momento realizar ajustes nas demandas a serem executadas.

Os valores se baseiam em comprometimento, foco, respeito, abertura e coragem. Todos eles são essenciais para a revolução industrial que estamos passando. Essa ideia de certa forma ultrapassa a teoria, chegando a ser uma necessidade para o mundo moderno.

O time Scrum entende o Dono do Produto, como um representante do cliente diante da organização, de partes interessadas e do próprio time. O Scrum Master, na posição de um líder servidor e facilitador da análise de negócio e de processos diante de todas as partes envolvidas. E por fim o time de desenvolvimento, autônomo e multifuncional para executar as demandas.

Os eventos Scrum também seguem a tradição:

Sprint: Intervalo de tempo onde uma parte do projeto é executada
Sprint Planning: Encontro onde se deve buscar responder – O que? Pra quê? Por que? Como? Quem? Se deve fazer determinado conjunto de demandas.
Daily Scrum: Um evento periódico, na grande parte diariamente, onde o time revisa a meta e analisa a estratégia de execução do que foi planejado.
Sprint Review: Momento em que o time scrum, analisa o que foi entregue, e apresenta em termos de produto os itens construídos e rapidamente levantam os possíveis débitos técnicos.
Sprint Retrospective: De uma forma mais geral todos analisam, processo, comportamento, entregas, satisfação, desempenho e qualquer outro assunto que acharem pertinente. Em busca de pontos fortes e de melhoria para manter a evolução contínua do time.

Os artefatos se conectam e também não mudaram:

Product Backlog: O produto ou projeto como um todo, a entrega por completo, o que realmente irá dar visibilidade e garantir uma entrega de valor sólida e viável.
Sprint Backlog: A demanda escolhida pelo time a cada intervalo de tempo (sprint) é parte mínima do Product Backlog.
Increment: A entrega realizada somada a todas as entregas anteriores, ou seja, todo o conjunto pronto do Product Backlog.

Por fim, vamos trazer as 8 principais mudanças do Scrum, pois de forma analítica e interpretativa, alguns analistas vão dizer que mudou e outros não. Portanto é somente um apanhado optativo que estamos apresentando. Vamos lá:

A versão de 2017 tinha 19 páginas e a de 2020 tem 13. Com isso nota-se que ele foi literalmente reescrito, com o intuito de ficar mais leve e possibilitar que outras áreas, que não sejam Engenharia de software, possam utilizá-lo.
Adição do Objetivo do Produto. O time agora terá um objetivo claro e de grande valor, para orientar. Mesmo que cada sprint tenha uma meta, uma missão maior deverá ser perseguida.
Três compromissos em vez de um. Objetivo do Produto para o Product Backlog. Objetivo da sprint para o sprint backlog. Definição de Feito para o incremento.
Development Team agora é somente Developers, o termo team foi removido com a ideia de dar foco a um objetivo. O que remete a ideia de não ter sub-times nem hierarquia entre os membros.
A estrutura da daily não tem mais as três perguntas. Não que não possa ter, porém o estilo do encontro é livre limitado a 15 minutos de uma reunião de status do andamento em direção aos objetivos.
O time scrum se auto organiza literalmente. Dispensando a necessidade de gerentes e o dono do produto é como um integrante do time.
A Sprint Planning terá que responder a três questões. Por que esse sprint é viável? O que pode ser feito na sprint? Como será feito o trabalho?
Os papéis estão distribuídos por responsabilidades em vez de uma descrição detalhada do trabalho.

De fato, o que sabemos é que o planeta está em constante mudança de revoluções industriais, um framework de processo e uma rotina de trabalho que de uma hora para outra exige uma necessidade do trabalho ser totalmente em casa. A projeção é que o engajamento,  a comunicação objetiva, o apoio e liderança servidora serão como um guia para as adaptações.

Artigo escrito Pedro Saint Clair Garcia. Revisado por Prof. Tiago Carneiro.

REFERÊNCIAS

[1] Guia do Scrum, disponível em https://www.scrum.org/resources/scrum-guide SCRUM Guide. Acessado em 16/08/2020.

[2] Princípios e Práticas para Desenvolvimento de Software com Produtividade, disponível em https://engsoftmoderna.info/, acessado 16/08/2020, Versão atual: 2020.1.4 – ISBN: 978-65-00-01950-6 (impresso) e 978-65-00-00077-1 (e-book)."
Workshop TerraLAB 2021/1,http://www2.decom.ufop.br/terralab/workshop-terralab-2021-1/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/03/workshop-terralab-2021_1-17-730x350.png,"No dia 6 de março de 2021, sábado, a partir das 8 horas da manhã,  o TerraLAB irá realizar um Workshop com o tema: “Carreiras e práticas atuais na indústria de software”. Serão 12 palestras divididas em 30 minutos de apresentação e 10 minutos de perguntas e respostas. 

É necessário fazer a inscrição neste formulário para participar. O evento será realizado na plataforma Google Meet. O link da sala será enviado por e-mail aos inscritos. Para acessá-la, deve-se utilizar o e-mail institucional da UFOP. 

Os nossos palestrantes são profissionais de vasta experiência e reconhecidos pelo mercado de Tecnologia da Informação e Desenvolvimento de Software.

Tiago Carneiro que atua como  Professor Associado e é coordenador do laboratório TerraLAB do Departamento de Computação (DECOM) da UFOP
Vitor Paiva que atua como Engenheiro de software Sênior na empresa Gerencianet
Amanda Pinto Founder da empresa DevOps BootCamp
Conrado Carneiro que atua como CEO e é fundador da empresa Usemobile
Filipe Santos que atua como Engenheiro de Software Pleno na empresa Gerencianet
Lucas Uchôa que atua como Team Lead e Engenheiro Android na empresa iFood
Glícia Braga que atua como Supervisora de recursos na empresa Gerencianet
Pedro Garcia que atua como Agilista na empresa Take.Blip
Francisco Costa que atua como Developer na empresa Tembici
Gabriel Machado que atua como Feature Lead e Engenheiro de Software Sênior na empresa Nubank
Paulo Henrique Mendonça que atua como CIO na empresa Stilingue
João Vitor Mattos que atua como Diretor de Tecnologia na empresa Cachaça Gestor

A empresas que estarão nos apoiando e oferecerão palestras no evento são, portanto:

Gerencianet:  A Gerencianet é uma uma conta digital focada em negócios, para que empreendedores possam emitir e gerenciar recebimentos. Atualmente, é responsável pela emissão de mais de 60 milhões de cobranças anuais e possui 210 mil clientes cadastrados em todo o Brasil. 
Origine:  A Origine é uma empresa com o foco de desenvolver uma cultura de inovação e melhoria da gestão e de processos, por meio da mentalidade ágil e de métodos mais eficientes e eficazes para entrega de produto ou serviço e mais geração de valor para o cliente.
Usemobile: A Usemobile nasceu em 2015, na cidade de Ouro Preto.  É uma empresa especialista no desenvolvimento de aplicativos mobile, para Android e iOS, aplicações web e desktop com o foco de oferecer soluções inteligentes e adequadas para pessoas e empresas que precisam de tecnologia.
iFood: O iFood é a maior foodtech da América Latina. Com 8 anos de atuação, tem o propósito de revolucionar o universo da alimentação por uma vida mais prática e prazerosa. Atualmente, o iFood atende milhões de usuários no Brasil e possui uma ampla rede de restaurantes e mercados parceiros. O iFood conta com a participação da Movile – líder global em marketplaces móveis – e da Just Eat.
Take.Blip: Há 21 anos, a Take.Blip é destaque no mercado latino-americano de tecnologia mobile e mensageria. Além disso,  a empresa é uma das candidatas a atingir o status de unicórnio segundo o estudo Corrida dos Unicórnios 2021. Em 2016, lançou uma plataforma de desenvolvimento de chatbots. Tal plataforma ajudou grandes empresas a inserirem suas marcas nos aplicativos de mensagens. Recentemente a Take.Blip foi considerada a 2ª melhor empresa para trabalhar em Minas Gerais pela Great Place to Work (2020), a 11ª melhor empresa média para trabalhar no Brasil (2020) e a 15º melhor empresa para trabalhar no segmento de Tecnologia (2020).
Tembici: A Tembici é líder na América Latina em tecnologias para micromobilidade que criam soluções para inspirar uma revolução do espaço urbano. A empresa utiliza tecnologia e engaja parceiros públicos, privados e a sociedade civil para que o convívio das pessoas com as cidades seja mais eficiente, inteligente e agradável.  Em 2021, a Tembici foi apontada como uma das startups brasileiras candidatas a atingir o status de unicórnio pelo estudo Corrida dos Unicórnios 2021.
Nubank: O Nubank é uma fintech que desenvolve soluções simples, seguras e 100% digitais para a vida financeira de seus clientes. Hoje, a empresa é o maior banco digital independente do mundo e conta com mais de 20 milhões de clientes em todo o Brasil. O Nubank busca ter processos justos e transparentes na conduta, diretos e objetivos na comunicação, e tratamos cada cliente como uma pessoa. Contra burocracia, papelada, agências e centrais de atendimento caras e ineficientes. 
Stilingue: A Stilingue é uma plataforma com Inteligência Artificial brasileira, que escuta, resume e prioriza a voz do cliente em um só lugar, fazendo com que marcas fiquem um passo à frente, enxerguem oportunidades e melhorem as experiências de seus consumidores. A tecnologia é admirada pelos mais modernos Departamentos de Marketing e Customer Service da América Latina, mais precisa que grandes nomes estrangeiros de Inteligência Artificial e a mais usada no Brasil, segundo pesquisa dos profissionais de redes sociais. Recentemente a Stilingue anunciou a captação de recursos no valor de R$18 milhões, liderada pela DGF Investimentos. O valor será destinado a inovação em produtos, com o desenvolvimento de novos módulos e o aprimoramento da plataforma.
Cachaça Gestor: A Cachaça Gestor fornece um sistema computacional que visa tornar a vida do produtor de cachaça mais eficiente. O objetivo da empresa é aumentar a produtividade do empreendimento de seus clientes e garantir um controle melhor e mais dinâmico dos processos de produção e gestão. O sistema também fornece informações importantes para o negócio, como dicas para boas práticas na produção de uma cachaça de qualidade e orientações sobre a produção da bebida e certificação.
DevOps Bootcamp: O DevOps Bootcamp é um projeto de capacitação, mentoria em educação corporativa e de profissionais de tecnologia, focada em inovação, automação, segurança, gestão e aplicação das melhores ferramentas de DevOps e Cloud Native do mercado. A empresa reconhece que a Tecnologia da Informação é o caminho alcançar a excelência e satisfazer as necessidades mais desejadas das organizações em tecnologia e seus profissionais, levando a automação dos processos.

O Workshop é também a primeira atividade obrigatória para os inscritos no Processo Seletivo Trainee 2021/1, que permanece aberto até dia 05 de março. 

Há oportunidades para estudantes de todos os cursos da UFOP. Em 2020, capacitamos 22 estudantes e 95% deles já foram absorvidos por empresas.

Venha prestigiar o evento! Venha fazer parte do nosso time!!!!.

Para se inscrever no Workshop TerraLAB 2021 clique aqui.

Para se inscrever no Processo Seletivo Trainee 2021 clique aqui."
Aberto o processo Trainee TerraLAB 2021/1,http://www2.decom.ufop.br/terralab/aberto-o-processo-trainee-terralab-2021-1/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/02/Programa-trainee-terralab-2021_1-10-730x350.png,"Inscrições de 22 de fevereiro a 5 de março de 2021

Aprenda as tecnologias mais pedidas no mercado e desenvolva suas habilidades trabalhando com profissionais experientes e qualificados. Você desenvolverá aplicativos mobiles e web, ambos integrados a serviços na nuvem! Dentre outras tecnologias, você vai ter contato com React, React Native, Node JS, Jest, Cucumber e Selenium, Apium, AWS e mais.

Graduandos e pós-graduandos da UFOP: Aumente a sua chance de conseguir uma vaga de emprego e domine as tecnologias utilizadas na indústria de software.

Nosso objetivo é aumentar sua empregabilidade e, ao mesmo tempo, reduzir o custo e tempo da seleção e treinamento de recursos humanos por parte da indústria.

Os estudantes serão capacitados em um ambiente profissionalizante, similar ao de uma fábrica de software, onde o estudante pode vivenciar os papéis existentes no ecossistema dessa indústria.

Nominalmente, os estudantes podem vivenciar os seguintes papéis: Analista de Negócio, Gerente de Projeto, Analista de Sistema, Engenheiro de Teste, Engenheiro de Software, Engenheiro DevOps, Designer de UX, Cientista de dados, Analista de Geoprocessamento, Analista de TI, Análise de marketing, Analista contábil e Analista jurídico.

Coordenação: O laboratório TerraLAB é coordenado pelo professor Dr. Tiago Garcia de Senna Carneiro, do Departamento de Computação (DECOM) da Universidade Federal de Ouro Preto (UFOP).

Mentoria: Os estudantes selecionados receberão mentoria de profissionais experientes que atuam em empresas parceiras. Estas empresas, de antemão, expressaram interesse em contratar, em médio prazo, os estudantes que apresentarem bom desempenho na realização das atividades propostas nesta iniciativa.

Processo seletivo: Após o período de inscrição, no dia 6 de março, sábado, faremos um workshop onde as empresas parceiras apresentarão seus cases e ambientes de desenvolvimento aos estudantes inscritos. Depois, os estudantes terão aproximadamente 5 semanas para realizarem auto-treinamento por meio dos cursos online oferecidos pelas empresas parceira sobre as atuais tecnologias por elas utilizadas para o desenvolvimento de frontends (web e mobile), backends (em nuvem) e testes (web, mobile e backend). Durante esse tempo, serão propostos desafios para os estudantes, que devem utilizar os conhecimentos adquiridos para resolvê-los. Um deles será o desenvolvimento de um aplicativo, utilizando-se das tecnologias descritas acima.

Pré-requisitos: É essencial que os candidatos sejam pessoas automotivadas e comprometidas com o sucesso dos projetos com os quais se envolvem. Apesar dos projetos pedirem demandas técnicas específicas, os candidatos receberão treinamento e mentoria para desenvolvimento de suas tarefas. No ambiente de fábrica de software, é clara a necessidade de profissionais que atuam para apoiar o processo de produção de software e que não são programadores, este é o caso do Analista de TI que mantém a rede de comunicação e servidores linux em funcionamento, além do Designer Gráfico que cuida das peças (imagens, ícones, telas e textos) de comunicação associadas aos produtos e o Gerente de Projetos que tem como incumbência administrar os resultados, prazos, custos e recursos dos projetos. Então, se você ainda não é um programador no paradigma orientado por objetos, pense em nos ajudar desempenhando um desses papéis.

Inscrição: De 22 de fevereiro a 5 de março de 2021. Os interessados devem enviar currículo LATTES e fornecer todas as informações solicitadas no seguinte formulário .

Bolsa: Neste semestre, os estudantes deverão se voluntariar e receberão como contrapartida a mentoria e o treinamento nas tecnologias supracitadas. Após esse período de tempo, os estudantes com bom desempenho poderão receber bolsas para a continuidade dos projetos que iniciaram neste semestre.

Carga Horária: 20h/semanais

Atuação remota: Todas as atividades que serão realizadas durante o processo seletivo ocorrerão de forma remota. Os encontros serão síncronos, via Google Meet, após o término do período de inscrição.

Número de vagas : Não há um número máximo de vagas em aberto, todos os estudantes que tiverem bom desempenho no processo seletivo serão convidados a tomar parte nesta iniciativa. No passado, o laboratório TerraLAB já teve 36 colaboradores atuando simultaneamente em diversos projetos.

As inscrições para o processo seletivo podem ser feitas do dia 22 de fevereiro ao dia 5 de março de 2021.

A proposta está aberta aos estudantes de todos os cursos de graduação e pós-graduação da UFOP.

A primeira atividade obrigatória do processo seletivo é a presença durante todo o Workshop TerraLAB que irá ocorrer no dia 6 de março de 2021, sábado. 

Ajude-nos a divulgar essa iniciativa!"
5 extensões para melhorar a produtividade no Visual Studio Code,http://www2.decom.ufop.br/terralab/5-extensoes-para-melhorar-a-produtividade-no-visual-studio-code/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/02/Group-10-730x350.jpg,"Na rotina de um desenvolvedor, o retrabalho pode tornar as suas atividades um pouco cansativas e improdutivas ao ter que identificar erros simples quando precisa-se realizar entregas dentro do prazo. Isso pode causar grandes problemas, por isso é necessário buscar ferramentas que possam auxiliar no desenvolvimento como um todo, buscando agilidade e produtividade nos processos.

Pensando nesse assunto, este artigo tem como objetivo apresentar 5 extensões que facilitam a vida do desenvolvedor, além de tornar o trabalho ainda mais prazeroso. Falaremos sobre a função de cada extensão, e logo após um breve tutorial de como instalar no VS Code, de forma rápida e fácil.

Primeiro você precisa ter o visual studio code instalado na máquina, se ainda não utiliza, você pode fazer o download no site oficial, clicando aqui.

Obs.: Baixe sempre a versão estável do software.
Após realizar a instalação do VS code ou caso já utilize, você precisa abrir o programa e selecionar o botão extensões, no painel esquerdo do ambiente, no qual irá surgir outro painel contendo todas as extensões disponíveis, conforme a imagem abaixo:

Será por aqui que iremos pesquisar as nossas extensões e instalar cada uma no editor de textos.

1ª Javascript (ES6) Code Snippets

Essa extensão se torna essencial para todo desenvolvedor front-end, pois fornece trechos não só do código JavaScript como também do Vue, React e Type Script e HTML com muita praticidade. 
Pesquise pelo nome javascript code snippets e selecione a primeira opção. Ao lado vai aparecer toda a documentação da extensão, que pode ser consultada sempre que tiver dúvidas. Para instalar no editor de texto, basta clicar em install, e pronto, já pode utilizar os recursos em seu código.

2ª – Color HighLight

Essa extensão também serve para quem atua como front-end, pois a mesma pode ajudar na verificação das cores, que vai ser criado no código do CSS e Sass. Para isso, basta que digite os caracteres alfanuméricos referentes a cor desejada, para que o sistema sinalize qual a cor o código digitado se refere.

Confira mais sobre a extensão aqui.

3ª – Dracula Official

Para poder personalizar o ambiente de desenvolvimento, muitos optam por utilizar temas que alteram todo o visual do editor, sendo o Drácula um dos mais utilizados pela comunidade. Para instalar essa extensão, basta pesquisar por Drácula Official e selecionar a primeira opção:

Após a instalação, o seu código ficará conforme a figura abaixo, trazendo uma melhor visibilidade e facilidade para encontrar qualquer variável ou comando através da cor indicada.

É possível escolher cores variadas de acordo com o gosto de cada Desenvolvedor.   Confira mais sobre a extensão aqui. 

4º – Tabnine

O Tabnine foi criado com base em um modelo de inteligência artificial, que usa Machine Learning para sugerir o que provavelmente será digitado, em tempo real. Essa extensão fornece uma pequena parte do código-fonte reutilizável (snippets), além de reconhecer o código de qualquer linguagem. A mesma mostra também a probabilidade do uso do snippet.

Após instalado, você deve tentar codificar algo, para que o sistema retorne as possíveis sugestões de continuidade do código.

Confira mais sobre a extensão aqui.

5º – Debugger for Chrome

Essa extensão é ideal para verificar linha por linha até encontrar qualquer bug, que esteja difícil de resolver. A mesma está voltada para desenvolvedores front-end, e permite usar o Debugger do Chrome integrado ao VS code. Pode ser executada ao mesmo tempo e na mesma tela do editor, sem que haja a necessidade de abrir o navegador para isso.

Essa extensão é restrita à linguagem Javascript e depois de instalada, basta definir os pontos de interrupção dentro do seu código e executá-lá pressionando o botão debug ou F5 para poder depurar o seu código, linha por linha.

Você pode encontrar a documentação completa clicando neste link.
Essas e outras extensões podem ser encontradas no próprio site do Visual Studio. Certamente com o uso dessas extensões, aliadas a outros tipos de métodos e técnicas, você será muito mais produtivo e terá um visual agradável pra poder trabalhar no dia a dia.

E você tem alguma dica de extensão para usar no VS Code? Deixe seu comentário, compartilhe e contribua com o nosso blog!

Referências

Marketplace de extensões do Visual Studio Code

Debugger for Chrome

10 extensões para aumentar a sua produtividade no Visual Studio Code

Top Extensões Úteis para VSCode – 2020

Artigo escrito por Walisson Farias França e revisado por Carlos Magalhães Silva e Prof. Tiago Carneiro."
Como o TerraLAB e suas equipes são estruturados?,http://www2.decom.ufop.br/terralab/como-o-terralab-e-suas-equipes-sao-estruturados/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/02/Como-o-TerraLAB-e-suas-equipes-sao-estruturados_-1-730x350.png,"Existem diversas maneiras de se organizar uma equipe de desenvolvimento de software. Não existe “a melhor maneira” de estruturar equipes de software. Muitas vezes a natureza do projeto tem impacto direto sobre esta estrutura, como é fácil imaginar que equipes de desenvolvimento de jogos possuem papéis/funções muito diferentes de equipes de desenvolvimentos de sistema de informação geográfica ou de desenvolvimento de simuladores. 

Por outro lado, a estrutura das equipes têm impacto direto no fluxo de trabalho e informações. Ao estruturar uma equipe é importante ter em mente a escalabilidade do processo de desenvolvimento de software, isto é, é sempre bom planejar uma estrutura que permita à equipe aumentar sua produtividade pela simples adição de novos membros. Contudo, quanto maior a equipe, maiores são as chances de falhas na comunicação acontecerem, papéis centralizadores podem se tornar gargalos desse processo e impedir a almejada escalabilidade. 

É neste contexto que este artigo tem como objetivo apresentar como a equipe do TerraLAB está estruturada, atualmente. É provável que o entendimento  desta estrutura interesse aqueles estudantes e profissionais que estejam planejando construir a sua própria startup. A equipe do laboratório se estrutura conforme mostra a figura abaixo.

Estrutura Geral da Equipe

Figura 1. Estrutura da Equipe do laboratório TerraLAB.

A equipe se divide em duas dimensões transversais em esquadrões (SQUADs)  ou em capítulos (CHAPTERS). Nos capítulos, as pessoas são agrupadas por suas habilidades, cada capítulo é um grupo de especialistas em uma área específica do conhecimento. Na indústria de desenvolvimento de  software as equipes são comumente agrupadas nos capítulos de experiência do usuário (UX – User eXperience), de desenvolvimento (DEVS – Development), de garantia de qualidade (QA – Quality Assurance), de operações (DEVOPS – Development and Operations) e de agilidade (SM – Scrum Master). Alguns nichos de atuação poderão demandar outros capítulos como os capítulos de geoprocessamento, de modelagem e de jogos.  Tudo realmente depende do ramo de atuação desta equipe. Cada capítulo possui um líder  (CHAPTER LEAD) que é responsável por auxiliar tecnicamente os demais colaboradores de seu capítulo e lhes oferecer feedbacks sobre sua atuação. Estes líderes devem manter alinhamento e atender às demandas gerência técnica (HEAD ENGENHARIA).  

Na rotina de trabalho, quando algum desafio surge e a equipe precisa ser mobilizada para produzir uma solução em resposta a esse desafio, primeiro, identifica-se as habilidades necessárias para o desenvolvimento desta solução, depois, monta-se um esquadrão selecionando pessoas dos diferentes capítulos e entrega-lhes a missão de vencer o desafio. Assim, que desafio é vencido o esquadrão se desfaz. É comum que os colaboradores participem de mais de um esquadrão, afinal muitos desafios podem demandar mais esforço de algumas especialidades que de outras. 

Por outro lado, nos esquadrões, é comum existir integrantes de diferentes habilidades. Na imagem, os esquadrões se organizam verticalmente. Os esquadrões devem se auto-organizar para desenvolver uma solução que atenda às necessidades do cliente que a encomendou. Somente deixando esse cliente satisfeito ele terá cumprido sua missão.  Muitas vezes, o cliente é externo à empresa na qual a equipe trabalha e, outras vezes, o cliente é interno.  Independente da origem do cliente, uma coisa é certa, muitas vezes o cliente não sabe exatamente o que ele quer ou o que ele  precisa, são coisas muito diferentes. É comum que o cliente vá respondendo esta pergunta a si mesmo à medida que experimenta versões evolutivas das soluções desenvolvidas pelos esquadrões. Isto pode ser um problema, visto que para cumprir sua missão é crucial que qualquer esquadrão receba especificações exatas sobre o desafio que precisa ser vencido, sobre quais requisitos a solução a ser construída precisa satisfazer e sobre as que restrições tecnológicas, legais ou negociais são impostas a esta solução. É no meio deste impasse que, tanto para guiar os clientes quanto para informar esquadrões que surgem os donos de produtos – os Product Owners. 

Cada desafio a ser vencido pertence a um Product Owner (PO) que dele deve conhecer todos os detalhes. Para isso, os POs devem se alinhar com seus clientes, entender profundamente suas necessidades, gerir suas expectativas e intermediar todas as comunicações entre os clientes e os esquadrões que os atenderão, fazendo que a solução produzida convirja rapidamente para algo que satisfaça as necessidades reais do cliente. Para os esquadrões, os POs são os clientes, pois os POs precisam aprovar todas as entregas feitas pelos esquadrões antes que elas cheguem aos verdadeiros clientes. Para um cliente, o PO é a equipe! Isto significa, se a solução que esse cliente recebeu não o satisfez, então, toda a equipe falhou! Portanto, para desempenhar bem o seu papel, um PO precisa se comunicar bem no jargão do cliente para evitar ao máximo ruídos de comunicação e, então, traduzir as demandas do cliente para o esquadrão. No sentido complementar, o PO também precisa ser capaz de conversar tecnicamente com o esquadrão de forma a especificar com muita clareza aquilo que precisa ser desenvolvido e, também, para ser capaz de entender os impedimentos e restrições técnicas ou tecnológicas encontradas pela equipe e, então, explicá-las para o cliente. Desta maneira, os POs são peças cruciais na engrenagem das equipes de desenvolvimento de software, traduzindo todas as comunicações entre clientes e esquadrões.

Há ainda uma outra peça neste quebra-cabeça. Uma vez que qualquer solução consome recursos e tem prazos estimados para seu desenvolvimento, os POs também precisam se manter alinhados com os gerentes de projetos (PM – Project Manangers). Estes últimos possuem como principais responsabilidades controlar prazos e custos, alocando recursos com eficácia.  Os gerentes de projeto são catalisadores, devem fazer com que o fluxo de trabalho flua da maneira mais eficiente e eficaz possível, sem desperdícios e gerando resultados. Eles precisam resolver todas as questões não técnicas que estejam impedindo ou atrasando o desenvolvimento de uma solução. Eles determinam o ritmo dos trabalhos, engajando, motivando, tranquilizando e harmonizando as equipes. Gerentes de projetos e donos de produto estabelecem uma relação cliente-fornecedor.  À medida que o gerente de projetos, no papel de cliente, cobra dos donos de produto entregas de qualidade dentro dos prazos  e custos estabelecidos, os donos de produto solicitam e justificam recursos que os gerentes de projeto devem adquirir e aplicar com sabedoria. Assim, donos de produtos e gerentes de projetos precisam se manter alinhados com o cliente sobre questões de naturezas diferentes, uma técnica e outra negocial.

Uma coisa a mais é importante ressaltar sobre o funcionamento de um esquadrão. Apesar dos integrantes terem atribuições únicas, realizar as tarefas para o desenvolvimento de uma solução, isto é, realizar a missão, é dever compartilhado por todos. Ninguém deve ficar à toa se há algo que precisa ser feito para atingir a conquista! Ninguém fica ocioso enquanto houver tarefa pendente! Por isso, todos os integrantes de um esquadrão devem sempre buscar o autodesenvolvimento, de maneira que em algum tempo, eles se habilitem a realizar qualquer tarefa de desenvolvimento e operação e, então, se tornem o tipo de profissional que o mercado busca com avidez: fullstack e praticante da cultura Devops!  

Finalmente, na dimensão do capítulo que se estrutura horizontalmente na figura acima,  os colaboradores são agrupados com seres pares em habilidades. Nesta dimensão, todos os desenvolvedores podem se ajudar tirando dúvidas com colegas e fortalecendo o conhecimento instalado no capítulo, muitas vezes um colega pode cobrir a ausência do outro ou reforçar sua presença. É um ambiente onde todos colaboram e co-evoluem.

 No TerraLAB, além dos papéis já citados (UX, DEV, QA, PO, DEVOPS e SM) e que comumente aparecem nas equipes de desenvolvimento de software, há o capítulo formado por especialistas em Geoinformática. Isto é um reflexo do nicho de atuação do laboratório junto a seus parceiros. Porém, há ainda outras duas gerências, Jurídica e de Marketing, que oferecem suporte a todos os esquadrões e os esforços de seus membros são compartilhados por todas as demandas do laboratório.  Abaixo, falaremos um pouco mais sobre estas funções. Clique nos links a seguir caso você queira saber um pouco mais sobre as funções de:

UX(O que é User Experience e quais etapas compõe o seu processo)
PO(A vida de Product Owner não é fazer piada)
PM(O papel e os desafios do gerente de projetos)
O que faz a equipe de Geoprocessamento?

A equipe de geoprocessamento é responsável de dotar os produtos e serviços do TerraLAB de Inteligência Espacial, isto é, eles devem ser capazes de desenvolver funcionalidades que permitam que nossos software transformem dados espaciais (mapas, imagens de satélite, dados GPS) em informações úteis a seus usuário. Para isso, a equipe deve conhecer bem os conceitos da GeoInformática e dominar os métodos e ferramentas computacionais disponíveis nos Sistemas de Informação Geográfica (SIG), como os aplicativos QGIS, TerraView, ArcGIS e Spring.

Mas além disto, é de especial interesse do TerraLAB que sua equipe de Geoprocessamento seja capaz de programar para que possam desenvolver inovações e estender as funcionalidades presentes nos atuais SIGs pagos ou gratuitos. Por meio de pequenos programas chamados scripts, essa equipe deveria ser capaz de reutilizar as funcionalidades disponíveis em bibliotecas de código encontradas na Internet para criar novos métodos e ferramentas computacionais de interesse de um projeto ou cliente/parceiro.

O que faz a equipe de Marketing?

A equipe de marketing é responsável por desenvolver a imagem/sentimento que os colaboradores, parceiros e clientes terão do TerraLAB e de seus produtos/serviços. Para isso, ela deve planejar, promover e realizar campanhas de publicidade, ou eventos, sobre importantes acontecimentos ligados ao TerraLAB. Assim, o Marketing deve gerir todas as mídias e redes sociais do TerraLAB, incluindo: Blog, Wiki, Instagram, Linkedin, Facebook, Twitter, etc. Tudo deve ser realizado pensando sempre nas perspectivas de três atores essenciais à existência do laboratório, seus colaboradores, seus clientes e seus parceiros.

O que faz a equipe Jurídica?

“O jurídico é um dos departamentos com maior permeabilidade dentro de uma empresa, já que faz interface com diversas áreas – desde compras e vendas até marketing e finanças. É responsável por orientar a empresa sobre as melhores práticas a seguir, o departamento jurídico é um dos setores mais importantes quando falamos do contexto estratégico do negócio.” 

(https://www.aurum.com.br/blog/departamento-juridico-em-uma-empresa/, 2020).

Recentemente, o setor jurídico das melhores empresas deixou de ser passivo, deixou de ser acionado apenas quando ações judiciais ou outros processos jurídicos aparecem, e passou a ser um setor ativo, tornando a rotina jurídica organizada e mantendo a empresa em conformidade com a lei. O jurídico precisa ter uma mentalidade empreendedora e ter uma visão global do negócio. No desenvolvimento ou no lançamento de um produto ou serviço, o jurídico deve realizar a análise do contexto legal e também fornecer um posicionamento ou previsão sobre os impactos futuros de tais medidas. É preciso conhecimento aprofundado em legislação, é preciso aperfeiçoar a construção de um raciocínio crítico e analítico para embasar e mapear possíveis riscos à imagem da empresa, à saúde e ao bem-estar dos colaboradores. É preciso cuidar dos impactos das decisões da empresa em contratos, registro de patente, etc.

O TerraLAB é um laboratório de uma universidade, portanto, não é uma organização (não tem CNPJ), apenas a UFOP o é. Portanto, é missão desse setor buscar caminhos viáveis para captação de projetos, recursos humanos e recursos financeiros. Para facilitar todas essas captações, busca-se responder às seguintes questões: Como o próprio TerraLAB deveria ser caracterizado dentro da estrutura da UFOP: Como um projeto de extensão? Como um núcleo de pesquisa? Ambos? Quais normas da UFOP são relevantes neste contexto? Quais leis regem estas questões? Como receber doações de empresas parceiras? É possível a dedução de impostos das empresas doadoras? Como fixar e pagar colaboradores? São muitos os desafios à frente. Esperamos que setor jurídico nos ajude a construir um modelo viável para a inovação.

Considerações Finais

Neste artigo apresentamos como o TerraLAB está estruturado para conferir escalabilidade a sua equipe de desenvolvimento de software e produtividade a todos os seus colaboradores. Neste artigo, também descrevemos quais são as funções atribuídas a alguns especialistas que formam a equipe. Esperamos ter contribuído para todos aqueles estudantes e profissionais que pensam em algum momento construir sua própria startup. Diga-nos se esse texto lhe foi útil e compartilhe conosco idéias para melhorar a produtividade e bem estar nossa equipe.

Artigo escrito por Prof. Tiago Carneiro."
Resultados do TerraLAB no mês de Novembro,http://www2.decom.ufop.br/terralab/resultados-do-terralab-no-mes-de-novembro/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/12/Banner-artigo-de-resultados-1.png,"Esta é uma sequência de artigos que tem como objetivo mostrar a evolução dos integrantes do TerraLAB ao longo de cada mês. No artigo anterior mostramos parte do desenvolvimento dos três projetos que estão em andamento no TerraLAB envolvendo as nossas equipes de UX, Engenharia, Análise de Dados, Infraestrutura, Gestão de Projetos e Marketing, são eles: o Serviço de Geocodificação em Massa (SGM) e o BatCaverna. 

SGM:  Na área de logística e de pesquisa de mercado, a geocodificação criou certas facilidades, que até então não existiam, e proporcionou uma transformação nos métodos de planejamento estratégico, organizacional e distributivo de mercadorias. Um serviço de geocodificação em massa na nuvem, deve ser desenvolvido pela grande aplicabilidade em diversas empresas de diferentes ramos, pois garante perspectiva de crescimento logístico, financeiro e tecnológico. Além disso, na grande maioria das vezes, dados sobre os sistemas sociais são vinculados a um endereço de logradouro ao invés de a uma coordenada geográfica. Este é comumente os casos dos eventos de saúde ou criminais. Consequentemente, o serviço de geocodificação é útil para a espacialização destes eventos e para o entendimento das maioria dos sistemas sociais. Com essas informações o projeto AWS SGM: Crawler de Geocodificação, busca trabalhar no problema: Como conhecer a localização de um logradouro, sem ir a campo?
BatCaverna: O aplicativo tem como objetivo fazer a coleta detalhada do patrimônio espeleológico da área delimitada no estado de Minas Gerais e busca contribuir para o incremento do banco de dados das cavernas presentes. A principal ferramenta será a de cadastro de pontos de controle ou de feições com as suas respectivas descrições e imagens associadas. Aliado a esse cadastro de pontos, o aplicativo irá coletar os caminhos feitos pelo o usuário e, durante esse caminho, ele terá a liberdade de criar quantos pontos forem necessários. Outra funcionalidade importante que está atribuída a esse aplicativo é a de visualizar os caminhamentos, tanto de uma forma individual, clicando sobre o caminho realizado, quanto de forma geral que será exibida em seu menu principal. O sistema também contará com um gerador de relatórios baseado nos pontos e fotografias tiradas em campo.

Na última semana do mês de novembro ocorreu a validação mensal do trabalho realizado pelas equipes do TerraLAB, acompanhe nos vídeos abaixo o que foi produzido e quais são as principais ferramentas e tecnologias que estão sendo utilizadas e estudadas nesses projetos.

GESTÃO DE PROJETOS

O gerente de projetos, Emanuel Xavier, nos conta que foi iniciado o processo de documentação, de forma atualizada em relação aos processos atuais do Lab, as informações sobre a gerência de projetos, o que ajudará a orientar as próximas pessoas que vierem a ocupar este cargo. Além disso, a estrutura das reuniões foi alterada, com ajuda de nosso Consultor Agile Pedro Garcia, para aplicar melhor os métodos ágeis em nossos processos, facilitando a validação das atividades planejadas ou realizadas pela equipe.

ENGENHARIA 

O nosso gerente de Engenharia de Software, Higor Duarte de Oliveira, apresenta no vídeo a seguir a implementação do módulo do npm, commitlint, nos projetos do Lab programados em Node.Js, para garantir a padronização de commits no formato convencional. Também são apresentados mais alguns experimentos realizados no Geocodificador.

DATA ANALYTICS

No vídeo deste mês, o gerente de Data Analytics, Alan Santandrea, nos conta que o chapter de Data Analytics apresentou 3 entregas. A primeira e provavelmente mais importante foi a criação da versão 0.0.1 da base de dados que alimenta o Serviço de

Geocodificação em Massa. Em segundo o fluxo de ETL que retira os dados do banco de dados da infra para análises. E terceiro, porém não menos importante, foram feitas análises de métricas para validar os dados recebidos. O time segue engajado e planejamos para o próximo mês fazer análises nos dados novos com o intuito de melhorar a base de dados do serviço. Este aprimoramento se promete constante gerando novas versões da base cada vez mais consistentes.

INFRAESTRUTURA

Agora, a equipe de Infraestrutura do TerraLAB, liderada pela gerente Ana Luiza Soares, apresenta os três serviços ofertados durante o mês de Novembro pelo time: Operacional, Serviço Transferência de Conhecimento  e Serviço de Geocodificação Serverless. 

Para o Serviço Operacional, aquele onde executamos as necessidades de infraestrutura de outras equipes do laboratório, não houve demandas. 

No Serviço de Transferência de Conhecimento foram feitos 1 artigo e 2 vídeos para publicação no blog do TerraLAB e capacitação da equipe.

No Serviço de Geocodificação Serverless foram desenvolvidos o experimento de análise de tempo e custo do projeto MVP, o banco de dados para armazenar os endereços geocodificados e o script para limitação das requisições por segundo, dia e mês, respeitando as limitações da cada API utilizada.

USER EXPERIENCE

A seguir, o gerente de UX, João Pedro Siqueira nos mostra o que o seu time realizou, como um questionário que foi preenchido através de uma reunião com cliente, juntamente com a criação de duas Personas, além de um template de jornada de usuário. Para saber mais confira o vídeo:

MARKETING

Enfim, o gerente de marketing, Luka Menin, nos mostra os serviços prestados por sua equipe neste mês, em âmbitos Operacional – como as postagens semanais nas redes sociais do Lab, artigos escritos por membros do Lab que foram publicados, documentação de processos e pesquisas criadas e aplicadas – e Negocial – como as alterações do nosso Blog WordPress utilizando um container Docker implementado pela equipe de Infraestrutura. Para conhecer os outros serviços e entender melhor o que foi feito, veja o vídeo:

Nós estamos há sete meses trabalhando nos projetos aqui citados. Utilizamos o primeiro mês para planejamento e esta é, portanto, nossa sexta entrega. Caso você represente uma empresa ou seja um/uma profissional experiente, gostaríamos do seu feedback para que possamos melhorar continuamente o nosso trabalho. 

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. 

O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab! 

Artigo escrito por Luka Menin. Revisado por Prof. Rodrigo Cesar."
Como as Máquinas Aprendem?,http://www2.decom.ufop.br/terralab/como-as-maquinas-aprendem/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/11/Como-as-maquinas-aprendem-6-730x350.png,"Édifícil encontrar, hoje em dia, qualquer pessoa que não tenha pelo menos escutado os termos inteligência artificial ou aprendizado de máquina. O número de aplicações destas técnicas é enorme e a tendência é que continue crescendo. Mas afinal, como as máquinas aprendem?

Vamos começar com um pouco de história. 

Em 1936, no artigo, On Computable Numbers [1], Alan Turing, introduzia os princípios que definem o que conhecemos hoje como computador. Possivelmente, fascinado com o número de possibilidades e o poder de resolver problemas daquele construto que acabara de criar, em 1950, Turing já discutia no artigo, Computing Machinery and Intelligence [2], se computadores seriam capazes de pensar. Esta discussão está fora do escopo deste post mas já neste artigo, Turing discute, de forma ainda um pouco superficial, procedimentos para “ensinar” computadores. 
O grande problema quando estamos tentando ensinar um computador é não entendermos em detalhe como nós mesmos aprendemos as coisas. Existe uma vasta literatura na pedagogia sobre como crianças e adultos aprendem, mas os mecanismos físicos, químicos e biológicos deste processo ainda não são conhecidos em detalhe. Ainda assim, algoritmos de aprendizado de máquina existem e funcionam razoavelmente bem. Então, como estes algoritmos funcionam?

Definindo aprendizado

Um algoritmo de aprendizado de máquina existe para cumprir um ou mais objetivos. Assim, o primeiro passo é definir uma função de utilidade, U, que envelopa todos este objetivos de forma que quanto maior U, mais próximo o algoritmo está de cumprir este objetivo. Uma vez que U é definido matematicamente, podemos definir “aprendizado” como um problema de otimização da seguinte forma:

Então temos que encontrar os parâmetros internos, p, do algoritmo de forma a maximizar a função de utilidade, U. Fazendo uma analogia, bastante imprecisa, com o nosso próprio corpo, é como se p definisse como os neurônios no nosso cérebro estão conectados e como eles se comportam dados os estímulos sensoriais, internos e externos, que o nosso corpo recebe. 

Com uma definição matemática do problema de aprendizagem, podemos utilizar técnicas de otimização, ou programação matemática, para resolver o problema. Provavelmente, a técnica mais utilizada neste tipo de problema é a técnica de subida do gradiente. 

Se você não está interessado na parte matemática, mas achou o assunto interessante, foi um prazer. Dê uma lida no artigo Computing Machinery and Intelligence [2] brilhantemente escrito por Turing e nos vemos no próximo post :D. Se está interessado, siga comigo, está quase acabando 

Aprendendo

O gradiente de uma função U, definido como ∇U,  é um vetor que aponta para direção de maior crescimento da função. Ele é composto pelas derivadas parciais da função em relação a cada uma das variáveis. Em outros termos, o gradiente define qual a taxa de variação instantânea da função em relação a cada um dos parâmetros.

Esta informação é extremamente útil, pois assim o algoritmo sabe quais parâmetros devem ser ajustados e como eles devem ser ajustados de forma a maximizar a função de utilidade ∇U.

Neste contexto, aprender significa então executar o seguinte algoritmo:

Dado um conjunto de estímulos, ξ, e um conjunto de parâmetros, p, no tempo t, pt,

Calcular o gradiente de U em pt
Atualizar os parâmetros

pt+1 = pt + α∇U(pt)

Ir  para o passo (1) até atingir algum critério de parada

Onde α é conhecido como taxa de aprendizado e define o tamanho do passo a ser dado na direção do gradiente da função de utilidade. 

Assim, a cada iteração os parâmetros são ajustados de forma que o valor da função de utilidade fique cada vez maior. Eventualmente, este algoritmo converge para um máximo de U.  Note que, em muitos casos, o objetivo deste algoritmo pode ser minimizar alguma medida de erro. Neste caso, basta seguir no sentido oposto do gradiente fazendo a atualização dos parâmetros com a equação abaixo. 

  pt+1 = pt – α∇U(pt)

Até aqui, tudo parece muito bom. Temos um modelo matemático do que se deseja “aprender” e um algoritmo que executa o “aprendizado”. No entanto, nem tudo são flores. Definir uma função de utilidade adequada pode ser complicado. Além disso, o algoritmo acima pode demorar demais para convergir e ainda ficar preso em máximos locais [3]. Outro problema comum é o chamado overfitting [4], que acontece quando o algoritmo atinge valores altos de utilidade durante o treinamento mas este comportamento não se repete durante o teste.  

Existem na literatura, várias maneiras de contornar estes problemas, mas estes são tópicos para próximos posts. 

Você sabia que este artigo foi escrito pelo time do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Siga-nos nas redes sociais para saber mais!

Este artigo foi escrito por Prof. Rodrigo Silva e Alan Santandrea, revisado por Luka Menin.

Referências: 

[1] Turing, A. M. (1936). On computable numbers, with an application to the Entscheidungsproblem. J. of Math, 58(345-363), 5.

[2] Turing, A. M. (1950). Computing Machinery and Intelligence. Mind, 59, 433–460.

[3] Local Optimization Versus Global Optimization (machinelearningmastery.com)

[4] Roelofs, R., Fridovich-Keil, S., Miller, J., Shankar, V., Hardt, M., Recht, B., & Schmidt, L. (2019, December). A meta-analysis of overfitting in machine learning. In Proceedings of the 33rd International Conference on Neural Information Processing Systems (pp. 9179-9189)."
Resultados do TerraLAB no mês de Outubro,http://www2.decom.ufop.br/terralab/resultados-do-terralab-no-mes-de-outubro/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/11/Outubro.png,"Esta é uma sequência de artigos que tem como objetivo mostrar a evolução dos integrantes do TerraLAB ao longo de cada mês. No artigo anterior mostramos parte do desenvolvimento dos três projetos que estão em andamento no TerraLAB envolvendo as nossas equipes de UX, Engenharia, Análise de Dados, Infraestrutura, Gestão de Projetos e Marketing, são eles: o Serviço de Geocodificação em Massa (SGM) e o BatCaverna. 

SGM:  Na área de logística e de pesquisa de mercado, a geocodificação criou certas facilidades, que até então não existiam, e proporcionou uma transformação nos métodos de planejamento estratégico, organizacional e distributivo de mercadorias. Um serviço de geocodificação em massa na nuvem, deve ser desenvolvido pela grande aplicabilidade em diversas empresas de diferentes ramos, pois garante perspectiva de crescimento logístico, financeiro e tecnológico. Além disso, na grande maioria das vezes, dados sobre os sistemas sociais são vinculados a um endereço de logradouro ao invés de a uma coordenada geográfica. Este é comumente os casos dos eventos de saúde ou criminais. Consequentemente, o serviço de geocodificação é útil para a espacialização destes eventos e para o entendimento das maioria dos sistemas sociais. Com essas informações o projeto AWS SGM: Crawler de Geocodificação, busca trabalhar no problema: Como conhecer a localização de um logradouro, sem ir a campo?
BatCaverna: O aplicativo tem como objetivo fazer a coleta detalhada do patrimônio espeleológico da área delimitada no estado de Minas Gerais e busca contribuir para o incremento do banco de dados das cavernas presentes. A principal ferramenta será a de cadastro de pontos de controle ou de feições com as suas respectivas descrições e imagens associadas. Aliado a esse cadastro de pontos, o aplicativo irá coletar os caminhos feitos pelo o usuário e, durante esse caminho, ele terá a liberdade de criar quantos pontos forem necessários. Outra funcionalidade importante que está atribuída a esse aplicativo é a de visualizar os caminhamentos, tanto de uma forma individual, clicando sobre o caminho realizado, quanto de forma geral que será exibida em seu menu principal. O sistema também contará com um gerador de relatórios baseado nos pontos e fotografias tiradas em campo.

Na última semana do mês de outubro ocorreu a validação mensal do trabalho realizado pelas equipes do TerraLAB, acompanhe nos vídeos abaixo o que foi produzido e quais são as principais ferramentas e tecnologias que estão sendo utilizadas e estudadas nesses projetos.

GESTÃO DE PROJETOS

O gerente de projetos, Emanuel Xavier, nos conta que no mês de Outubro as equipes estão sendo reestruturadas para ter uma melhor distribuição do trabalho e evitar sobrecarga de alguns membros. Além disso o Jira foi atualizado para ser capaz de mostrar nos dashboards uma visão do trabalho de toda a equipe durante a sprint, e ao mesmo tempo os outros dashboards informam o trabalho dos serviços de cada equipe, usados para revisões mais detalhada e reuniões

ENGENHARIA 

O nosso gerente de Engenharia de Software, Higor Duarte de Oliveira, apresenta no vídeo a seguir a realização de um experimento no Geocodificador e API Gateway, e a coleta e análise de métricas referentes ao experimento. Ele também fala de como e por que foi criado um repositório Git, contendo um setup do Blog do TerraLab feito em WordPress e configurado para executar dentro de um container Docker. Enfim se mencionam a criação de um documento, para uso interno do departamento, com dicas de leitura de tutoriais e artigos técnicos na capacitação de membros da equipe.

DATA ANALYTICS

No vídeo deste mês, o gerente de Data Analytics, Alan Santandrea, apresenta as tarefas realizadas por seu time, que ajudou a equipe de Infraestrutura com o projeto do banco de dados e fez alterações na biblioteca de Análise Geo, a fim de melhorar as funções para produção em escala dos dados. Também foram testadas e implementadas algumas ideias de tratamento de entrada nos endereços e a concepção de uma aplicação para os clientes onde é possível renderizar mapas de dispersão, se os endereços solicitados existirem em nossa base ou a geolocalização dos mesmo for passada pelo usuário.

INFRAESTRUTURA

Agora o Guilherme Carolino, gerente de Infraestrutura do TerraLAB, nos apresenta os serviços ofertados durante o mês de Outubro pelo time. São 4 serviços: Serviço de Geocodificação Serverless, Serviço de Capacitação da equipe, Serviço Transferência de Conhecimento e Operacional. No Serviço de Geocodificação foram desenvolvidos o script para consumo de API, o Banco de Dados relacional e o monitoramento de tempo via Cloudwatch. No Serviço de Capacitação foram concluídos cursos de Segurança e Kubernetes. No Serviço de Transferência de Conhecimento foram confeccionados artigos e gravações de vídeos para publicação. Por fim, no Serviço de Operação, foram realizadas as demandas internas das equipes do laboratório.

USER EXPERIENCE

No vídeo a seguir, o gerente de UX, João Pedro Siqueira nos mostra o que o seu time realizou no mês de outubro. Em relação ao projeto GeoService (GS) a equipe de UX concluiu algumas tarefas como a elaboração de telas e cenários de testes para essas telas, a análise de um documento de briefing para gerar histórias de usuário e um benchmark de aplicações e serviços semelhantes ao GS; Quanto ao projeto BatCaverna foram concluídas as telas referentes à funcionalidade de configurações da aplicação.

MARKETING

No mês de Outubro o marketing deu uma desacelerada, pois todos os membros da equipe performam também posições técnicas (onde está o foco atual de entrega de resultados do TerraLab). O gerente de marketing, Luka Menin, nos fala sobre as mudanças na operação do departamento, mostra os artigos publicados (leia eles aqui e aqui) e o que foi postado nas redes sociais do Lab, decorre sobre as pesquisas realizadas pelo time e sobre a criação de um relatório detalhando o que é o “estado da arte” para o marketing de empresas de TI. Veja o vídeo abaixo:

Nós estamos há seis meses trabalhando nos projetos aqui citados. Utilizamos o primeiro mês para planejamento e esta é, portanto, nossa quinta entrega. Caso você represente uma empresa ou seja um/uma profissional experiente, gostaríamos do seu feedback para que possamos melhorar continuamente o nosso trabalho. 

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. 

O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab! 

Artigo escrito por Luka Menin. Revisado por Prof. Rodrigo Cesar."
O que são as histórias de usuário e como escrevê-las bem?,http://www2.decom.ufop.br/terralab/o-que-sao-as-historias-de-usuario-e-como-escreve-las-bem/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/10/O-que-sao-as-historias-de-usuario-e-como-escreve-las-bem-2-730x350.png,"No desenvolvimento de software e gerenciamento de produto, uma história de usuário é uma explicação informal e geral sobre um recurso de software escrito a partir da perspectiva do usuário final ou cliente. Neste artigo mostraremos o que são, como escrevê-las e os pontos positivos de se utilizar histórias de usuário.

Introdução

Uma história de usuário é uma descrição curta, informal e em linguagem simples de alguma funcionalidade de um sistema sob o ponto de vista do usuário. Utilizadas nos métodos ágeis de desenvolvimento de software, cada história deve ter valor de negócio na visão do cliente e é uma pequena parte da funcionalidade, não necessariamente uma especificação completa, o que minimiza a necessidade de uma extensa documentação. 

O intuito deste artigo é descrever o que são as histórias de usuários e como escrevê-las, pois com histórias de usuário você dá a uma equipe de desenvolvimento o contexto e o porquê do que está sendo desenvolvido. Isso os ajuda a entender como eles estão fornecendo valor para o negócio e para manter o usuário/cliente no foco das atenções, além de aprimorar a eficiência do time.

Como escrever uma boa história de usuário?

Uma boa história deve ser:

Independente (não se sobrepor a outra, sendo possível desenvolvê-las em qualquer ordem).
Negociável (Devem ser co-criadas pelo cliente e o time de desenvolvimento, sendo aptas a responder às mudanças quando necessário).
Valiosa (Precisa entregar o valor para o cliente, sendo que inclusive os aspectos técnicos precisam ser apresentados de forma que o cliente os considere importantes).
Estimável (A equipe precisa conseguir estimá-la e para isso precisa entendê-la).
Pequena (Boas histórias tendem a serem pequenas, reduzindo incertezas e facilitando as estimativas).
Testável (Permite ser validada e para isso, utilizamos critérios de aceitação).
Critérios de aceitação

Critérios de aceitação são os itens que precisam ser atendidos para que a história do usuário seja aceita por um usuário ou cliente. 

Nos critérios existem as informações necessárias para a construção e o funcionamento do sistema, tais como regras de negócio, restrições de acesso e mensagens.

Eles facilitam o entendimento de como a funcionalidade será executada pelo usuário e eliminam ambiguidades, trazendo mais clareza aos requisitos. Além disso, delimitam fronteiras para a história do usuário.

Devem ser escritos com linguagem clara o suficiente para serem entendidos por todos e serem independentes de implementação. Critérios de aceite definem o que fazer e não como fazer.

Criando histórias de usuário

Crie histórias de usuário de forma colaborativa, elas nunca devem ser simplesmente entregues a uma equipe de desenvolvimento, e sim eles devem ser inseridos em uma conversa onde o Product Owner e a equipe devem discutir as histórias juntos. Isso permite que você capture apenas a quantidade mínima de informações, reduza a sobrecarga de trabalho e acelere a entrega.

Dessa forma, adicione critérios de aceite, pois eles complementam a narrativa e permitem a você descrever as condições que devem ser atendidas para que a história seja feita. Além de enriquecerem a história, as tornam testáveis garantindo que possam ser demonstradas ou divulgadas para os usuários e outras partes interessadas.

Uma história de usuário geralmente segue o seguinte “padrão”:

Como um <tipo de usuário>, quero <algum recurso>, para que <algum motivo>.

Como um <tipo de usuário> – Para quem estamos construindo isso? Quem é o usuário? / Papel que desempenha alguma ação no produto em teste.

Gostaria de <algum recurso> – O que estamos construindo? Qual é a intenção? / Realizar alguma ação no produto sobre teste.

Para <alguma razão> – Por que estamos construindo? Qual é o valor para o cliente? / Algum resultado específico, ou que algo seja feito.

Para ficar mais claro, observe os exemplos abaixo:

Como um usuário não cadastrado

Gostaria de realizar o cadastro

Para que eu possa acessar as funcionalidades do aplicativo

Critérios de aceite:

– Um usuário não pode se cadastrar sem preencher todos os campos obrigatórios (nome, sobrenome, número de telefone, e-mail e senha).

– Um usuário não pode se cadastrar caso digite um número de telefone inválido.

– Um usuário cadastrado não pode se cadastrar novamente.

Como um vendedor

Gostaria de verificar se um livro está disponível no estoque

Para que eu possa vendê-lo ao cliente

Critérios de aceite:

– O vendedor não pode solicitar a busca se não informar o nome do livro. 

– O sistema, encontrando o livro, deve apresentar todos os dados do livro (nome completo, autores, editora, ano de edição).

– O sistema não encontrando o livro, deve informar que o livro não foi encontrado.

Considerações finais

Além de sua agilidade e leveza, as histórias mantêm o foco no usuário. Uma lista de histórias mantém a equipe focada em resolver problemas de usuários reais, aumentando a qualidade do produto final.

Nesse sentido, elas também permitem a colaboração possibilitando a equipe de trabalhar em conjunto para decidir como atender melhor o usuário. Ademais, elas impulsionam soluções criativas, incentivando o pensamento crítico e criativo da equipe sobre a melhor maneira de resolver para chegar na meta final.

Agora que você já sabe o que são e como criar as histórias de usuário, já pode utilizá-las para a aplicação do método ágil em sua equipe!

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Siga-nos nas redes sociais para saber mais!

Escrito por Italo Gustavo Guasti e revisado por Prof. Tiago Carneiro."
Um breve histórico sobre virtualização,http://www2.decom.ufop.br/terralab/um-breve-historico-sobre-virtualizacao/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/10/page-blog-1.png,"Avirtualização surgiu em 1960, quando computadores eram muito grandes e caros e, desta forma, era necessário compartilhar seus recursos para que não ficassem ociosos. Atualmente, os servidores são computadores relativamente robustos cujos recursos também precisam ser compartilhados. Eles são utilizados para hospedar diversos serviços como: autenticação (firebase), armazenamento e processamento de dados (servidor de bancos de dados), aplicativos web (google docs, whatsapp), etc. Muitas vezes, estes serviços são hospedados em um mesmo servidor e precisam ter a ilusão de que estão instalados um equipamento dedicado a ele, tendo limites de uso de memória, de disco e de processamento definidos pela pessoa que administra o servidor.  É a tecnologia de virtualização que oferece aos serviços essa ilusão.

Logo, podemos entender que as tecnologias de virtualização tem como objetivo permitir o compartilhamento de recursos computacionais, oferecendo aos usuários (sejam eles pessoas ou softwares) a ilusão de que possuem um equipamento dedicado a eles. Atualmente, a tecnologia de virtualização é utilizada por diversas empresas para oferecer servidores virtuais que podem ser alugados por qualquer usuário para a hospedagem de serviços que ele(a) próprio(a) ou sua empresa desenvolveu.  Desta maneira, esses usuários não precisam se preocupar com a manutenção dos equipamentos e com a administração e segurança da rede que os interconecta, nem mesmo com o espaço físico e com a energia elétrica que estes equipamentos demandam. 

Servidores onde se hospedam serviços virtuais

Este artigo tem como objetivo introduzir e distinguir conceitos como máquinas virtuais, containers docker mostrando como as tecnologias de virtualização evoluíram ao longo do tempo.

Isolamento de serviços por meio da virtualização

Uma vez que um servidor dificilmente vai executar apenas um único serviço a virtualização também é uma ótima estratégia para isolá-los. Como a virtualização oferece a cada serviço a ilusão de que eles possuem um equipamento exclusivo a eles dedicados, os serviços que possuem conjunto de dependências (bibliotecas ou frameworks, por exemplo) diferentes, ou que trabalhem com versões diferentes destas dependências, ou que demandam regras de firewall distintas, não irão interferir uns nos outros. 

Máquinas virtuais e Sistemas Operacionais

Nenhum aplicativo (software) conversa diretamente com a máquina (hardware). Os sistemas operacionais são programas de computador que intermediam essa conversa, tornando-a mais fácil e gerenciando todos os recursos da máquina. Assim, os aplicativos executam sobre o sistema operacional e quando precisam de qualquer recurso computacional da máquina, eles precisam solicitar ao sistema operacional que aloque este recurso, seja ele espaço de memória ou disco, seja tempo de processamento (CPU), ou qualquer acesso aos dispositivos de entrada (teclado, mouse, joystick) e saída (tela, impressora).

A virtualização permite que vários sistemas operacionais sejam simulados dentro de um outro sistema operacional e é uma prática amplamente utilizada até os dias de hoje. Alguns benefícios da virtualização são: Reduzir a quantidade de equipamentos utilizadas e aproveitar melhor um hardware potente (com partes ociosas), diminuir os custos com hardware, eliminar o hardware não confiável, diminuir o gasto com energia elétrica, diminuir o calor produzido pelas máquinas, obter mais espaço físico, etc.

Máquina virtual é o nome que recebem os softwares de virtualização que atuam desta maneira, permitindo a simulação de um sistema operacional dentro de um sistema virtual hospedeiro. As máquinas virtuais mais famosas são: VMware, Virtualbox, Parallels Desktop e Microsoft Hyper-V. Todavia, cada novo sistema operacional virtualizado acarreta em um custo alto de hardware, que é consumido apenas apenas para manter os diversas camadas de sistema operacional (sem o serviço). Por exemplo, é possível instalar o sistema operacional Windows dentro de uma máquina executando o sistema operacional Linux e vice-versa.  No entanto, em ambos os casos, serão consumidos recursos computacionais em dobro (memória, processamento e disco) para manter os dois sistemas operacionais em execução e ainda para manter o software de virtualização que cria a máquina virtual. Além disso, os serviços instalados sobre o sistema operacional virtualizado também demandará recursos e executará um pouco mais lento. Uma fragilidade crítica advém do fato de que se uma máquina virtual apresenta problema, então, todas as dependências dos serviços instalados sobre ela serão afetadas. É claro, o mesmo acontece se o sistema operacional falhar.

Containers e Dockers

Uma alternativa ao sistema de virtualização tradicional é a tecnologia Linux Container (LXC). Essa tecnologia move para o núcleo (do inglês, kernel) do sistema operacional as funcionalidades que permitiam as máquinas virtuais oferecer o compartilhamento de recursos computacionais e o isolamento dos ambientes de execução, oferecendo a ilusão de um equipamento exclusivo e dedicado aos usuários. Como estas funcionalidades se tornaram nativas ao núcleo do Linux, elas executam de forma mais rápida, podem ser compartilhadas por diversos serviços e evita a sobrecarga trazida por uma camada de virtualização. Caso esteja curioso, o núcleo do sistema operacional é seu principal componente, ele é responsável pela interface entre hardware e processos em execução, além de coordenar a comunicação entre esses processos. Infelizmente, a tecnologia de containers não funciona nativamente no Windows e no MacOS, é exclusiva do Linux.

A tecnologia de container isola um conjunto de processos e arquivos em um “container” (conceito fictício para representar o isolamento desse conjunto de arquivos). Ela também permite a um usuário controlar a quantidade máxima de hardware que um container poderá consumir (CPU, memória, disco, rede, etc). Desta forma, é como se ambientes totalmente isolados fossem criados, em que um não tem acesso e nem sabe da existência do outro, mas compartilham o mesmo núcleo do sistema operacional.

A empresa Docker criou o mais famoso e utilizado software ao redor da tecnologia  Linux Containers. Um serviço esse que possibilita diversas ações como: Criação de imagens Docker, isto é, conjuntos de códigos automatizados para criação de um container que também pode possuir o código fonte para outros programas (banco de dados, frameworks, etc); Iniciar e parar a execução de um container; Compartilhamento de volume entre o computador principal e os containers de forma automatizada; Criação de uma rede própria entre os containers; Etc. Desta forma, o uso de containers se torna muito mais prático e funcional.

Exemplo Prático com Docker

Esta seção de texto traz uma demonstração básica do funcionamento de containers Docker, para auxiliar na compreensão do artigo. Após fazer o download e instalar o programa Docker, você vai acessar o terminal do seu sistema operacional e seguir o passo a passo abaixo:

docker run –name some-postgres -e POSTGRES_PASSWORD=mySecret -d postgres

O comando “docker run” executa uma imagem docker qualquer. Os parâmetros “–name” e “-e” permitem a um usuário dar um nome ao container sendo criado e configurar variáveis de ambiente nesse container, respectivamente. Neste caso, o comando criará um container a partir de uma imagem do servidor de banco de dados “postgres”, conforme requisitado pelo último parâmetro. O parâmetro “-d” é o comando disattached para a imagem rodar em background, liberando o terminal após o comando ser executado . O container recebe o nome “some-postgres” e a variável de ambiente “POSTGRES_PASSWORD” recebe a senha do servidor de banco de dados. Caso você não tenha a imagem “postgres” no seu computador,  o docker irá fazer o download da imagem a partir da coleção de imagens disponíveis no Docker Hub, isto é,  um repositório para o download de imagens pré-configuradas. Ao final do comando “run”, o docker e irá te retornar o container ID no seguinte formato:

ee16c32b855f42d349f5d7f2820549294630a05877f53b09ab101e7ca8759caa

Caso o usuário necessite, ele poderá acessar o servidor Postgre no endereço localhost, em seu porto padrão 5432. Mas para isso, ele precisará primeiramente redirecionar este porto para o container que ele acaba de criar, por meio do parâmetro “-p 5432:5432” (host:docker port), redirecionando o porto 5432 do computador para o porto 5432 do seu container.

O comando docker ps retorna todos os containers dockers em execução no momento, e ainda mostra algumas informações como o ID do container, a imagem utilizada para sua criação, o estado do container (status), a data de criação (created),a porta em que o container está escutando.

docker ps

O comando abaixo é utilizado para interromper a execução do container cujo ID é fornecido como parâmetro. Caso seja interrompido com sucesso, o comando retorna o ID do próprio container. Neste caso, por não haver ambiguidades, as 4 primeiras letras do ID são suficientes para identificá-lo.

docker stop id_do_container

Arquitetura de Microsserviços

A partir da tecnologia de container, surge a arquitetura de microsserviços que se molda de forma que cada parte de um serviço fica isolada da outra e se comunicam através da rede que interliga os contêineres. Por exemplo, tradicionalmente, todos os componentes de uma aplicação WEB desenvolvida em PHP seriam instaladas em um único hardware (lado esquerdo da imagem abaixo). Na arquitetura de microsserviços, uma instância docker executaria e isolaria o servidor de banco de dados MYSQL, outra instância docker executaria o servidor de aplicação que foi desenvolvido em PHP-FPM, então, uma terceira instância docker hospedaria os componentes do site em um servidor HTTP da Apache. A arquitetura de microserviços confere maior estabilidade, manutenção mais fácil, melhor escalabilidade (é mais fácil escalar os containers docker) e atualização mais fácil e rápida. Por esses e outros benefícios a arquitetura de microsserviços é uma das arquiteturas mais utilizadas atualmente.

Considerações finais

A virtualização vem se tornando uma tecnologia cada vez mais necessária para a computação. Ela vem reduzindo o custo de recursos computacionais e vem permitindo que os usuários paguem apenas pelo uso efetivo destes recursos. Ela também vem aumentando a utilização de equipamentos, evitando desperdícios. Ela desonera os usuários do esforço de manutenção dos equipamentos e da segurança das redes que os conecta.

Neste artigo, abordamos as diversas formas de virtualização apresentando um breve histórico de como evoluíram. Na segunda metade do artigo, demos ênfase ao seu uso para o desenvolvimento de arquiteturas de microsserviços. No entanto, este ainda é um artigo superficial e introdutório. Diversas  tecnologias foram criadas ao redor da virtualização e existe uma extensa gama de conteúdos para se aprender. Gostaria de aprender mais sobre virtualização, arquitetura de microsserviços e as tecnologias que as rodeiam?  Você atua profissionalmente na área? Tem algo a acrescentar nesta discussão? Alguma dúvida sobre aquilo que apresentamos? O seu comentário é muito bem-vindo! Comente, compartilhe esta publicação e não deixe de acompanhar nosso blog e redes sociais.

Referências 

Docker Docs

Virtualização: uma realidade flashback

Linux Containers vs Docker – What is the Difference and Why Docker is Better

Artigo escrito por Guilherme Carolino .Revisado por Prof. Tiago Carneiro."
5 Dicas de UX/UI sobre como construir um bom formulário,http://www2.decom.ufop.br/terralab/5-dicas-de-ux-ui-sobre-como-construir-um-bom-formulario/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/09/BlogPost-29-09-1-730x350.png,"Aferramenta de formulários, nos dias atuais, é uma das mais utilizadas em diversas plataformas, tanto web quanto mobile, e é ideal para quem precisa solicitar feedback sobre algo, organizar inscrições para eventos, pedir avaliações, realizar pesquisas ou coleta de dados de diversos públicos e com diversos objetivos.

Uma das principais preocupações ao criar um é “qual seria a melhor interface para esses formulários?”. Nesse artigo você verá 5 dicas de como fazer um formulário mais interativo, eficaz e menos cansativo para os usuários.

1 – Agrupe todos os campos que forem relacionados

Quando se está construindo um formulário é necessário ficar bem atento com a sua estruturação. Um dos erros mais comuns nessa etapa é colocar as informações dos formulários de qualquer jeito e isso é uma má ideia do ponto de vista do usuário, pois o confunde em qual parte se encontra e o que se deseja coletar com essas perguntas.

Uma das melhores formas de se construir um bom formulário é manter as perguntas em uma sequência intuitiva, de forma lógica e sempre agrupando os campos relacionados de acordo com as perguntas feitas pelo formulário, assim estará sempre informado aos usuários em qual seção ele se encontra e qual é o objetivo de cada uma daquelas perguntas.

Bom agrupamento vs. Agrupamento incorreto
2 – Defina os campos em uma única coluna

Durante a elaboração de um formulário, é importante criá-lo com apenas uma coluna, pois o usuário terá uma velocidade de leitura e de compreensão muito mais rápida lendo de forma vertical, em comparação ao em formato de Z, onde o tempo é bem maior e a desorganização é mais comum de acontecer.  Porém toda regra possui sua exceção. Quando o campo é pequeno, como, por exemplo, município e estado, é possível manter em duas colunas sem problemas porque não é prejudicial ao desempenho do formulário.

Leitura em Uma coluna vs. Duas colunas
3 – Validação dos campos

Durante uma validação de qualquer campo que seja, é fundamental que o campo apresente para os seus utilizadores um feedback de sua ação no determinado campo preenchido, seja um feedback positivo ou negativo. É importante deixar claro que aquele campo é válido, com isso é possível prevenir que o usuário cometa um erro.

Abaixo estão dois modos de mensagem de feedback para os seus utilizadores. 

Feedback positivo vs. Sem feedback
Feedback Negativo vs. Sem feedback
4 – As opções devem ficar visíveis quando possuir mais de 6 opções

Ao criar uma secção do formulário em que o usuário precise fazer uma escolha entre uma das opções listadas, caso hajam mais de 6 opções de escolha é aconselhável que se utilize o componente de dropdown, pois este permite deixar o formulário menos poluído e requer apenas dois cliques: um para exibir as informações e outra para esconder as opções de seleções.

Assim o processo de escolha fica mais intuitivo para os seus usuários. Caso haja menos de 6 opções de escolha é agradável deixar todas as opções visíveis, como em botões. 

Muitas opções de escolha vistas em menu Dropdown vs. Sem Dropdown
5 – Formatação adequada para campos de respostas

O Instituto Baymard fez uma pesquisa sobre o estudo de usabilidade  e percebeu que quando os campos apresentam um tamanho muito maior do que o necessário, faz o usuário se questionar se preencheu o campo corretamente, se a resposta esperada era realmente aquela. Então o ideal é sempre ajustar o tamanho do campo conforme o tamanho da resposta, por exemplo: quando já possuírem campos definidos, como o CEP, colocar o tamanho do campo de acordo com o tamanho dos CEPs brasileiros.

Campos de tamanho ajustado ao seu conteúdo vs. Sem ajuste
Considerações finais

Pronto! Aproveite as dicas para criar formulários online de preenchimento de mais rápido, fácil e intuitivo para seus utilizadores.

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Siga-nos nas redes sociais para saber mais!

Artigo escrito por João Pedro Mendes. Revisado por Luka Menin."
Breve introdução ao Figma: Um editor vetorial para prototipagem e projetos de interfaces gráficas,http://www2.decom.ufop.br/terralab/breve-introducao-ao-figma-um-editor-vetorial-para-prototipagem-e-projetos-de-interfaces-graficas/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/09/page-blog-.png,"OFigma é uma ferramenta completa para a prototipagem e desenvolvimento colaborativo de  projetos de interfaces gráficas com o usuário (GUIs – Graphical User Interfaces). Ele próprio possui uma interface intuitiva, não importa o seu nível de experiência.

Alternativa online a produtos como Sketch, Adobe XD e Adobe Illustrator, o Figma permite o acesso simultâneo em um mesmo projeto, possibilitando o desenvolvimento em conjunto de um projeto em tempo real. Todo trabalho é salvo automaticamente, além de ser possível ver o histórico de versões.

Entendendo a interface do Figma
Figura 1. Visão geral da interface

Podemos separar a interface do Figma em quatro grandes áreas, sendo elas:

Área verde: Ferramentas mais importantes (Toolbar)
Área vermelha: Camadas de informação e páginas (Layers)
Área azul: Área de trabalho (Canvas)
Área amarela: Painel de propriedades (Property panel)
Como usar o Figma
Figura 2. Criando um Frame

O primeiro passo é escolher um formato, e para isso temos o Frame, que permite você escolher uma área da tela para criar seus projetos. Há várias opções como tela para celular, desktop, mídias sociais, entre outros. Frames também dão acesso a funcionalidades como Layout Grids, Layout Automático, Constraints e Prototipagem.

Figura 3. Exemplo de um Frame de postagem para Instagram e tela para Android.

Cada Frame vem com o tamanho padrão, podendo ser redimensionado. As informações de propriedades ficam na área amarela, Figura 1. Pode ser alterado cor, tamanho e ainda é possível adicionar efeitos, entre outras coisas.

No próximo passo vamos adicionar elementos ao Frame e começar a ver a hierarquia das camadas sendo formada. Cada elemento é adicionado no topo da hierarquia e irá sobrepor todos os elementos adicionados anteriormente.

Figura 4. Hierarquia dos elementos

Ao selecionar o elemento, suas propriedades serão apresentadas e editadas no painel à direita, podendo alterar cor, tamanho, adicionar efeitos como contorno, sombra, entre outros. 

Figura 5. Propriedade dos elementos

Fill: É o local onde será definida a cor. Para isso, basta clicar no quadradinho com a cor e aparecerão outras. Além da cor sólida, também é possível escolher entre Linear, Radial, Angular, Diamante e uma imagem.

Stroke: É contorno da forma.

Effects: Possui quatro tipos de efeitos:

Drop shadow: Sombra por trás;
Inner shadow: Sombra por dentro;
Layer blur: Borrado;
Background blur: Borra o fundo do elemento.
Visualizando o projeto

Ao clicar no botão de Play, abrirá uma nova guia com a apresentação em tamanho real

Botão para Apresentação do projeto
Figura 6. Apresentação do projeto
Considerações finais

O Figma é uma ferramenta extremamente poderosa, podendo ser utilizado para o desenvolvimento de diversos tipos de peças gráficas, desde imagens simples para publicação em redes sociais até o desenvolvimento de telas de aplicativos (GUIs) de alta fidelidade. Mesmo não tendo experiência, o usuário consegue aprender rápido e se adequar devido a sua interface intuitiva.  Por ter a opção de ser usado online, atrai usuários que não possuem muitos recursos de hardware. Outra grande vantagem é a variedade de plugins de ícones, tipografia, gráficos, entre outros, o que torna o fluxo de trabalho ainda mais rápido. Tudo é salvo automaticamente, podendo ser acessado de qualquer lugar.

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa.

O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software. Siga-nos nas redes sociais para saber mais!

Artigo escrito por Vitor Hugo Leles Fonseca. Revisado por Prof. Tiago Carneiro."
Full Stack ou especialista? A resposta pode estar em você!,http://www2.decom.ufop.br/terralab/full-stack-ou-especialista-a-resposta-pode-estar-em-voce/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/09/blog-730x350.png,"Énotório o crescimento de vagas no setor de TI em todo o mundo, mas especificamente no Brasil, vemos um crescimento de mais de 170% das vagas no período de 2007 até 2017, segundo dados da RAIS (Relação Anual de Informações Sociais). Logo, é normal que diversos profissionais de outras áreas ou até mesmo recém integrantes do mercado de trabalho busquem empregos nesse setor. Mas em um mundo de possibilidades, onde existem milhares de tecnologias, bibliotecas, frameworks, linguagens e outras tantas opções, surge uma dúvida que atinge principalmente os iniciantes no assunto: Devo ser um especialista ou me tornar Full Stack?

Você já deve saber que um desenvolvedor  Full Stack é basicamente aquele que reúne conhecimentos que vão desde o back-end até o front-end, ou seja, compreende o projeto como um todo e é capaz muitas vezes de realizar um projeto inteiro sozinho, sendo assim, é de se esperar que as empresas busquem sempre esse perfil de desenvolvedor, certo? Errado! O Full Stack é sim dotado de muitos conhecimentos e tem uma visão ampla sobre todos os processos do projeto, porém sai em desvantagem quase sempre quando a vaga é para uma equipe de desenvolvedores experientes e especialistas em suas respectivas áreas, principalmente, em grandes equipes onde não falta recurso para contratar o melhor profissional de cada área. Porém, ao mesmo tempo, um desenvolvedor Full Stack é realmente muito reconhecido no mercado. Nas equipes pequenas ou com recursos limitados, um desenvolvedor capaz de resolver problemas de várias áreas é perfeito para corte de gastos e para diminuir o gargalo de comunicação na equipe, já que ele será capaz de entender e repassar informações de todas as partes de um projeto.

Então o melhor é realmente ser Full Stack? A resposta é clássica: depende! Diversos artigos e desenvolvedores experientes defendem que essa escolha deve estar muito mais atrelada ao seu perfil como desenvolvedor do que ao seu interesse profissional, empregos não faltam em ambas as áreas, inclusive faltam profissionais, e aos montes! Por ser um mercado que não para de crescer principalmente no Brasil, falta muita mão de obra qualificada no setor. O mais importante aqui é realmente identificar seu perfil de desenvolvedor e tentar entender como você se imagina trabalhando com TI. Especialistas normalmente são pessoas que gostam de se destacar em determinada área e tem consciência de que precisam entender a fundo sobre sua área e principalmente se manter atualizado sobre ela. Ao mesmo passo que um desenvolvedor Full Stack também têm de estudar diversas áreas e se manter atualizado sobre todas elas, mas, como sabemos que é humanamente impossível saber tudo sobre todas as áreas, é indicado que mesmo desenvolvedores Full Stack escolham uma certa área para se aprofundar um pouco mais.

E então, já sabe para qual lado seguir? Sendo assim, fique de olho nos outros artigos do blog que quem sabe você encontra seu caminho. Aproveite para nos seguir nas redes sociais e ficar por dentro de tudo o que acontece no lab.

Artigo escrito por Filipe Rodrigues, revisado por Prof. Tiago Carneiro."
Resultados do TerraLAB no mês de Agosto,http://www2.decom.ufop.br/terralab/resultados-do-terralab-no-mes-de-agosto/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/09/Arte-em-acao-18.png,"Esta é uma sequência de artigos que tem como objetivo mostrar a evolução dos integrantes do TerraLAB ao longo de cada mês. No artigo anterior mostramos parte do desenvolvimento dos três projetos que estão em andamento no TerraLAB envolvendo as nossas equipes de UX, Engenharia, Análise de Dados, Infraestrutura, Gestão de Projetos e Marketing, são eles: o Serviço de Geocodificação em Massa (SGM), o aplicativo Segurança da Mulher e o BatCaverna. 

SGM:  Na área de logística e de pesquisa de mercado, a geocodificação criou certas facilidades, que até então não existiam, e proporcionou uma transformação nos métodos de planejamento estratégico, organizacional e distributivo de mercadorias. Um serviço de geocodificação em massa na nuvem, deve ser desenvolvido pela grande aplicabilidade em diversas empresas de diferentes ramos, pois garante perspectiva de crescimento logístico, financeiro e tecnológico. Além disso, na grande maioria das vezes, dados sobre os sistemas sociais são vinculados a um endereço de logradouro ao invés de a uma coordenada geográfica. Este é comumente os casos dos eventos de saúde ou criminais. Consequentemente, o serviço de geocodificação é útil para a espacialização destes eventos e para o entendimento das maioria dos sistemas sociais. Com essas informações o projeto AWS SGM: Crawler de Geocodificação, busca trabalhar no problema: Como conhecer a localização de um logradouro, sem ir a campo?
Segurança da Mulher: O aplicativo tem como objetivo apontar o nível de segurança, com relação à mulher, de diversos locais da cidade com base em denúncias anônimas feitas pelas usuárias sobre qualquer tipo de importunação sexual. Essas denúncias serão exibidas em um mapa, bem como o nível de segurança dos locais, para que qualquer pessoa possa consultá-las. O aplicativo também conta com um espaço para que o importunador tenha direito de resposta e para que os representantes dos locais denunciados exponham o seu lado da história. Ao mostrar um mapa detalhado com indicadores de zonas seguras e de perigo, espera-se poder contribuir para a livre circulação com segurança das usuárias do aplicativo.
BatCaverna: O aplicativo tem como objetivo fazer a coleta detalhada do patrimônio espeleológico da área delimitada no estado de Minas Gerais e busca contribuir para o incremento do banco de dados das cavernas presentes. A principal ferramenta será a de cadastro de pontos de controle ou de feições com as suas respectivas descrições e imagens associadas. Aliado a esse cadastro de pontos, o aplicativo irá coletar os caminhos feitos pelo o usuário e, durante esse caminho, ele terá a liberdade de criar quantos pontos forem necessários. Outra funcionalidade importante que está atribuída a esse aplicativo é a de visualizar os caminhamentos, tanto de uma forma individual, clicando sobre o caminho realizado, quanto de forma geral que será exibida em seu menu principal. O sistema também contará com um gerador de relatórios baseado nos pontos e fotografias tiradas em campo.

Na última semana do mês de agosto ocorreu a validação mensal do trabalho realizado pelas equipes do TerraLAB, acompanhe nos vídeos abaixo o que foi produzido e quais são as principais ferramentas e tecnologias que estão sendo utilizadas e estudadas nesses projetos.

USER EXPERIENCE

No vídeo a seguir, o gerente de UX, João Pedro Siqueira nos mostra o que o seu time realizou no mês de agosto. O backlog da equipe foi colocado no Jira, software voltado para o monitoramento de tarefas e acompanhamento de projetos, foi feita a divisão de cargos dentre os membros da da equipe e, por fim, além da prototipagem das telas, que foram validadas pelo cliente.

ENGENHARIA

O nosso gerente de engenharia, Vinícius de Paula, mostra no vídeo a seguir a reestruturação do GitLab e os backlogs dos times de desenvolvimento definidos e adicionados no Jira. Vinícius fala do planejamento de experimentos de análise de desempenho, análise de escalabilidade e impacto de diferentes arquiteturas. E, por fim, mostra os relatórios sobre API Gateway e Kubernetes, usados nas aplicações.

DATA ANALYTICS

No vídeo deste mês, o gerente de Data Analytics, Alan Santandrea, apresenta a biblioteca em Python criada pelo chapter do seu time. A biblioteca contempla funções boas para gerar insights e análises em cima de dados geoespaciais, contribuindo para aqueles que precisam tratar bases de dados de endereços ou querem gerar valor em cima de bases de dados geográficas. O vídeo mostra uma possível análise usando as funções da biblioteca e mostra como agregar valor à base além de algumas visualizações bem interessantes de mapas.

INFRAESTRUTURA

Agora o Guilherme Carolino, gerente de infraestrutura do TerraLAB, nos mostra o projeto feito pelo seu time que funciona como um serviço de geocodificação utilizando a tecnologia Serverless. O projeto possui duas lambdas principais: A primeira faz requests para APIs de geocodificação dados “N” endereços e escreve em uma fila de mensagens, que é instanciada em uma máquina virtual, enquanto a segunda lambda consome a fila de mensagem e escreve no banco de dados.

GESTÃO DE PROJETOS

No mês de Agosto, com o fim do processo seletivo, a equipe do TerraLAB cresceu. Isso levou os gerentes de projeto, Emanuel Xavier e Bernardo Santos, a realizarem o remanejamento das equipes para integrar os novos Trainees aos projetos do laboratório. Além disso, a forma de uso da ferramenta Jira foi estudada e implantada, se adequando tanto às necessidades do lab quanto à metodologia da Gerência de Projetos. Estabelecida a plataforma de controle de projetos e negócios, passamos a dispor de relatórios e gráficos que nos permitem monitorar e avaliar os avanços dos projetos, mostrados no vídeo a seguir.

MARKETING

No mês de agosto a equipe de marketing atualizou as abas de artigos científicos e empresas parceiras do lab em nosso site, além de desenvolver um documento de controle de publicações nas redes sociais. O backlog da equipe também foi adicionado no Jira. Os nossos artigos mais visualizados e as publicações que mais nos trouxeram resultados são mostrados no vídeo a seguir pela nossa gerente de marketing, Paloma Bento.

Nós estamos há quatro meses trabalhando nos projetos aqui citados. Utilizamos o primeiro mês para planejamento e esta é, portanto, nossa terceira entrega. Caso você represente uma empresa ou seja um/uma profissional experiente, gostaríamos do seu feedback para que possamos melhorar continuamente o nosso trabalho. 

Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab! 

Você sabe como a escassez de recursos humanos tem impactado negativamente as empresas de software? Você sabia que uma parceria conosco lhe ajudaria a reduzir esses impactos? Leia mais sobre estas duas questões neste artigo escrito pelo time TerraLAB. "
Mudança de carreira,http://www2.decom.ufop.br/terralab/mudanca-de-carreira/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/08/blog.png,"Que tal trabalhar com Tecnologia da Informação?

Permanecer no mesmo setor e seguir em uma mesma carreira por muito tempo, às vezes até a aposentadoria, era algo comum e visto com bons olhos até certo tempo atrás. Porém, nos dias atuais, em que o mercado de trabalho exige dinamismo e certas áreas estão em ascensão, enquanto outras estão saturadas de profissionais, as pessoas não costumam permanecer por muito tempo na mesma atividade. Também existem as que procuram fazer a transição de carreira a fim de serem absorvidas pelo mercado.

O presente artigo traz uma breve reflexão a respeito da transição de carreira para área de Tecnologia da Informação (TI), discutindo os desafios acarretados, as motivações para a ocorrência de tal mudança e o impacto do TerraLAB para o auxílio no aprendizado e na abertura de novas oportunidades.

O que é transição de carreira?

A transição de carreira é um processo que faz parte de uma boa gestão de carreira e que se feita de maneira correta poderá render bons frutos aos envolvidos. Para isso é necessário que cada escolha faça parte de um plano com objetivos bem definidos. Porém a transição de carreira também pode acontecer de forma inesperada em algumas situações, como por exemplo:

·    Quando a profissão está deixando de existir: O impacto da evolução tecnológica e a sua aplicação nas mais diversas áreas, faz com que novas profissões surjam e que profissões antigas simplesmente desapareçam ou com que o trabalho seja diferente e exija novas habilidades. Um exemplo é que com o surgimento da energia elétrica, surgiu a profissão de eletricista e a de acendedor de lamparinas deixou de existir.

·    Quando há um limite de idade: Certas profissões, como por exemplo um atleta de alto nível, possuem um limite natural que as impedem de serem exercidas para toda a vida. Nesses casos é comum que o atleta se torne treinador ou empresário, no mesmo ramo.

·    Estresse e saturação: A falta de tempo livre, o estresse excessivo e a cobrança permanente comum em algumas atividades podem gerar a necessidade e/ou desejo da realização de uma transição de carreira.

·    Crises e demissões em massa em determinados setores: As crises podem gerar mudanças estruturais e induzir diversos profissionais a se adaptar a novos contextos e realizar novas atividades, mesmo que de forma momentânea.

Fonte: Fundação Instituto de Administração
Os desafios

Atualmente, a alta oferta de vagas no setor de TI, na contramão de algumas áreas da engenharia, faz com que profissionais com certa afinidade e aptidão para o setor optem por realizar a transição de carreira. Um dos desafios para o profissional prestes a optar pela transição para o setor de TI é a qualificação. Segundo o artigo Como a escassez de mão de obra qualificada afeta as empresas de TI, a falta de capacitação têm relação direta com a produtividade das empresas e também com a diminuição considerável do período de vínculo empregatício do profissional de TI.

Outro desafio, é a necessidade de tempo para adaptação. Na maioria dos casos, o profissional que opta pela transição de carreira se encontra nessa situação por conta de um acontecimento inesperado que precisa ser gerido. Além disso,  deixar de exercer uma atividade na qual já se tem experiência e conhecimento para ir em busca de outras oportunidades não é algo que acontece da noite pro dia. É importante ter um bom planejamento financeiro aliado ao planejamento de carreira para que a transição ocorra de maneira saudável.

A motivação financeira  

Além das motivações já supracitadas, a transição de carreira pode acontecer com por motivação puramente financeira, visto que a também já citada escassez de mão de obra qualificada faz com que um profissional dotado das habilidades e expertise requeridas pelo mercado, seja altamente valorizado, como pode-se observar realizando uma pesquisa rápida. Atualmente, profissionais de TI são concorridos pelos departamentos de recursos humanos das empresas deste setor. Este profissional também pode atuar como freelancer em modalidade pessoa jurídica.

Fonte: Catho
Considerações finais

O TerraLAB, por meio de suas parcerias, com o auxílio de mentores e tutores altamente capacitados, aliados à troca de conhecimento entre os membros e outras vantagens elencadas em TerraLAB, oferece oportunidade de desenvolvimento profissional e pessoal em diversas das áreas do setor de Tecnologia da Informação, promovendo um ambiente vantajoso para as empresas e para os seus colaboradores.

Se você é aluno de qualquer curso da UFOP, da graduação ou pós, ou representa uma empresa, entre em contato conosco e conheça mais sobre a iniciativa, nossa equipe, nossos projetos, nossos atuais parceiros e benefícios! Para mais informações, acesse a nossa página e nossas redes sociais. 

Artigo escrito por Lucas Cassimiro, Revisado por Prof. Tiago Carneiro."
Por que devemos ensinar programação para crianças?,http://www2.decom.ufop.br/terralab/por-que-devemos-ensinar-programacao-para-criancas/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/08/Arte-em-acao-17.png,"Segundo especialistas, não saber programar é considerado o “analfabetismo do futuro”. Os computadores estão cada vez mais presentes em nosso cotidiano e muitas vezes cumprem tarefas que seriam impossíveis para nós realizarmos. Estamos prontos para viver em um mundo cercado por tamanha tecnologia?

A nova geração, composta pelos chamados “nativos digitais”, está constantemente exposta a telas de computadores, televisores, tablets e smartphones, mas será que eles entendem as máquinas e têm a capacidade de dominá-las? A resposta para a  maioria é não! Apesar do contato constante com a tecnologia, jovens e crianças são expostos a séries, vídeos, filmes, redes sociais e videogames na maior parte do tempo, conteúdos esses empobrecedores que fazem dessa geração apenas consumidora e não produtora de tecnologia. E é aí que mora o problema. 

No livro A Fábrica de Cretinos Digitais¹, o neurocientista francês Michel Desmurget, diretor de pesquisa do Instituto Nacional de Saúde da França, faz um alerta à sociedade sobre como o uso indevido da tecnologia pode afetar o desenvolvimento cognitivo das crianças. Desmurget observou que o tempo gasto em frente a uma tela para fins recreativos atrasa a maturação anatômica e funcional do cérebro em várias redes cognitivas relacionadas à linguagem e à atenção.

Em entrevista à BBC News Mundo, Desmurget foi questionado se os alunos devem aprender habilidades e ferramentas básicas de informática por meio de tecnologia digital; o neurocientista respondeu que se o uso de um determinado software promove efetivamente a transmissão do conhecimento, então sim, a tecnologia digital pode contribuir para o desenvolvimento dos jovens e crianças.

Mas afinal, existe idade certa para começar a aprender a programar?

A britânica Stephanie Shirley, pioneira, empresária e filantropa britânica do ramo da tecnologia da informação, defende que quanto mais cedo as crianças forem introduzidas ao mundo da programação, mais fácil será de assimilar os seus conceitos. Shirley acredita que crianças podem ter o primeiro contato com os fundamentos da lógica de programação a partir dos dois anos de idade, por meio de plataformas voltadas ao ensino para crianças. 

Por meio de conceitos relacionados à sequência, condição e repetição, ligados à uma estrutura lógica disposta em comandos, a programação é capaz de desenvolver a criatividade, o raciocínio lógico, a capacidade de resolver problemas das crianças e adolescentes, além de impactar no desempenho acadêmico de áreas relacionadas à matemática. Apesar de ser promissor, o mercado da indústria de tecnologia da informação sofre e sofrerá com muitos problemas vinculados à falta dessas habilidades (leia mais) que devem ser desenvolvidas em crianças, mesmo que elas não sigam carreira no ramo da tecnologia.

Considerações finais 

Diante da dificuldade que muitos pais têm em proibir o uso das telas ou até mesmo reduzi-lo, nós trouxemos algumas plataformas de programação online e gratuitas que podem, de forma lúdica e atrativa, fazer com que o tempo passado em frente às telas contribua de forma significativa para essa geração, são elas:

 Scratch: O Scratch é uma linguagem de programação que te permite “programar seus próprios jogos, animações e histórias interativas — e compartilhar suas criações com outras pessoas na comunidade on-line. O Scratch ajuda os jovens a aprender a pensar criativamente, raciocinar sistematicamente, e trabalhar em grupo — habilidades essenciais para a vida no século 21.”
Code: “A Code.org® é uma organização sem fins lucrativos dedicada a expandir o acesso à ciência da computação em escolas e aumentar a participação das mulheres e das minorias não representadas. A visão dessa organização é de que todo estudante em toda escola tenha a oportunidade de aprender ciência da computação, assim como aprende biologia, química ou álgebra. Essa organização oferece o currículo mais utilizado para o ensino de ciência da computação nas escolas de ensino fundamental e médio. Além de organizar a campanha anual Hora do Código, que envolveu 10% de todos os alunos do mundo. A Code.org é apoiada por doadores generosos, incluindo a Amazon, o Facebook, o Google, a Infosys Foundation, a Microsoft e muitos outros.”
MIT App Inventor: “o MIT App Inventor é um ambiente de programação visual intuitivo que permite a todos – até crianças – criar aplicativos totalmente funcionais para smartphones e tablets Android e iOS. Aqueles que são novos no MIT App Inventor podem ter um primeiro aplicativo simples instalado e funcionando em menos de 30 minutos. E mais, nossa ferramenta baseada em blocos facilita a criação de aplicativos complexos e de alto impacto em muito menos tempo do que os ambientes de programação tradicionais. O projeto MIT App Inventor visa democratizar o desenvolvimento de software ao capacitar todas as pessoas, especialmente os jovens, para passar do consumo de tecnologia à criação de tecnologia.”

Gostou do assunto? Quer saber mais sobre o mundo da tecnologia? Conheça o objetivo do TerraLAB em nosso site, siga-nos nas nossas redes sociais e acompanhe este blog para ficar por dentro de tudo o que acontece no lab.

¹ DESMURGUET, Michel. A fábrica de cretinos digitais: os perigos das telas para nossos filhos. Península, 2020.

Artigo escrito por Paloma Bento, revisado por Prof. Rodrigo Pedrosa."
Resultados do processo seletivo 2021/2,http://www2.decom.ufop.br/terralab/resultados-do-processo-seletivo-2021-2/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/08/Arte-em-acao-15.png,"Alcançamos a linha de chegada, pois se encerra, nesta semana, o processo seletivo do Programa Trainee TerraLab 2021/2. Abordaremos neste artigo como foi conduzido o processo seletivo, os resultados alcançados e as dificuldades vivenciadas pelos candidatos durante as 5 semanas que duraram o processo.

O processo seletivo, que hoje se conclui, é um programa que vem capacitando estudantes de graduação e pós-graduação de qualquer curso da Universidade Federal de Ouro Preto (UFOP) para atuar nas empresas de Tecnologia da Informação, e é composto por diversas equipes, cada uma dedicada a uma área de trabalho segundo o modelo organizacional de empresa adotado pelo TerraLAB. Após 5 sprints – nome pelo qual são chamados os ciclos semanais de tarefas – os trainees tiveram bastante a aprender sobre as áreas em que se inseriram durante o processo, sobre cooperação em ambiente profissional, e tiveram também a oportunidade de pôr em prática os conhecimentos que adquiriram nesse período. 

Os trainees foram questionados acerca do motivo que os fizeram se interessar no processo seletivo, e as principais respostas falam da oportunidade que o lab oferece de capacitar com eficiência seus membros para atuar no mercado de trabalho, e o desejo de se conhecer melhor as áreas de atuação disponíveis no mercado de Tecnologia da Informação, para quem ainda não tem familiaridade com as áreas ou ainda não tenha definido qual área  seguir. Apresentaremos alguns de seus depoimentos em seguida.

O processo seletivo e seus resultados
User Experience (UX)

Primeiramente a equipe de UX realizou alguns cursos na plataforma da Alura e se reuniu com o cliente para conhecer o produto. Depois elaborou o briefing e o benchmarking, mostrando os pontos fracos e fortes dos concorrentes. Então, se reuniu novamente com o cliente a fim de obter informações dos seus possíveis usuários e projetaram algumas personas para o produto. Por fim, a equipe elaborou o wireframe da interface gráfica (GUI). Desta maneira, desenvolveram um protótipo de um aplicativo para auxiliar profissionais de engenharia no levantamento de dados de campo.

Desenvolvimento de Software

Todos os squads de desenvolvimento de software foram desafiados a desenvolver um aplicativo móvel que permite aos usuários registrar pontos  geográficos de interesse e registrar caminhamentos. Para completar o desafio, os candidatos foram instruídos a utilizar as tecnologias já empregadas dentro do laboratório (React, React Native e NodeJS) e a aproveitar tutoriais e artigos já escritos em nosso blog. Ensinamos aos candidatos como utilizar o GitLab de acordo com os nossos processos e, com base nisso, planejar backlogs, executar  sprints e coletar métricas de produtividade e qualidade  do projeto.

Data Analytics

Durante o processo seletivo os inscritos tiveram dois desafios. O primeiro era um problema de classificação no qual os participantes deveriam gerar análises e insights em cima da tão famosa base de dados IRIS. Após essa etapa deveriam propor e treinar um modelo de Machine Learning capaz de separar corretamente as classes.O segundo desafio foi um pouco mais complexo e demandou dos participantes a habilidade de extrair informações  de um site, gerar análises e, então, criar um modelo de regressão para  de predizer a coluna ‘PTS’. O vídeo apresenta algumas das análises feitas, os modelos e resultados que os trainees produziram. Ao final, apresenta-se  algumas opiniões dos trainees acerca de todo o processo.  

Infraestrutura

Primeiramente os trainees de Infraestrutura hospedaram uma API (previamente desenvolvida) em Node.Js, no Ec2 da AWS, se familiarizando com servidores e com o sistema Linux, Depois, percorreram um caminho de desafios, que a cada sprint exigia tecnologias novas para ser concluído, sempre atentos à arquitetura e fluxo de execução de seu projeto. As tecnologias mais utilizadas foram aquelas que estão mais presentes nos projetos da infraestrutura no TerraLAB, são elas: Docker, Serverless Framework e AWS Lambda, Ec2, TerraForm, PostGreSQL e CI/CD.

Marketing

Para integrar os trainees aos conceitos do marketing, foram disponibilizados dois cursos: marketing de conteúdo e inbound marketing, após a conclusão dos cursos, foi dada aos trainees a tarefa de elaborar um calendário editorial para futuras publicações do lab, com base nos conceitos recém aprendidos e direcionado às personas criadas por eles. Além disso, os trainees produziram conteúdo digital para as nossas redes sociais e, por fim, escreveram este artigo para divulgar todo o trabalho realizado pelos times durante o processo.

De acordo com a opinião dos trainees, as propostas do TerraLAB são diversas, como: Mostrar como é o ambiente de desenvolvimento de projetos numa empresa de software, dar espaço aos erros de iniciante na área, proporcionar uma vivência de multiplicação de conhecimento ensinar os trainees a fazer networking, permitir a vivência do mercado de trabalho ainda durante a graduação e, ainda, melhorar as habilidades técnicas e de trabalho em equipe dos trainees. Logo, percebe-se que o processo seletivo do lab traz uma grande chance para o desenvolvimento pessoal e profissional do candidato. Esse fato é percebido pelos mesmos desde o primeiro momento.

Considerações finais

Ao longo do processo de Trainee, muita coisa foi trabalhada, desenvolvida e melhorada nos participantes, desde conhecimentos de TI até às habilidades de socialização e comunicação. O processo insere o candidato numa realidade em que metas precisam ser cumpridas, para ver como o mesmo trabalha sob pressão e lidando com mais de uma tarefa ao mesmo tempo. Para isso, o planejamento é fundamental, e somado a ele o feedback e o trabalho em equipe são fundamentais para a avaliação necessária. Ademais, o TerraLAB também proporcionou aos estudantes o reconhecimento de que ser autodidata é necessário e, ao mesmo tempo, aperfeiçoou suas habilidades sociais, ao colocar as equipes para trabalharem juntas com foco num mesmo objetivo.

Agora que o processo chega ao fim, resta uma dúvida: O que esperar agora em diante? Questionamos os participantes sobre suas expectativas para a vida profissional agora que experimentaram um pouco do TerraLAB, e como acham que o projeto os ajudará nesse quesito. Veja algumas respostas:

“A estrutura que o Lab possui é algo bem diferente de tudo que existe dentro da UFOP. A proximidade do Lab com empresas e como a estrutura do Lab funciona (igual as principais empresas do mercado de TI) faz com que as pessoas que participam do projeto já saibam como o mundo fora da universidade funciona. Isso ajuda a ter uma maior facilidade de ingresso no mercado.”  — Os depoimentos dos participantes nos dão contexto para entender como o processo seletivo pode redefinir as expectativas de como é trabalhar numa empresa de tecnologia moderna;

“O Lab mostra a realidade de um projeto real e nos capacita para trabalhar neles.”  — Nos elucidam quanto ao quê os trainees consideram ser frutos do processo;

“Através das parcerias formadas – com empresas da área de tecnologia –  e, acima de tudo, devido ao laboratório utilizar toda a tecnologia para qualificação de contratações de recém formados que as empresas gastam tempo e dinheiro para desenvolver.” — Sobre o quê é visto como um diferencial do projeto TerraLab por aqueles que estão chegando agora; 

“Com o conhecimento adquirido enquanto participamos do projeto, já dá pra notar que iremos aprender muito.” — E suas novas expectativas para o futuro dentro do TerraLab.

Por tudo que foi mencionado, é perceptível que o processo seletivo do TerraLAB foi de grande importância para os participantes e o laboratório como um todo. Após todas essas semanas, finaliza-se a seleção e se inserem no projeto novas pessoas trazendo novas ideias, e buscando sempre manter o alto nível das atividades desenvolvidas dentro do lab. 

Assim, o lab inicia mais uma jornada para capacitar mais pessoas para o mercado de trabalho de Tecnologia da Informação, uma das áreas mais cresce e e carece de bons profissionais.  Você sabe como a escassez de recursos humanos tem impactado negativamente as empresas de software? Você sabia que uma parceria conosco tem auxiliado grandes empresas  a reduzir esses impactos? Leia mais sobre estas duas questões neste artigo escrito pelo time TerraLAB.

Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab! 

Artigo escrito por Luka Menin e Vinícius Alvarenga. Revisado pelo Prof. Tiago Carneiro."
Resultados do TerraLAB no mês de Julho,http://www2.decom.ufop.br/terralab/resultados-do-terralab-no-mes-de-julho/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/08/Arte-em-acao-14.png,"Esta é uma sequência de artigos que tem como objetivo mostrar a evolução dos integrantes do TerraLAB ao longo de cada mês. No artigo anterior mostramos os três projetos que estão em desenvolvimento no TerraLAB envolvendo as nossas equipes de UX, Engenharia, Análise de Dados, Infraestrutura, Gestão de Projetos e Marketing, são eles: o Serviço de Geocodificação em Massa (SGM), o aplicativo Segurança da Mulher e o BatCaverna. 

SGM:  Na área de logística e de pesquisa de mercado, a geocodificação criou certas facilidades, que até então não existiam, e proporcionou uma transformação nos métodos de planejamento estratégico, organizacional e distributivo de mercadorias. Um serviço de geocodificação em massa na nuvem, deve ser desenvolvido pela grande aplicabilidade em diversas empresas de diferentes ramos, pois garante perspectiva de crescimento logístico, financeiro e tecnológico. Além disso, na grande maioria das vezes, dados sobre os sistemas sociais são vinculados a um endereço de logradouro ao invés de a uma coordenada geográfica. Este é comumente os casos dos eventos de saúde ou criminais. Consequentemente, o serviço de geocodificação é útil para a espacialização destes eventos e para o entendimento das maioria dos sistemas sociais. Com essas informações o projeto AWS SGM: Crawler de Geocodificação, busca trabalhar no problema: Como conhecer a localização de um logradouro, sem ir a campo?
Segurança da Mulher: O aplicativo tem como objetivo apontar o nível de segurança, com relação à mulher, de diversos locais da cidade com base em denúncias anônimas feitas pelas usuárias sobre qualquer tipo de importunação sexual. Essas denúncias serão exibidas em um mapa, bem como o nível de segurança dos locais, para que qualquer pessoa possa consultá-las. O aplicativo também conta com um espaço para que o importunador tenha direito de resposta e para que os representantes dos locais denunciados exponham o seu lado da história. Ao mostrar um mapa detalhado com indicadores de zonas seguras e de perigo, espera-se poder contribuir para a livre circulação com segurança das usuárias do aplicativo.
BatCaverna: O aplicativo tem como objetivo fazer a coleta detalhada do patrimônio espeleológico da área delimitada no estado de Minas Gerais e busca contribuir para o incremento do banco de dados das cavernas presentes. A principal ferramenta será a de cadastro de pontos de controle ou de feições com as suas respectivas descrições e imagens associadas. Aliado a esse cadastro de pontos, o aplicativo irá coletar os caminhos feitos pelo o usuário e, durante esse caminho, ele terá a liberdade de criar quantos pontos forem necessários. Outra funcionalidade importante que está atribuída a esse aplicativo é a de visualizar os caminhamentos, tanto de uma forma individual, clicando sobre o caminho realizado, quanto de forma geral que será exibida em seu menu principal. O sistema também contará com um gerador de relatórios baseado nos pontos e fotografias tiradas em campo.

Na última semana do mês de julho, ocorreu a validação mensal do trabalho realizado pelas equipes do TerraLAB, acompanhe nos vídeos abaixo o que foi produzido e quais são as principais ferramentas e tecnologias que estão sendo utilizadas e estudadas nos projetos do lab.

User Experience

No vídeo a seguir, o gerente de UX, João Pedro Siqueira nos mostra o que o time realizou no mês de julho, como a criação de telas de alta fidelidade, os fluxos das telas, o guia de estilo e algumas pesquisas internas realizadas pela equipe.

Engenharia

O nosso gerente de engenharia, Vinícius de Paula, mostra no vídeo a seguir o Code Review aplicado nos times de desenvolvimento. Time de inteligência geográfica: Utilizando kubernets e Kong API Gateway para instanciar e atualizar as APIs de geocodificação, realização de testes de stress em cada API com a ferramenta jmeter. E, por fim, fala um pouco sobre os projetos: Serviço de Geocodificação em Massa, framework e componentização, BatCaverna.

Data Analytics

No próximo vídeo o nosso gerente de análise de dados, Diego Matos, faz uma breve apresentação de algumas funcionalidades desenvolvidas pela sua equipe dentro do projeto SGM. Já é possível validar se um ponto de endereço está dentro da malha da cidade, através da malha do IBGE, e visualizar o grau de confiança de um endereço no mapa, utilizando da dispersão espacial dos pontos de diferentes APIs de geocodificação. 

Infraestrutura

Agora o Guilherme Carolino, gerente de infraestrutura do TerraLAB, nos mostra a arquitetura da implementação do projeto SGM na nuvem AWS, utilizando o framework serverless (AWS Lmabda) e sistema de mensageria (AWS MQS).

Gestão de projetos

Quanto à gerência de projetos, o vídeo conduzido por Emanuel Xavier e Bernardo Santos procura compreender os deveres de um Gerente de Projetos, aplicando os pilares conceituais nos processos mensais do Laboratório. Tendo isso em base, os gerentes analisam as falhas e conquistas do Processo Seletivo, sugerindo novas possibilidades e metodologias em sua execução, além de comentar as medidas e ações tomadas em relação a execução dos projetos em desenvolvimento dentro da instituição. Por fim, detalham a implantação da nova plataforma de gestão denominada Jira e também falam do que está por vir dentro do Lab.

Marketing

O marketing tem como objetivo divulgar todo esse trabalho realizado pela equipe do TerraLAB. No blog do laboratório ocorre a publicação semanal de artigos de alta qualidade, escritos pelos próprios estudantes em sua maioria. Produtos, eventos e parcerias também são publicados em diversas redes sociais. Dados destas plataformas mostram que temos crescido enquanto marca e atraído o nosso público alvo. Vale ressaltar que se estivéssemos investindo no Google Ads para ter o mesmo número de acessos que temos em nosso blog, isto nos custaria cerca de 13 mil reais mensais. Os nossos artigos mais visualizados e as publicações que mais nos trouxeram resultados também são mostrados no vídeo a seguir pela nossa gerente de marketing, Paloma Bento.

Nós estamos há três meses trabalhando nos projetos aqui citados, caso você represente uma empresa, gostaríamos do seu feedback para que possamos melhorar continuamente o nosso trabalho. 

Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab! 
Você sabe como a escassez de recursos humanos tem impactado negativamente as empresas de software? Você sabia que uma parceria conosco lhe ajudaria a reduzir esses impactos? Leia mais sobre estas duas questões neste artigo escrito pelo time TerraLAB.

Artigo escrito por Paloma Bento. Revisado por Prof. Tiago Carneiro."
Como a escassez de mão de obra qualificada afeta as empresas de TI,http://www2.decom.ufop.br/terralab/como-a-escassez-de-mao-de-obra-qualificada-afeta-as-empresas-de-ti/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/07/Arte-em-acao-13.png,"Este artigo traz um resumo sobre a problemática gerada pela alta demanda por recursos humanos na indústria de Tecnologia da Informação (TI), discutindo a escassez de  mão de obra qualificada no mercado brasileiro e a alta rotatividade de profissionais percebida pelas empresas de software, agravada pelo acelerado crescimento do número de startups criadas nos recentes anos. 

Para caracterizar quantitativamente estes problemas e seus impactos negativos, o artigo apresenta estatísticas levantadas por consultorias e empresas especializadas no tema, assim como por associações representativas da indústria de TI. 

Finalmente, este artigo apresenta uma solução viável para resolver os problemas citados ou, pelo menos, reduzir seus impactos negativos.

O desafio da indústria de TI no Brasil

Apesar da alta oferta de vagas, atualmente, o setor de TI sofre com a falta de mão de obra especializada. A qualificação dos recursos humanos de uma empresa é um dos principais fatores que impactam no seu sucesso ou fracasso. Segundo a empresa de recursos humanos Korn Ferry,  a falta de capacitação dos profissionais de TI terá impacto significativo na produtividade das empresas, além de diminuir consideravelmente o tempo do trabalhador no mesmo emprego.  Em 2019, o Brasil perdeu duas posições no Global Competitiveness Index (GCI), divulgado pelo Fórum Econômico Mundial, caindo do 94º para o 96º lugar dentre os 141 países avaliados. O cenário do país piora se analisarmos o quesito facilidade de encontrar profissionais qualificados, ficando na 129º posição. Ficamos ainda abaixo da média da região da América Latina e Caribe, como mostra o gráfico:


Fonte: The World Bank – GovData360

Jornais e Revistas destacam o problema que as empresas de TI enfrentam na busca por profissionais qualificados. Além da escassez da mão de obra, o setor corre o risco de sofrer um apagão e dificultar o crescimento do país.

Fonte: Exame
Fonte: Exame

A escassez e profissionais de TI em números

O gargalo produtivo, ou ponto de estrangulamento, é utilizado no jargão administrativo para descrever o processo limitador de crescimento da capacidade produtiva da empresa. 

Uma das pautas de estrangulamento significativo encontrada na indústria brasileira reside na falta de mão de obra qualificada. Segundo estudo da Confederação Nacional da Indústria (CNI), em 2020, 50% das indústrias afirmaram enfrentar esse problema (SondEsp 76). 

Segundo a Brasscom, Associação Brasileira das Empresas de Tecnologia da Informação e Comunicação, o mercado brasileiro de TIC (Tecnologia da Informação e Comunicação), que atualmente é o 7º no mundo, atrás de Estados Unidos, China, Japão, Reino Unido, Alemanha e França, aponta que o setor deve gerar 420 mil vagas no Brasil até 2024. No entanto, mesmo diante da enorme oferta de vagas, a Brasscom considera que o déficit profissionais de TI no mercado de trabalho chegará a 264 mil, em agosto de 2020, e que o Brasil apresenta um déficit anual de 24 mil profissionais na área de TI.

A Associação Brasileira de Startups também considera a oferta de desenvolvedores de software (Dev) insuficiente. Segundo dados do IDC Brasil, o país tem cerca de 150 mil a 200 mil vagas sem candidatos no setor de tecnologia, em um país com grandes índices de desemprego, cerca de 13 milhões de desempregados, a contrariedade centra-se na formação deficiente dos profissionais.

Para agravar a escassez de profissionais qualificados, há um acelerado surgimento de startups e alta rotatividade de profissionais nas empresas de TI. Segundo a consultoria Accenture, há um surgimento marcante de startups brasileiras, 12.700 em 2019, o triplo de 2016, e uma acelerada expansão da economia digital, que já corresponde a 24,3% do PIB brasileiro. Estatísticas do Linkedin mostram que este é o mercado com maior rotatividade de profissionais (13,2%). 

Mão de obra qualificada gera mais produtividade e menos custos

A capacitação da mão de obra na área de TI confere todo o portfólio de conhecimento técnico e normativo para que o colaborador possa desempenhar as suas funções de forma produtiva e com qualidade apurada, além de prolongar a permanência deste dentro da empresa ao ver suas habilidades sendo valorizadas. 

A falta dessa qualificação acarreta prejuízos financeiros às empresas de TI, afinal há uma elevação de custos por causa de retrabalhos, baixa produtividade e desperdício de tempo no desenvolvimento.  

Além disso, a problemática da mão de obra inadequada faz com que muitas empresas reduzam as suas exigências, consideradas básicas para exercer plenamente as funções necessárias para a corporação. Ao mesmo tempo, o custo para manter uma indústria funcionando aumenta, pois torna-se necessário oferecer inúmeros benefícios como atrativo e incentivo, além de em alguns casos, optarem por capacitar os profissionais posteriormente à contratação para conseguir preencher as suas vagas disponíveis. Segundo estudos da Confederação Nacional da Indústria (CNI)(2017), 81% das organizações brasileiras utilizam essa estratégia.

O futuro: Parcerias entre empresas e universidade 

Uma das soluções para reduzir os impactos negativos destas ameaças ao crescimento da indústria brasileira de software tem sido o estabelecimento de parcerias entre empresas e instituições de ensino. Dos anos 2000 em diante, vários avanços promovem essa parceria, desde a criação dos Fundos Setoriais, até a aprovação das Leis da Inovação e do Bem e a recente revisão da legislação, consolidando o Marco Legal de Ciência, Tecnologia e Inovação (Lei nº 13.243/2016).

Há claros estímulos à constituição de alianças estratégicas e o desenvolvimento de projetos de cooperação que envolvam empresas, instituições de ciência e tecnologia (ICTs) e entidades privadas sem fins lucrativos. Entre este estímulos, destacamos: 

Facilidades para a transferência de tecnologia de ICT pública para o setor privado.
Os direitos de propriedade intelectual podem ser negociados e transferidos da instituição de ciência e tecnologia (ICT) para os parceiros privados, nos projetos de cooperação para a geração de produtos inovadores.
Autorização para a administração pública direta, as agências de fomento e as ICTs apoiarem a criação, a implantação e a consolidação de ambientes promotores da inovação.
Documentação exigida para contratação de produto para pesquisa e desenvolvimento poderá ser dispensada, no todo ou em parte, desde que para pronta entrega ou até o valor de R$ 80 mil.
O aperfeiçoamento de instrumentos para estímulo à inovação nas empresas, como a permissão de uso de despesas de capital na subvenção econômica, regulamentação de encomenda tecnológica e criação de bônus tecnológico.
Regulamentação dos instrumentos jurídicos de parcerias para a pesquisa, o desenvolvimento e a inovação: termo de outorga, acordo de parceria para pesquisa, desenvolvimento e inovação, convênio para pesquisa, desenvolvimento e inovação.
Prestação de contas simplificada, privilegiando os resultados obtidos nos acordos de parceria e convênios para pesquisa, desenvolvimento e inovação.
Considerações Finais

Este artigo mostrou que mesmo diante a uma enorme oferta de vagas no mercado de TI, muitas das vagas não são preenchidas devido a escassez de profissionais qualificados. O déficit de profissionais ameaça o crescimento desta indústria e seus impactos negativos vêm sendo noticiados pela mídia especializada, por consultorias e pelas associações de empresas. Da mesma forma, o avolumado crescimento de startups nos últimos anos e a alta rotatividade de profissionais agravam estas ameaças. 

Para evitar colapsos e reduzir impactos negativos, é preciso estreitar as parcerias entre universidades e empresas, investir na capacitação conjunta de recursos humanos como um caminho para aumentar os lucros e reduzir as perdas. Para isso, há claros incentivos para a construção conjunta de um ambiente de inovação aberta que é fortalecido pelo Marco Legal de Ciência, Tecnologia e Inovação.

Diante deste cenário, o TerraLAB oferece capacitação e recrutamento de profissionais de TI para empresas, visando a redução de custo e de tempo consumidos na seleção e treinamento de colaboradores. Formamos profissionais com vivência nos diversos papéis existentes no ecossistema de desenvolvimento e operação de software, entre eles:  Gerente de Projeto, Product Owner (PO), Designer de Experiência do Usuário (UX), Desenvolvedor de Aplicativos WEB, Desenvolvedor de Backend, Desenvolvedor de Aplicativos Móveis, Engenheiro de Qualidade (Testers), Arquitetos de Infraestrutura,  Cientista de Dados, Analista de Marketing e Analista Jurídico. 

Nossos colaboradores também ganham experiência nos mais modernos processos e ferramentas por meio das atividades práticas que desempenham em projetos reais e simulados. Nós adotamos as metodologias ágeis e mantemos nosso o foco na produtividade de nossos times e na qualidade dos produtos e serviços computacionais que desenvolvemos. 

Se você representa uma empresa, entre em conosco e conheça mais sobre os benefícios mútuos de uma parceria. Conheça nossos atuais parceiros! Para saber mais sobre esses assuntos, acesse a nossa página e veja os resultados atingidos em nossas redes sociais. 

Artigo escrito por Gustavo Moreira. Revisado por Prof. Tiago Carneiro."
Como realizar o empacotamento de scripts Python com o NodeJS,http://www2.decom.ufop.br/terralab/como-realizar-o-empacotamento-de-scripts-python-com-o-nodejs/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/07/Arte-em-acao-11.png,"Caso você tenha familiaridade com o software Node.js, você deve saber das facilidades que ele oferece para o desenvolvimento de serviços WEB. Além de ser uma plataforma considerada leve e de código aberto, é um interpretador de Javascript no lado do servidor, muito utilizado pelos desenvolvedores  comunicação e integração entre backend e frontend.

Por outro lado, Python é uma linguagem largamente utilizada pela comunidade de Ciência de Dados e de Visualização Científica. Esta plataforma de desenvolvimento oferece diversas bibliotecas para a análise e a visualização de dados. 

Neste artigo vamos aprender como realizar o empacotamento de scripts PyQGIS em um servidor Node.js. PyQGIS é uma biblioteca Python para a manipulação, análise e visualização de dados geoespaciais em diversos formatos. 

Para isso,  vamos desenvolver o servidor Node.js em um container docker, a fim de avaliá-lo em um ambiente similar ao ambiente de produção de um serviço WEB executando na nuvem. Caso ainda não tenha utilizado o docker, sugerimos a leitura da seção “Instalação do Docker” do artigo: “Entendendo o funcionamento do CICD dentro do Git flow”.

Antes de implementar nosso serviço geográfico, iremos conhecer alguns componentes, estruturas e softwares básicos que irão nos ajudar no processo de “mão no código”.

Python para geotecnologias: O que são QGIS e PyQGIS?

O QGIS é um Sistema de Informações Geográficas open source, que permite a visualização, análise e edição de dados geoespaciais. Ele possui uma interface de programação na linguagem Python que,  através da biblioteca  de funções chamada PyQGIS, permite que sejam escritos scripts para o desenvolvimento de plugins e para a automação de tarefas rotineiras.

O download e instalação do QGIS é simples e pode ser feita pelo link abaixo:

Link de instalação do QGIS 

Entendendo sobre o componente “multipart/form-data”

Executada sobre o ambiente Node.js, a estrutura “framework Express”, implementa uma rota raiz que responde a uma requisição POST com o tipo de conteúdo multipart/form-data que contém dois arquivos: um script PyQGIS e um arquivo shapefile. Este último arquivo armazena dados geoespaciais em formato vetorial. Estes dados serão utilizados pelo script PyQGIS. 

No backend, estes dois arquivos serão tratados pelo middleware multer que é capaz de manipular formulários do tipo multipart/form-data. Middleware é uma função com acesso a qualquer requisição recebida por um servidor, ela é executada entre o momento em que o servidor recebe a requisição HTTP e o momento em que o servidor responde.  No nosso estudo de caso, o multer é utilizado para salvar os dois arquivos recebidos em uma pasta de nome “tmp” . Após o tratamento realizado pelo multer, precisamos executar o script PyQGIS recebido. Utilizamos o método spawn do módulo child_process, que vem no Node.js por padrão, para executar um bash script que exporta as variáveis de ambiente do QGIS e executar o script PyQGIS. O módulo child_process fornece métodos para manipulação de processos filhos no sistema operacional da máquina em que é executado. O método spawn executa um processo e nos permite escutar os streams de dados: stdout e stderr. No nosso estudo de caso, o script PyQGIS lê o arquivo shapefile e gera um arquivo do tipo geojson como saída. Então, após sua execução, apenas retornamos o arquivo geojson para o cliente. Em outras palavras, nosso serviço geográfico apenas converte o arquivo shapefile recebido de um cliente HTTP para o formato geojson.

Mão no código

Na pasta src/config em nosso projeto, criamos um arquivo com o nome multer.js, onde configuramos o multer. Através da função diskStorage do multer, passamos como argumento uma função que determina onde os arquivos recebidos serão armazenados.  Também passamos como argumento outra função que define qual será o nome dos arquivos quando armazenados. E por fim criamos uma função com o nome fileFilter para permitir o recebimento somente de arquivos com a extensão .py e .shp.

const multer = require('multer');
const path = require('path');

module.exports = {
   storage: multer.diskStorage({
       destination: (req, file, cb) => {
           cb(null, path.resolve(__dirname, '..', '..', 'tmp'));
       },
       filename: (req, file, cb) => {
           const scriptName = 'script-qgis.py';

           if(path.extname(file.originalname) == '.py'){
               cb(null, scriptName)
           }
           else if(path.extname(file.originalname) == '.shp'){
               cb(null, file.originalname)
           }
       }
   }),
   fileFilter: (req, file, cb) =>{
       if(path.extname(file.originalname) == '.py' || path.extname(file.originalname) == '.shp'){
           cb(null, true);
       }
       else{
           cb(new Error('Invalid file type'));
       }
   }
};

Após a criação do arquivo de configuração do multer, criamos uma rota raiz  para responder a uma requisição do tipo POST e que define o multer como seu middleware. Após os arquivos serem processados pelo multer, a requisição é tratada e respondida através da função abaixo. Para criar um processo que executa um shell script, o método spawn do módulo child_process é utilizado, onde passamos como argumento o script que necessitamos executar.  Ao final da execução do script o arquivo gerado é retornado ao cliente da requisição.

routes.post('/', multer(multerConfig).array('file', 2), (req, res) => {
   var spawn = require('child_process').spawn;
   var batch = spawn('/app/launch.sh');

   batch.stdout.on('data', function (data) {
       console.log('stdout: ' + data);
   });

   batch.stderr.on('data', function (data) {
       console.log('stderr: ' + data);
   });

   batch.on('exit', function (code) {
       console.log('child process exited with code ' + code);

       res.download('tmp/rj_state_geometries.geojson');
   });   
});

O arquivo shell script executado pelo método spawn precisa importar as variáveis de ambiente do QGIS e executar o script recebido através da requisição e processado pelo multer. 

export PYTHONPATH=/usr/share/qgis/python
export LD_LIBRARY_PATH=/usr/lib
python3 ""tmp/script-qgis.py""

Para fins de execução em nuvem, utilizamos o ambiente Docker, o qual precisa de um arquivo com o nome Dockerfile na raiz do projeto. Este arquivo define os comandos utilizados para a criação de uma imagem Docker. Neste arquivo definimos como imagem base a última versão do Node.js. Utilizamos a instrução RUN para executar alguns comandos a fim de instalar o QGIS e configurar o projeto e suas dependências. Utilizando a instrução EXPOSE, expomos a porta 3333, que será escutada pelo projeto. E por fim utilizamos a instrução CMD que executará o projeto.

FROM node:latest

WORKDIR /app
COPY . /app
RUN apt update
RUN apt -y install gnupg software-properties-common
RUN wget -qO - https://qgis.org/downloads/qgis-2020.gpg.key | gpg --no-default-keyring --keyring gnupg-ring:/etc/apt/trusted.gpg.d/qgis-archive.gpg --import
RUN chmod a+r /etc/apt/trusted.gpg.d/qgis-archive.gpg
RUN add-apt-repository ""deb https://qgis.org/ubuntu $(lsb_release -c -s) main""
RUN apt update
RUN apt -y install qgis qgis-plugin-grass
RUN npm install
EXPOSE 3333
CMD [""npm"", ""start""]

Considerações finais

Neste artigo, ilustramos de forma prática como podemos empacotar scripts PyQGIS em um servidor Node.js utilizando alguns componentes importantes mencionados acima. 

Uma aplicação muito interessante para esse conhecimento é quando, em ambientes de desenvolvimento Web ou Mobile, se deseja exibir visualizações de dados resultantes de alguma análise realizada por scripts Python. 

Assim, esperamos que este artigo seja útil a todos que estejam iniciando no desenvolvimento de software WEB ou Mobile voltados para aplicações em Ciência da Dados, Visualização Científica ou Geoprocessamento.  

Gostaria de aprender mais sobre outros tópicos na área da tecnologia? Acompanhe as nossas redes sociais e comente, compartilhe essa publicação e não deixe de curtir! 

Sabia que este artigo foi completamente desenvolvido por estudantes de graduação da UFOP? Se você representa uma empresa e deseja contribuir ou se tornar parceira do TerraLAB, entre em contato conosco e conheça tudo que esta parceria pode lhe acrescentar! "
Dicas sobre como fazer uma boa apresentação,http://www2.decom.ufop.br/terralab/dicas-sobre-como-fazer-uma-boa-apresentacao/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/07/Arte-em-acao-9.png,"Tanto na indústria quanto na academia, possuir somente habilidades técnicas não é o suficiente. Tanto quanto fazer um bom trabalho, é importante saber apresentar o seu trabalho. A seguir, apresento dicas importantes para uma boa apresentação.

1 – Conecte-se com a sua audiência

Para fazer uma boa apresentação é fundamental conhecer a sua audiência. Qual o nível técnico da sua audiência? Quanto a sua audiência já sabe sobre o assunto que você está apresentando? Sem ter clareza das respostas para estas perguntas é impossível fazer uma apresentação efetiva. Por isso é importante sempre adequar o conteúdo da sua apresentação à sua audiência.


Photo by takje from FreeImages

2 – Foque no que a audiência precisa

Sua apresentação precisa ser construída em torno do que o seu público vai tirar dela. Foque no que a audiência quer e no que ela precisa saber, não no que você quer contar.

3 – Keep it simple

Defina a mensagem que você quer passar com a apresentação e não desvie dela. Se o que você está planejando dizer não contribui diretamente com a mensagem principal, não diga.

4 – Conte uma estória

A sua apresentação deve funcionar como uma estória. Comece contextualizando os assunto, apresente as personagens (pessoas envolvidas, artefatos construídos, hipóteses), desenvolva estas personagens, crie um clímax e apresente o desfecho das personagens. 

5 – Conclua

No fim da apresentação, exponha um resumo dos principais pontos e uma conclusão dos principais resultados atingidos.

6 – Minimize o texto

Uma apresentação não é o formato correto para a leitura. Você quer que a audiência preste atenção no que você está falando e não no que está escrito no slide. Sempre dê preferência para imagens e use-as para ilustrar o que você planeja falar. Um slide não deve ter muito mais do que 12 palavras.   

7 – Use a regra 10-20-30

– Mantenha o número de slides por volta de 10 ou menos;

– Mantenha o tempo de apresentação por volta de 20 minutos ou menos;

– Use fonte de tamanho 30, no MÍNIMO. Esta regra te protege de colocar texto demais nos slides, um erro fatal.

O vídeo abaixo é um bom exemplo de como fazer uma ótima apresentação. Note como logo no início a apresentadora já se conecta com a audiência. Nos primeiros segundos de apresentação ela já define, em linguagem acessível, o tema e mostra como os conceitos que serão explicados são vistos cotidianamente.

Obviamente, o vídeo acima teve uma equipe de produção profissional e não tem o padrão das nossas apresentações do dia a dia. Mesmo assim, a forma com a apresentação foi feita pode servir de inspiração. 

8 – Divirta-se e aproveite 

A apresentação é uma grande chance de mostrar os frutos de seu trabalho duro e colher opiniões para futuras melhorias. Aproveite esta oportunidade.

Artigo escrito por Prof. Rodrigo Cesar Pedrosa Silva.

Gostou deste artigo? Deixe-nos um comentário! Quer saber mais sobre o TerraLAB? Visite o nosso site e siga-nos nas redes sociais para ficar por dentro de todas as dicas, notícias, artigos e tutoriais do lab."
Resultados do TerraLAB no mês de Junho,http://www2.decom.ufop.br/terralab/resultados-do-terralab-no-mes-de-junho/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/07/Arte-em-acao-8.png,"Atualmente, três projetos estão em desenvolvimento no TerraLAB, envolvendo as nossas equipes de UX, Engenharia, Análise de Dados, Infraestrutura, Gestão de Projetos e Marketing, são eles: o Serviço de Geocodificação em Massa (SGM), o aplicativo Segurança da Mulher e o BatCaverna. 

SGM:  Na área de logística e de pesquisa de mercado, a geocodificação criou certas facilidades, que até então não existiam, e proporcionou uma transformação nos métodos de planejamento estratégico, organizacional e distributivo de mercadorias. Um serviço de geocodificação em massa na nuvem, deve ser desenvolvido pela grande aplicabilidade em diversas empresas de diferentes ramos, pois garante perspectiva de crescimento logístico, financeiro e tecnológico. Além disso, na grande maioria das vezes, dados sobre os sistemas sociais são vinculados a um endereço de logradouro ao invés de a uma coordenada geográfica. Este é comumente os casos dos eventos de saúde ou criminais. Consequentemente, o serviço de geocodificação é útil para a espacialização destes eventos e para o entendimento das maioria dos sistemas sociais. Com essas informações o projeto AWS SGM: Crawler de Geocodificação, busca trabalhar no problema: Como conhecer a localização de um logradouro, sem ir a campo?
Segurança da Mulher: O aplicativo tem como objetivo apontar o nível de segurança, com relação à mulher, de diversos locais na cidade com base em denúncias feitas pelas usuárias.
BatCaverna: O aplicativo tem como objetivo fazer a coleta detalhada do patrimônio espeleológico da área delimitada no estado de Minas Gerais e busca contribuir para o incremento do banco de dados das cavernas presentes.

Na última semana do mês de junho ocorreu a validação mensal do trabalho realizado pelas equipes do TerraLAB, acompanhe nos vídeos abaixo o que foi produzido e quais são as principais ferramentas e tecnologias que estão sendo utilizadas e estudadas nos projetos do lab.

User Experience

No vídeo a seguir, o gerente de UX, João Pedro Siqueira, nos mostra como são as demandas de sua área no lab, exibe um relatório analisando algumas possíveis soluções para a resolução do problema demandado, análise de APIs de serviços geográficos na WEB e, por fim, mostra a renderização do aplicativo BatCaverna, entre outras coisas.

Engenharia

O nosso gerente de engenharia, Vinícius de Paula, mostra no vídeo a seguir a definição e manuseio das APIs de geocodificação utilizadas dentro do lab. Apresenta os artigos produzidos pela equipe de desenvolvimento e o vídeo de lançamento do aplicativo Segurança da Mulher.

Data Analytics

No próximo vídeo o nosso gerente de análise de dados, Diego Matos, faz uma breve apresentação dos artefatos gerados e descobertos pela equipe, passando pela arquitetura do atual projeto, pelas decisões tomadas em atividades exploratórias utilizando-se do google colab e pelo novo backlog da equipe.

Infraestrutura

Agora o Guilherme Carolino, gerente de infraestrutura do TerraLAB nos mostra a implementação de um projeto teste baseado na arquitetura serverless que simula a nova arquitetura de nuvem do Terralab.

Gestão de Projetos

As principais demandas da gerência de projetos desse mês foram remanejar as equipes levando em consideração as atividades que cada um iria fazer e estudar o Jira para melhorar a coleta de métricas, mostra Emanuel, gerente de projetos.

Marketing

O marketing tem como objetivo divulgar todo esse trabalho realizado pela equipe do TerraLAB, com publicações semanais de artigos, de alta qualidade e escritos pelos próprios estudantes em sua maioria, em nosso blog e divulgação dos produtos e parcerias também nas redes sociais, temos crescido enquanto marca e atraído o nosso público alvo. Vale ressaltar que se estivéssemos investindo no Google Ads para ter o mesmo número de acessos que temos em nosso blog, seria cerca de 13 mil reais mensais. Os nossos artigos mais visualizados e as publicações que mais nos trouxeram resultados são mostrados no vídeo a seguir pela nossa gerente de marketing, Paloma Bento.

Gostaríamos de agradecer carinhosamente às empresas parceiras que acreditam TerraLAB e vêm nos apoiando: Gerencianet, GS Ciência do Consumo, Memory, Stilingue e Usemobile.

Nós estamos há dois meses trabalhando nos projetos aqui citados, caso você represente uma empresa, gostaríamos do seu feedback para que possamos melhorar continuamente o nosso trabalho. Estamos de braços abertos para novas parcerias e sugestões, então deixe um comentário, visite o nosso site, envie-nos um e-mail e não deixe de nos seguir nas redes sociais para ficar por dentro de tudo o que acontece no lab!

Artigo escrito por Paloma Bento. Revisado por Prof. Tiago Carneiro."
Tipos de aprendizado de máquina e algumas aplicações,http://www2.decom.ufop.br/terralab/tipos-de-aprendizado-de-maquina-e-algumas-aplicacoes/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/06/Arte-em-acao-6.png,"Estamos em 2021 e quando falamos em tendências de mercado podemos incluir o aprendizado de máquina no topo da lista, segundo uma pesquisa feita pela Glassdoor, o cargo de Cientista de Dados é o segundo colocado da lista de melhores empregos na América para 2021, além disso, em pesquisa feita pela Algorithmia, 76% das empresas priorizam a inteligência artificial e o aprendizado de máquina nos orçamentos de TI de 2021. Parece atrativo, não é?

Porém, não é de hoje que essa subárea da inteligência artificial está em alta, ao longo dos anos ela vem mostrando sua importância na medida em que possibilita realizar tarefas árduas e/ou impossíveis para o ser humano além de contribuir no aprimoramento das mais diversas técnicas e processos existentes.

Mesmo tendo grande influência na vida da maior parte das pessoas, pouquíssimas dessas sabem de fato do que se trata, como ela está inserida no nosso dia a dia e o quão poderosa é. 

Mas se você é uma das pessoas interessadas em descobrir sobre essa área, leia esse artigo até o final porque nele você vai descobrir o que é o aprendizado de máquina, métodos de aprendizado, aplicações e tudo o que você precisa para conhecer um pouco mais desse mundo.

1. O que é o Aprendizado de Máquina? 

Em 1959, Arthur Lee Samuel, engenheiro do MIT definiu o aprendizado de máquina como “um campo de estudo que dá aos computadores a habilidade de aprender sem terem sido programados para tal”. Vamos entender o que isso quer dizer a seguir.

Quando estamos criando um modelo de aprendizado de máquina não informamos ao computador os passos a seguir para que ele aprenda o que precisa, isso porque o conhecimento é adquirido, no geral, a partir de modelos estatístico/matemáticos que reconhecem padrões em dados, criando a possibilidade que eles aprendam com seus erros e façam previsões em cima do que foi aprendido. Esses modelos podem ser definidos por diferentes formas de aprendizado e elas serão tratadas na seção a seguir.  

2. Tipos de Aprendizado

Cada um dos itens abaixo vai explicar detalhadamente as principais formas que uma máquina pode aprender, elas irão compor o ecossistema do aprendizado de máquina, de forma que seja possível resolver diferentes problemas baseado nas abordagens que mais se encaixem a elas. 

2.1. Aprendizado Supervisionado

O aprendizado supervisionado é um paradigma de aprendizado de máquina, que tem como objetivo adquirir informações de relacionamento entre entrada e saída de um sistema, baseado em um conjunto de amostras de treinamento. 

Um algoritmo de aprendizado supervisionado analisa os dados de treino e produz uma função inferida que será utilizada para mapear novos exemplos. Para deixar menos abstrato, vamos considerar um exemplo, a classificação de e-mails como spam.

Provavelmente, você utiliza e-mail e sabe que conteúdos maliciosos são quase sempre enviados para uma pasta específica, com o objetivo de te proteger. Mas como isso acontece? 

A experiência que permite você não precisar classificar quais e-mails são maliciosos ou não, é proporcionada por um modelo de classificação que é baseado em entradas rotuladas. Nessas entradas, possuímos e-mails classificados como confiáveis ou não, dessa forma, o modelo irá aprender a reconhecer a classe que um novo dado pertence baseado no que já aprendeu sobre esses dados rotulados.

Porém, pode te bater aquela curiosidade, por que então o meu provedor de e-mail ainda me pergunta se a mensagem que eu recebi é ou não um spam? Apesar de já existirem modelos confiáveis treinados em cima de enormes conjuntos de dados, a sua validação permite que esse modelo seja aprimorado cada vez mais, permitindo que essa não seja uma preocupação sua.

O exemplo acima demonstra como seria um problema de Classificação, mas vale lembrar que existe outra gama de problemas que podem ser denominados como problemas de Regressão. Para que não sobrem dúvidas do que se trata cada um desses tipos de problemas, eles estão definidos abaixo:

Classificação 

A classificação é o processo de categorizar um determinado conjunto de dados em classes. No exemplo da classificação de e-mails como spam, teríamos um exemplo de classificação binária, no qual o modelo através dos dados fornecidos, precisaria gerar como resposta se o e-mail é spam ou não.

Alguns dos algoritmos mais famosos são:

KNN
Naive Bayes
Logistic Regression
Support Vector Machines
Decision Trees

Regressão

Os modelos de regressão são utilizados quando queremos prever valores, por exemplo, prever o preço de uma casa ou o número de produtos que serão vendidos em determinado mês.

Os modelos de regressão são dos mais diversos e suas possibilidades são descritas pela imagem abaixo:


Imagem 1-Introdução a regressão e suas possibilidades. Link de acesso.

Analisando a imagem acima podemos perceber que a primeira subdivisão dos modelos de regressão diz respeito ao número de variáveis envolvidas, modelos de regressão simples envolvem apenas uma variável e os múltiplos duas ou mais. Em seguida, para cada um dos tipos descritos ainda existe outra ramificação que divide esses modelos em lineares ou não lineares. 

Alguns modelos são famosos para realizar regressão, são eles:

Linear Regression
Polynomial Regression
Logistic Regression
Principal Components Regression (PCR)
2.2. Aprendizado Não Supervisionado

O aprendizado não supervisionado consiste em treinar uma máquina a partir de dados que não estão rotulados e/ou classificados. Os algoritmos que fazem isso buscam descobrir padrões ocultos que agrupam as informações de acordo com semelhanças ou diferenças, por exemplo. 

Para que isso fique mais claro, vamos imaginar um algoritmo de aprendizado não supervisionado, que receba uma imagem contendo cachorros e gatos.

Ao receber essa imagem nada se sabe sobre as características que cada animal possui, ou seja, não é possível categorizá-los. Porém, esse algoritmo será responsável por descobrir semelhanças, padrões e/ou diferenças que permitam diferenciar cães e gatos.  

No exemplo citado anteriormente utilizamos uma técnica chamada de agrupamento (Clustering), porém existem outras técnicas como regras de associação (Association Rules) e redução de dimensionalidade (Dimensionality Reduction). Falaremos um pouco de cada uma delas abaixo.

Agrupamento

A técnica de agrupamento como explicado no exemplo anterior, consiste em agrupar dados não rotulados com base em suas semelhanças ou diferenças.  Esses algoritmos de agrupamento ainda podem ser subdivididos em agrupamentos exclusivos, sobrepostos, hierárquicos e probabilísticos.

Regras de Associação

Ao usar as regras de associação, buscamos descobrir relações que descrevem grandes porções dos dados. A associação é muito utilizada em análises de cestas de compras, no qual a empresa pode tentar entender relações de preferências de compras entre os produtos.

Quando falamos de algoritmos para gerar regras de associação os principais são: Apriori, Eclat e FP-Growth.

Redução de dimensionalidade

Existem casos nos quais ao estudar um conjunto de dados, podemos encontrar nele um grande número de recursos (dimensões). Por mais que existam situações onde isso é positivo, o excesso pode impactar o desempenho dos algoritmos causando, por exemplo, o overfitting. 

Utilizando a técnica de redução de dimensionalidade, será feita uma redução no número de recursos, de forma que torne-os gerenciáveis por parte do modelo, além de preservar a integridade dos dados.
E para executar essa tarefa existem algumas técnicas que podem ser utilizadas, como: Missing Values Ratio, Low Variance Filter, High Correlation Filter, Random Forests / Ensemble Trees, Principal Component Analysis (PCA), Backward Feature Elimination e Forward Feature Construction.

2.3. Aprendizado por reforço

Para entendermos melhor como funciona o aprendizado por reforço usaremos a seguinte imagem para ilustrar qual é o princípio.


Imagem 2- Modelo de aprendizado por reforço. Link de acesso.

O primeiro passo é definir os elementos presentes na imagem, o agente (Agent) é aquele que toma as decisões com base nas recompensas e punições, esse agente pode realizar uma ação (Action) que irá variar de acordo com o contexto. O ambiente (Environment) é o mundo físico ou virtual em que o agente opera, a recompensa (reward) é o feedback do ambiente baseado na ação tomada e o estado (state) é a situação atual do agente.

A imagem acima demonstra um exemplo de como o aprendizado por reforço pode ser utilizado. Nesse caso, o robô é o nosso agente e ele está situado no estado inicial do nosso ambiente, que é representado pelo “labirinto” que o robô terá de percorrer. Desta forma, o objetivo é chegar ao diamante evitando os obstáculos (fogueiras). 

Definido o objetivo, o robô deve buscar pelo melhor caminho possível para chegar até o diamante. Dessa forma, a cada ação do robô, ele poderá caminhar em uma determinada direção, caso ele escolha corretamente, ele irá inserir pesos diferentes, para diferentes respostas. Com isso, espera-se que ao final o robô consiga realizar seu objetivo de forma que obtenha a maior recompensa cumulativa.

3. Aplicações

Lembra quando eu mencionei no início do artigo que as diferentes abordagens do aprendizado de máquina contribuem na realização de tarefas árduas e melhoria de processos? Nessa seção iremos conhecer em quais cenários o machine learning está inserido.

Diagnósticos médicos

Na área médica as técnicas de machine learning são utilizadas para fazer o reconhecimento de doenças. Com o crescimento da tecnologia tem sido possível construir modelos 3D que podem prever a posição exata de lesões no cérebro, permitindo a detecção de tumores e outros diagnósticos relacionados muito mais fácil. 

Além disso, muito trabalho vem sendo feito com imagens como, por exemplo, o reconhecimento de padrões que identificam câncer de pulmão, de pele, dentre outros.

Detecção de fraudes online

Se considerarmos uma instituição financeira que lida com milhares de transações por dia, ela está sujeita a fraudes a todo momento e sabendo que avaliar toda essa quantidade de operações seria totalmente exaustivo e ineficiente, modelos de machine learning são criados para que possam ser detectadas anomalias nas transações. 

Para ficar mais claro vamos supor que uma pessoa tenha um cartão de crédito de um banco com limite de 2000 reais, porém, ela tem um histórico de uso mensal de no máximo 800 reais, se por acaso em um determinado dia houver uma compra no seu cartão no valor de 2000 reais, o modelo de detecção de fraudes irá perceber que essa compra não se encaixa no seu padrão e, com isso, o banco será notificado colocando a transação em espera.

Sistemas de recomendação

Presente nos mais diversos tipos de aplicações, os sistemas de recomendação tiram aquela velha necessidade de procurar tudo aquilo que desejamos. No sistema de varejo, por exemplo, se você tiver cadastro na plataforma de algum desses varejistas você terá um sistema de recomendações de produto ao seu dispor, ele cria essas recomendações baseado em compras anteriores, históricos de navegação, dentre outras informações complementares.

Dessa forma, quando você está com o carrinho de compras e percebe que esqueceu mais um item da compra que estava planejando, provavelmente ele estará em uma seção destinada a seus possíveis interesses.

Reconhecimento de fala

Provavelmente o exemplo mais famoso para o reconhecimento de fala são os assistentes de voz. Então, a Siri da Apple, Alexa da Amazon, Cortana da Microsoft, dentre outros assistentes de voz usam machine learning através de técnicas de processamento de linguagem natural (NLP) para reconhecerem a fala, posteriormente transformam essa fala em números para que possam formular uma resposta de acordo.

4. Conclusões

Neste artigo você deu os primeiros passos dentro de uma área gigante, passamos pelos conceitos iniciais, vimos as formas de aprendizado e alguns dos modelos mais famosos de machine learning, além de conhecer diversas aplicações. Dessa forma, agora você pode destinar seus estudos para a forma de aprendizado e aplicações que mais te chamaram atenção.

Portanto, se você quer saber mais sobre machine learning e outros tópicos quentes na área de tecnologia, recomendo acompanhar todas nossas redes sociais. Não se esqueça também de curtir e compartilhar, seu apoio é muito importante!



Artigo escrito por Lucas Natali Magalhães Silva. Revisado por Prof. Rodrigo César Pedrosa Silva."
Introdução à arquitetura serverless com Amazon Lambda,http://www2.decom.ufop.br/terralab/introducao-a-arquitetura-serverless-com-amazon-lambda/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/06/Blog.png,"Neste artigo iremos falar sobre a arquitetura serverless, cuja tradução literal seria arquitetura “sem servidor”. Para um introdução prática, apresentaremos uma visão básica do serviço serverless Amazon lambda, do serviço de armazenamento Amazon S3, e do Serverless framework,  seguida de um exemplo prático em Python utilizando todas essas ferramentas.

Amazon lambda

Podemos dizer que o AWS Lambda é um serviço de computação serverless. Serverless é uma maneira de descrever os serviços, práticas e estratégias que permitem desenvolver aplicações mais ágeis.

As tarefas de gerenciamento de infraestrutura são executadas pela provedora da infraestrutura, neste caso a própria Amazon. Desta forma, você pode se concentrar em  apenas escrever códigos do seu serviço, a partir disso foi criado o nome serverless, em que a ausência da preocupação com os servidores tornou-se uma realidade para os desenvolvedores.

A computação serverless tem algumas vantagens e desvantagens. As vantagens são escalabilidade, facilidade de uso, baixo custo e uma grande quantidade de linguagens de desenvolvimento suportadas. Algumas desvantagens estão relacionadas ao Debug e aos testes que ficam mais complicados. Além disso, processos lambda com longo tempo de execução podem resultar em altos custos.

Amazon S3

O Amazon S3 ou simple storage service, é um serviço de armazenamento em que você pode guardar ou fazer o download de qualquer quantidade de dados de qualquer lugar da web.

O S3 é um serviço simples de ser utilizado, escalável e o próprio provedor do serviço escala ele para você, altamente durável, um custo baixo de $ 0,023 usd por GB + solicitações PUT, COPY, POST, LIST (por 1.000 solicitações) e também possui  integração com a maioria dos outros serviços.

Serverless framework

O Serverless é um framework utilizado para construir aplicações com funções lambda, ele cria e gerencia os servidores necessários para executar suas funções e responder aos eventos desejados, com isso você pode se concentrar melhor na parte da programação da lógica de sua aplicação.

Para iniciar, é preciso instalar o Serverless na sua máquina. Neste tutorial, todo o processo é realizado no Linux. Por ser um framework criado em NodeJS, utilizaremos o gerenciador de pacotes npm para fazer a instalação. (Caso você não tenha  o NodeJS instalado, faça o download)

Abra seu terminal e digite o comando:

npm install -g serverless


Agora precisaremos configurar as credenciais da AWS no seu Serverless

Faça login na AWS Console
Pesquise pelo serviço IAM, que é a forma de controlar acessos aos recursos da AWS.

Vá em Usuários > Adicionar usuário

Coloque um nome para seu usuário e adicione o Acesso Programático, e vá para o próximo passo

Vá até Anexar políticas existentes e adicione o AdministratorAccess, depois vá para o próximo passo

Continue avançando até finalizar a criação do usuário

Sua chave será criada e conterá um ID de acesso e uma Chave Secreta, como mostra a imagem a seguir, e você irá utilizá-la no próximo passo

Voltando a seu terminal, digite o seguinte comando:

serverless config credentials -o --provider aws --key=<ID da chave de acesso> --secret <Chave de acesso secreta>


Agora, iremos gerar o seu primeiro template no Serverless para uma função em Python. No terminal, insira o comando a seguir, que contém qual tipo de template será usado e a pasta em que ele será armazenado (a pasta não pode existir previamente, ela será criada pelo próprio comando):

serverless create --template aws-python3 --path <nome da pasta>


Finalmente, será gerado um arquivo .yml e um .py. Então, poderemos prosseguir para o nosso exemplo prático.

Exemplo prático

Para demonstrar o uso dessas ferramentas, faremos um exemplo prático: Para ser mais simples, invocaremos a função lambda manualmente sem utilizar gatilhos por enquanto. Essa função será encarregada de criar um arquivo de texto escrito ‘Hello World’ e colocá-lo dentro do AWS S3 Bucket, que é o serviço de armazenamento de objetos/arquivos.  Esse é um exercício bem simples que serve apenas para mostrar como criar um certo fluxo de dados na arquitetura serverless.

No arquivo serverless.yml, iremos configurar:
Região (São Paulo)
Tamanho da memória da função (256 MB)
Permissões de usuário (Pegar e colocar objetos no S3)
Função
Recurso Bucket s3.

e dessa forma o arquivo serverless.yml ficará assim:

service: exampleserverless
 
frameworkVersion: '2'
 
provider:
  name: aws
  runtime: python3.8
  lambdaHashingVersion: 20201221
  region: sa-east-1
  memorySize: 256
  iamRoleStatements:
    - Effect: Allow
      Action:
        - s3:GetObject
        - s3:PutObject
      Resource:
        - ""arn:aws:s3:::*""
 
functions:
  hello:
    handler: handler.hello
 
resources:
  Resources:
    BucketUpload:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: exampleterralabserverless

Partindo agora para o arquivo handler.py, criaremos a nossa função:
Importe a biblioteca de gerenciamento da AWS chamada “boto3”
Dê os nomes necessários aos arquivos e pastas
Escreva em um arquivo temporário a frase ‘Hello World’
Insira o código para adicionar esse arquivo temporário no AWS S3

seguindo esses passos, o seu código deverá ficar dessa maneira:

import json
import boto3
 
def hello(event, context):
 
    #Definindo os nomes
    caminho_temporario = ""/tmp/arquivotemp.txt""
    arquivo_S3 = ""hello.txt""
    nome_S3 = ""exampleterralabserverless""
    txt_hello = "" Hello World!""
 
    #Criando o arquivo temporário
    arquivo = open(caminho_temporario, 'w')
    arquivo.write(txt_hello)
    arquivo.close()
 
    #Adicionando o arquivo no AWS S3
    s3 = boto3.client('s3')
    with open(caminho_temporario,'rb') as arq:
        s3.upload_fileobj(arq, nome_S3, arquivo_S3)
 
 
    body = {
        ""message"": ""Go Serverless v1.0! Your function executed successfully!"",
        ""input"": event
    }
 
    response = {
        ""statusCode"": 200,
        ""body"": json.dumps(body)
    }
 
    return response


Agora apenas resta realizar o deploy do Serverless e testar a função. Para realizar um deploy, volte a seu terminal e digite:

serverless deploy -v


Ao finalizar esse processo com sucesso, podemos invocar nossa função, também via terminal com o seguinte comando:

serverless invoke -f hello -l


E pronto, sua função foi executada com sucesso. E para conferirmos se deu certo, podemos ir ao console da AWS, e pesquisar pelo serviço S3.

Se seu processo estiver funcionado, o serviço AWS S3 deverá conter 2 buckets, sendo um o que estará armazenando o seu próprio código lambda feito no tutorial, e o outro será o que definimos como exampleterralabserverless, o qual armazena o arquivo hello.txt gerado pela função. Abra o exampleterralabserverless para conferir se nossa função executou corretamente, ou seja, deverá conter o arquivo hello.txt

Considerações finais

A arquitetura Serverless hospedará seus serviços e funções sem necessitar de configurações de hardware. A sua aplicação será executada e todas suas dependências já estarão instaladas de forma nativa. Com a ascensão do mundo da nuvem, esse tipo de arquitetura vem crescendo bastante e é muito recomendado para execuções rápidas de funções.

Apesar de ser muito recomendado para certos tipos de aplicação, é sempre bom lembrar que analisar o seu problema e a arquitetura do seu produto é essencial para tomar a melhor decisão para a infraestrutura de seu projeto.

Gostaria de aprender mais sobre infraestrutura na nuvem? Você atua profissionalmente na área? Tem algo a acrescentar nessa discussão? Alguma dúvida sobre aquilo que apresentamos? O seu comentário será muito bem-vindo! Comente, compartilhe esta publicação e não deixe de acompanhar nosso blog e redes sociais!

Referências

Serverless com IBM Cloud Functions: como funciona esse tal “Serverless”?

Serverless com IBM Cloud Functions: como funciona esse tal “Serverless”?

Serverless AWS

Computação sem servidor – Amazon Web Services

Serveless (Dicionário do programador)

Serverless // Dicionário do Programador

Código prático Rocket Seat

Serverless com NodeJS e AWS Lambda | Diego Fernandes

Artigo escrito por Guilherme Carolino e Guilherme Ferreira Rocha. Revisado por Prof. Tiago Carneiro."
Como escrever artigos para um Blog?,http://www2.decom.ufop.br/terralab/como-escrever-artigos-para-um-blog/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/06/Arte-em-acao-3.png,"Semanalmente, um artigo é publicado em nosso blog. Estes artigos abordam e divulgam, em linguagem simples e objetiva, os conhecimentos tecnológicos ou científicos que o TerraLAB produz ou utiliza em sua rotina. Estes artigos podem ser um tutorial, uma notícia ou um artigo. Você já parou pra pensar como estes artigos são feitos? Qual estrutura seguem? Como escrever um desses artigos e contribuir?

Esta semana, o TerraLAB responde a essas questões neste tutorial. Ele explica como escrever um artigo para nosso blog e descreve, em cada tópico, as instruções que alguém deve seguir para desenvolver seu artigo. Ele também oferece dicas rápidas que ajudam a memorizar estas instruções.

INSTRUÇÕES GERAIS

Seguem alguma instruções gerais sobre como ler este tutorial e de como escrever um artigo para nosso blog: 

Em cada tópico, você encontrará instruções como desenvolvê-lo da melhor forma, além de dicas rápidas para aproveitá-los melhor;
Ao final, você terá uma estrutura de artigo, sabendo o que deve conter em cada parte: introdução, desenvolvimento e conclusão. Como uma boa redação conta uma história, tem início meio e fim,  o seu artigo não precisa ser separado nesses três tópicos, mas o conteúdo escrito precisa contemplar todos esses tópicos.
A ideia é que a partir dessa estrutura, seja possível desenvolver um conteúdo que transmita o seu conhecimento;
Não se esqueça de que um bom conteúdo é aquele que está alinhado com o interesse e linguagem do seu público alvo (leitor) e com o objetivo do TerraLAB. O TerraLAB tem como público alvo: Estudantes, Pesquisadores, Profissionais de TI e Empresas. Tenha um deles em foco e converse com ele. Dificilmente, um artigo atingirá vários sem parecer desinteressante para pelo menos um. Aqui você encontra algumas dicas sobre o que olhar antes de começar, a escrever um artigo;
Sem ideias de conteúdo? Liste o que você mais gosta, aquilo que você conhece bem, os desafios que superou e mostre que você é capaz de ter ideias incríveis.
AUTORES (ordem alfabética)

Apresente os nomes dos autores em ordem alfabética: 

aaaaaaaaaaaaa
bbbbbbbbbbbb
ccccccccccccccc
PALAVRAS-CHAVE

Apresente as palavras chaves na ordem de importância: 

Escrita;
Artigos;
TerraLab;
Blog.
DESCRIÇÃO GERAL (máximo 220 caracteres)

Forneça um resumo do conteúdo de seu artigo, em no máximo 220 caracteres. Este limite é imposto por algumas redes sociais e deve ser respeitado para evitar que alguém, que conheça menos do assunto abordado, precise retrabalhar seu texto para resumi-lo por você.

Exemplo: Em meio ao isolamento social causado pela pandemia do Coronavírus, muitos trabalhadores estão mantendo suas atividades via home office. Logo, uma lista com apps e ferramentas úteis a essa atividade se faz valiosa nesse cenário. Destas, vale a pena destacar e mencionar: Trello, Evernote, Wunderlist, Google Meet, Github, Pomodoro, Notion, TeamViewer, Pocket e Forest.
Passo 1. Planejamento

Utilize esse espaço para documentar questões estratégicas do artigo..

Público alvo: 
Possível data de publicação: 
Referências: 
Tema: 
Pergunta que guiará a produção de conteúdo: 
Passo 2. Título do artigo

Insira aqui 3 a 5 ideias de título. Não se esqueça de incluir a palavra-chave principal do artigo no título! Este item garante que os sites de busca orientem pesquisas para o seu texto.

 Um título eficiente precisa ter:

Tamanho ideal: Não pode ser muito longo, mas simples e direto, que diga aquilo que é preciso ser dito;
Apontar utilidade: Crie uma frase que aponte a utilidade de seu texto para fornecer uma solução (Ex: “ Passo a passo para implantar a Análise SWOT em sua empresa”);
Aguçar a curiosidade do leitor: Você pode tentar despertar a curiosidade do leitor para que ele abra o link de seu artigo ( Ex:  “segredos de como aumentar a produtividade de seus negócios!”);
Utilizar perguntas: Usar perguntas como forma de indicar que seu texto irá respondê-las (Ex: “Como emitir notas fiscais eletrônicas?”);
Lista com números: Outro tipo de título eficiente é aquele que utiliza números para apontar uma lista    (Ex: “5 coisas que você precisa saber antes de viajar para o exterior”).
Passo 3. Introdução

Descreva aqui o problema que você quer solucionar e de que forma isso impacta o dia a dia das pessoas e das empresas. Sua introdução deve fazê-la da maneira mais atrativa possível, para garantir que o leitor continuará a leitura. Então contar uma história que possa despertar a identificação do leitor com o conteúdo, fazendo uma citação de uma fonte confiável e relevante sobre o assunto, além de usar dados estatísticos, pode fazer com que o leitor aprenda a realizar perguntas, em sua mente, que serão respondidas ao longo do artigo. Então entenda seu público alvo, escolha um assunto que você goste e utilize imagens criativas e didáticas como as abaixo:


Imagem 1- Tente encher os olhos do seu leitor com boas imagens, bem distribuídas e sugestivas no texto

De uma forma muito prática, após apresentar o problema que tratará e apresentar suas motivações que o levaram a isto, em pelo menos um parágrafo (no mínimo), inicie o próximo parágrafo enunciando claramente o objetivo do seu artigo. Por exemplo, “O intuito deste artigo é…” ou “Este artigo tem por objetivo…”. 

Passo 4. Desenvolvimento em tópicos

Pense em 3 ou mais formas de resolver esse problema e as escreva aqui resumidamente, na forma de itens (bullets). Esses itens serão os subtítulos do seu artigo. Este exercício lhe ajudará a organizar seu conhecimento na história mais objetiva possível. Após levantar esses itens, descreva, em tópicos, qual será o conteúdo dentro de cada um.

No mundo online, as pessoas costumam ser multitarefas. Ao mesmo tempo em que visitam seu site, conversam com os amigos em alguma rede social, respondem seus e-mails, procuram alguma informação em mecanismos de busca etc. Diante deste fato,  o que você pode fazer para tornar seu conteúdo mais legal?

Quebre o texto em parágrafos curtos (um parágrafo trata apenas de um único assunto);
Use subtítulos;
Use bullet points (como estes);
Use negrito, itálico, citações e outros estilos (mas cuidado para não poluir o texto);
Incorpore outras mídias (imagens, vídeos, áudios, tweets, posts no Facebook e Instagram);
Uma maneira simples de desenvolver tópicos é escrever artigos em formatos de lista. É um tipo de artigo que costuma gerar bastante resultado.

Tente adicionar um vídeo em seu artigo! Quando é possível, pode-se melhorar a absorção do conteúdo abordado e trazer outras visões e experiências que dificilmente poderiam ser expressas por meio textual .

Vídeo 1- Fizemos um vídeo para acompanhar este artigo e tornar mais intuitiva a absorção do seu conteúdo.

Passo 5. Considerações finais/Conclusão

Esse é o momento de encerrar o texto e recapitular as ideias apresentadas até então. Fale aqui o que o leitor deverá esperar ao aplicar o que foi passado nos tópicos anteriores. Após produzir o conteúdo, é preciso terminá-lo. Não é porque você se empenhou até agora que pode escrever qualquer coisa no final do seu texto, deixando aquela sensação de que faltou alguma coisa, como aquele filme ou livro que não conta bem o final da história.

Por isso, conclua bem o seu conteúdo. É bem comum utilizarmos um capítulo de “conclusão” para fechar o texto, amarrar bem as ideias e sugerir ações a partir dos aprendizados do conteúdo. A ideia é recapitular o que foi dito, reforçar o objetivo principal do artigo e se possível criar links para outros artigos do TerraLAB. Qual a oferta final do conteúdo? O que o leitor pode fazer em seguida? Se o seu artigo for sobre um assunto introdutório, indique algum conteúdo mais profundo. Se for um artigo avançado, indique alguma oferta de meio ou fundo de funil, como um estudo de caso ou conversa com um consultor, para trazer mais credibilidade ao seu trabalho. O mais importante: Divirta-se.

Passo 6. Resumo/Linha fina

Como você diria, em poucas palavras, do que se trata o artigo? O resumo é uma forma de complementar o título usando mais palavras. Dica: Esse resumo também pode ser usado para compor a meta descrição do post. Para isso, siga as instruções a seguir e ele será usado para descrever as peças gráficas publicadas nas redes sociais.

Forneça o resumo na forma de quatro parágrafos, em linguagem simples e objetiva, conforme sugerimos a seguir:

Parágrafo 1 – O que é o artigo; 
Parágrafo 2 – Detalhes do artigo; 
Parágrafo 3 – Relacionamentos e hashtags; e 
Parágrafo 4. Um convite pessoal (CTA – call to action) e link para o artigo no blog.
Considerações Finais

Para que os componentes do TerraLAB construam bons artigos, nossa equipe de marketing buscou informações para orientar e melhorar as técnicas de redação de nossos colaboradores. 

Nós acreditamos que este assunto pode ser de interesse geral, visto que escrever bem é uma necessidade profissional e académica. Por essa razão, tivemos a iniciativa de tornar públicas nossas diretivas de escrita. Esperamos que sejam úteis para aquelas pessoas que necessitem desenvolver textos de divulgação científica, tecnológica em quaisquer áreas do conhecimento. Além disso, acreditamos, que as ideias expostas também possam ajudar na redação de textos em geral.

Finalmente, acreditamos que um bom artigo resulta mais facilmente de assuntos que os autores gostem e que, principalmente, é essencial se expressar de maneira a cativar o interesse do leitor.

A equipe do TerraLAB também acredita que a leitura de assuntos diversos e de textos que utilizem diferentes estilos literários podem contribuir para aumentar a criatividade dos autores na construção de conteúdo, assim como ampliar as técnicas de redação e auxiliar a escolha de estilos de escrita.  

Esperamos que tenha gostado desse artigo. Avalie este conteúdo, entre em contato conosco e compartilhe nossos artigos, para que juntos consigamos disseminar conhecimento. Conheça nossas redes sociais!

Artigo escrito por Josemar Félix. Revisado por Prof. Tiago Carneiro."
Uma introdução ao Git e Gitflow,http://www2.decom.ufop.br/terralab/uma-introducao-ao-git-e-gitflow/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/06/Arte-em-acao-2.png,"Uma das principais ferramentas utilizadas pelos desenvolvedores hoje em dia, são os controladores de versão de código fonte, entre eles temos o Subversion, CVS, e o mais popular deles, o Git. Estas ferramentas permitem que todas as mudanças realizadas sobre o código fonte de um software sejam rastreáveis e armazenadas sem sobrescrever umas às outras. Desta forma, é possível recuperar versões do código e verificar os autores e motivações das mudanças realizadas.

O intuito deste artigo é demonstrar o uso da ferramenta Git na prática, por meio de exemplos, trabalhando localmente e remotamente, seguindo o fluxo de trabalho estabelecido pelo Gitflow.

Conceitos e funcionamento básico

Antes de seguirmos, é  preciso que alguns conceitos básicos sejam entendidos para compreendermos o funcionamento básico do Git. Um repositório de código é um diretório onde os arquivos de código fonte do seu projeto e o histórico completo das revisões ficam armazenados. Ele pode ficar remotamente em um projeto no Git ou localmente em sua máquina.  

Para que a ferramenta de versionamento Git seja eficaz no controle de versões de um projeto de software, ela precisa ser utilizada de uma maneira específica.  O Gitflow é apenas uma ideia abstrata de como o fluxo de trabalho de uma equipe de desenvolvimento de software deve acontecer através do Git. Ele dita que tipos de ramificações devem ser configuradas em um projeto de software, quando as ramificações devem acontecer e como fazer a mesclagem de código. A figura baixo traz um exemplo de um repositório Git seguindo o fluxo de trabalho Gitflow. Podemos enxergar o repositório de código como uma árvore de versões de código. Inicialmente, quando criamos um repositório, há somente um ramo (do inglês, branch), com o nome “master” que representa o ramo principal (versão v 0.0.0). Posteriormente a criação do repositório, seguindo o modelo de fluxo de trabalho Gitflow, devemos criar um ramo com o nome “develop”, a partir do branch master. É este ramo que receberá todas as modificações para correção ou evolução de funcionalidades já existentes, assim como para o desenvolvimento de novas funcionalidades. O ramo “staging” também é criado a partir do ramo “develop” e é utilizado para colocar em testes as modificações de código realizadas pelos   desenvolvedores. Somente quando as modificações são aprovadas em todas as rotinas de testes existentes, isto é, testes regressivos, a versão do ramo “staging” é mesclada à versão “master”, dando origem a uma versão evoluída do software (v 0.0.1). Este processo de verificação por testes regressivo confere qualidade ao software ao garantir que a nova versão do software passa em todos os testes da versão anterior e também em novos testes que verificam as modificações realizadas no código.

Idealmente, um desenvolvedor deve criar um ramo a partir do ramo “develop” para cada correção ou evolução do código que pretende realizar. Desta maneira, ele poderá trabalhar sem a interferência do trabalho realizado por outros desenvolvedores e sem interferir no trabalho dos demais. Aṕos certo período de desenvolvimento, quando um desenvolvedor julgar que seu trabalho está concluído,  ele deverá mesclar seu código com o ramo “staging” para colocar as modificações em teste. Somente quando o ramo “staging” for aprovado em todos os testes regressivos, seu conteúdo dará origem a uma nova versão do software. Para saber mais sobre o Gitflow e como ele ajuda a automação do processo de desenvolvimento de software, leia o artigo “Entendendo o funcionamento do CICD dentro do Git flow”. Para conhecer mais sobre as regras de versionamento e atribuição de tag ou número de versões, leia o artigo “Como Funciona o Versionamento de Software e como criar Tags no Git”

Antes de demonstrarmos o uso do Git por meio de um exemplo prático, é preciso que você conheça alguns conceitos básicos:

Staging área – Local temporário onde são armazenadas todas as alterações que serão adicionadas no próximo commit;
Working directory – Todos os arquivos que estão sendo trabalhados no momento;
Commit – É um instantâneo (do inglês, snapshot) das modificações adicionadas na staging area, persistindo-as no repositório local;
Branch – É uma ramificação do código, apenas um ponteiro que aponta para um commit e tudo anterior a esse commit. O branch padrão é o master;
Head – É um ponteiro que aponta para algum commit ou alguma branch; e
Merge – Ação de juntar os commits de dois branches.

No Git um arquivo pode estar em um desses 4 estados:

untracked – Arquivos que não estavam no último commit;
unmodified – Arquivos não modificados desde o último commit;
modified – Arquivos modificados desde o último commit; e
staged – Arquivos preparados para comitar.

A figura abaixo mostra como os arquivos de código em um determinado repositório migram de um estado para o outro. Quando inseridos manualmente em um repositório local, isto é, no working directory, os arquivos são criados no estado untracked. Podemos alterar o estado de um arquivo para staged, através do comando git add <nome_do_arquivo>, isto informa ao Git que este arquivo deve ser versionado futuramente. Para removermos um arquivo da staging area, utilizamos o comando git rm –cached <nome_do_arquivo>, evitando seu versionamento. Quando editarmos qualquer arquivo em nosso working directory, o mesmo passará do estado unmodified, para o estado modified. Quando modificações em um arquivo de código são persistidas no repositório local pelo comando git commit, este arquivo transita do estado staged para unmodified.

Crie um repositório local para seu software

Agora que já entendemos um pouco sobre o git, vamos partir para a prática. A primeira coisa que devemos fazer quando queremos trabalhar com git, é criar um repositório, fazemos isso com o comando git init. Crie um diretório local para o código fonte do seu software e, em seguida, execute este comando. Esse comando cria um subdiretório .git com todos os metadados necessários. No exemplo abaixo, criamos o diretório “hello_world”.

Vamos criar um arquivo README.md em nosso projeto. Este arquivo é apenas um arquivo texto que, em geral, acompanha projetos de software explicando questões iniciais para seu uso ou instalação  Depois, vamos adicionar este arquivo à staging area para que faça parte do próximo commit. Fazemos isso com o comando git add. O comando touch do Linux é usado para criar arquivos vazios, além de alterar o registro de data e hora (timestamp) de arquivos ou pastas.

O comando git status pode ser utilizado para verificar que o arquivo README.md foi adicionado a staging area.

Agora vamos realizar o commit de nossas alterações, para que os arquivos na staging area passem a ser versionados. Só arquivos que sofreram commit terão suas versões controladas pelo Git. Para isso utilizamos o comando   git commit -m “Mensagem do commit”. A mensagem passada como parâmetro deve ser utilizada com inteligência para que seja útil ao time de desenvolvimento, ela deve justificar e explicar as alterações pelas quais o código fonte passou.

Utilizando o comando git log podemos ver o histórico de commits realizados em nosso repositório.



Ignore os arquivos que não lhe interessa versionar

Muitas vezes, em um projeto de software, haverá arquivos binários que não devem ser adicionados a um repositório de código, evitando que a ferramenta Git tente realizar o controle de versão dos mesmos. Para exemplificar, vamos criar um arquivo main.cpp que imprime a mensagem “Hello World!” na tela e, em seguida, compilá-lo.

Após compilarmos, podemos verificar que foi gerado o arquivo main que é nosso executável. 

Porém, nós não queremos realizar um commit com esse arquivo. Para resolver isso, vamos criar o arquivo .gitignore onde vamos colocar todos os arquivos que queremos ignorar ao realizar um commit.  O comando echo é utilizado apenas para adicionar a nova linha “main” ao arquivo .gitignore.

Agora vamos salvar nossas alterações adicionando os arquivos main.cpp e .gitignore no staging. Podemos fazer isso através do comando “git add .“, que adiciona todos os arquivos modificados no staging.

Se checarmos o estado do nosso repositório, podemos observar que todas as modificações foram adicionadas ao staging.

Se tentarmos agora adicionar o executável main no staging, recebemos um aviso. 

Se quisermos adicionar um arquivo ignorado ao staging, devemos utilizar a opção -f no comando git add, para forçar a adição. 

Agora podemos realizar o commit das nossas alterações

Utilizando o comando git log, podemos observar que agora temos dois commits. 

Restaurando versões anteriores do código

Mas e se nos arrependermos de algum commit e desejarmos desfazê-lo? Bom, para isso temos dois comandos, git reset e git revert. 

No comando git reset existem três opções, soft, mixed e hard. A opção soft, move o HEAD para o commit indicado, mas mantém o staging e o working directory inalterados. A opção mixed, move o HEAD para o commit indicado, altera o staging e mantém o working directory. A opção hard faz com que o HEAD aponte para algum commit anterior, mas também altera a staging area e o working directory para o estado do commit indicado, ou seja, todas as alterações realizadas após o commit ao qual retornamos serão perdidas. Isso não é recomendável se as alterações já tiverem sido enviadas para o repositório remoto. Nesse caso devemos utilizar o git revert.

Vamos supor que queremos desfazer as alterações do último commit em nosso exemplo, utilizando o comando git reset com a opção hard. Dessa forma precisamos indicar ao comando git reset o código SHA-1 do commit ao qual queremos que o HEAD aponte. Outra forma de utilizar o comando git reset, é indicando quantos commits queremos retornar o HEAD, fazemos isso com o comando git reset HEAD~n. O parâmetro HEAD~n, nos indica que queremos posicionar o HEAD para n commits atrás. Por exemplo, para retornar para o commit anterior usamos HEAD~1.

Na última linha da  figura acima, podemos observar que o arquivo main.cpp que havíamos adicionado no segundo commit, não está mais em nosso working directory.

Por outro lado, o comando git revert cria um novo commit com as alterações do commit indicado. Utilizando git revert em nosso exemplo, nosso repositório ficaria assim

Como podemos verificar, um novo commit foi criado revertendo para as alterações do commit f861569.

Entendendo o uso de um repositório remoto

Agora que já aprendemos como utilizar o Git no repositório local, vamos entender como utilizamos o Git com um repositório remoto seguindo o fluxo de trabalho Gitflow.

Vamos supor que tenhamos um repositório em um projeto do Gitlab. Nosso primeiro passo é utilizar o comando git clone para baixar o repositório em nossa máquina. 

Nosso repositório está vazio, então agora vamos realizar nosso primeiro commit. 

Vamos subir as alterações para o repositório remoto no Gitlab. Fazemos isso com o comando git push -u origin master.

Podemos observar que o ramo master foi criado em nosso repositório no Gitlab. Agora precisamos criar os ramos develop e staging. Para isso podemos utilizar o comando git checkout -b develop master, que cria o ramo develop a partir do ramo master. Realizamos o mesmo procedimento para o ramo staging com o comando git checkout -b staging develop. Após criarmos os ramos, utilizamos os comandos git push -u origin develop e git push -u origin staging para subir as alterações no repositório remoto.

Utilizando o comando git branch podemos verificar os ramos em nosso repositório local. Para verificarmos os ramos remotos, utilizamos o comando git branch -r.

Resolvendo issues no GitLAB

Agora vamos supor que durante as reuniões realizadas com seu time, seu líder (em geral, um Product Owner) definiu e atribuiu a você uma funcionalidade que deverá ser desenvolvida no próximo ciclo de desenvolvimento (sprint), que em nosso exemplo é a funcionalidade  “Add function to print hello world”. Para isso, no GitLab, ele adicionou essa issue ao board “Sprint backlog”, conforme ilustra a figura a abaixo.

Antes de começar a implementar esta funcionalidade, precisamos a nova issue para o board “Doing”, conforme ilustra a figura abaixo.

Agora, precisamos criar um novo ramo onde para essa issue será resolvida, no jargão da área chamamos esse ramo de feature branch. Para isso, o criamos a partir do ramo develop e começamos a desenvolver a funcionalidade. Faremos isso clicando com o mouse no nome da issue, então seremos redirecionados para a página com a definição da issue, conforme ilustra a figura abaixo. Depois, vamos clicar no botão “Create merge request” e digitar no campo “Source (branch ou tag)” o nome da branch na qual iremos trabalhar, em nosso exemplo o branch “develop”, e por fim clicar em “Create merge request”.

Então, seremos redirecionados para a tela de definição do merge request, apresentada abaixo. Perceba que não é possível realizar o merge, pois ainda não existem alterações na nova branch criada.

Ao entrar na página Active branches do repositório remoto, podemos observar que foi criado um ramo cujo nome é definido pelo número e nome da issue que deu origem ao ramo.

Agora, é preciso atualizar nosso repositório local baixando o novo branch criado. Fazemos isso com os comandos git fetch origin nome_branch_nova que irá buscar o branch criado do repositório remotoe  git checkout nome_branch_nova, que cria um branch local que rastreia o branch remoto que acabamos de criar.

Finalmente, após implementar a funcionalidade, devemos realizar o commit e enviar para o repositório remoto através do comando git push.

Como podemos ver na página principal do projeto, apresentada abaixo, o repositório remoto no Gitlab foi atualizado. 

Como terminamos de implementar a funcionalidade, precisamos mover a issue para o board “Waiting Acceptance” e aguardar que alguém aprove as mudanças realizadas.

Após atualizar o repositório no Gitlab, podemos voltar na página da definição do merge request (abaixo) e após uma alguém (uma liderança) garantir que a issue foi resolvida, podemos realizar o merge dessas alterações clicando em “Mark as ready” e depois em “Merge”. Neste exemplo, apenas para sermos breves evitaremos o passar pelo ramo staging e código será mesclado diretamente com ramo develop.

No entanto, o procedimento descrito anteriormente, também é válido para o branch staging. Além disso, em uma situação crítica, supondo que tenha ocorrido uma falha de software no ambiente de produção, é preciso corrigi-la o mais rapidamente possível. Para isso, deve-se seguir o mesmo procedimento descrito anteriormente. Porém, desta vez, deve-se criar um branch hotfix a partir do ramo master e fazer o merge de volta no ramo master e também no ramo develop para propagar as correções.

Considerações finais

Neste artigo, mostramos de forma prática como utilizar o Git tanto localmente quanto remotamente. Para isso, apresentamos alguns conceitos básicos do Git e discutimos o funcionamento esperado para o fluxo de trabalho de uma equipe de desenvolvimento de software conforme preconiza o Gitflow. Nós mostramos como criar um repositório local e ignorar arquivos que não devem ser versionados, como realizar commits e desfazê-los, como criar ramos e enviar alterações para o repositório remoto. Finalmente, mostramos como os desenvolvedores devem resolver as issues que lhes são atribuídas. 

Assim, esperamos que este artigo seja útil a todos que estejam se iniciando no uso do Git. Porém, se você tiver interesse em entender aprender mais sobre o Git, de uma forma completamente interativa, recomendamos acessar Learn Git Branching

Gostou do texto? Ele foi útil para você? Nos diga nos comentários, se você teve alguma dúvida que gostaria que fosse sanada! 

Artigo escrito por Higor Duarte. Revisado por Prof. Tiago Carneiro e Vinícius de Paula."
O TerraLAB e as parcerias em inovação e extensão tecnológica,http://www2.decom.ufop.br/terralab/o-terralab-e-as-parcerias-em-inovacao-e-extensao-tecnologica/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/06/Untitled-design-6.png,"Édada a largada para a convocação para o processo seletivo do Programa de Trainee TerraLAB, edição 2021.2. Programa que vem capacitando estudantes de graduação e pós-graduação de qualquer curso da UFOP para atuar nas empresas de Tecnologia da Informação. Para se inscrever, preencha este formulário até o dia 11 de junho, o processo seletivo terá início no dia 14 de junho. O Programa de Trainee é realizado totalmente online.

Os esforços do TerraLAB vem rendendo a seus voluntários um leque de conhecimentos práticos, vivências e experiências em funções, processos e tecnologias para a produção de software. A evidência maior do sucesso do Programa de Trainee TerraLAB é o fato de 100% dos trainees que concluíram o treinamento já foram absorvidos pelo mercado de trabalho, antes mesmo de se formarem, totalizando 23 estudantes em menos de 1 ano e meio. Atualmente, as empresas parceiras do TerraLAB oferecem um total de 10 estágios remunerados para que os estudantes que tiveram melhor desempenho no processo seletivo de 2021.1 continuem seu treinamento no laboratório. Este é um enorme incentivo e um tremendo investimento no aperfeiçoamento profissional e pessoal dos estudantes da UFOP.

Este artigo tem o objetivo de apresentar aos estudantes da UFOP, aos pesquisadores, profissionais e empresas de TI os resultados e as parcerias que o TerraLAB vem desenvolvendo ao longo de anos para levar à frente seus projetos de extensão e inovação tecnológica. Desta maneira, esperamos justificar e agradecer o apoio que recebemos até o momento e, sobretudo, despertar o interesse de outras pessoas e instituições em fazer parte do nosso time. Vem com a gente!

Breve histórico em pesquisa, produção de software e geração de startups 

O TerraLAB – Laboratório para Pesquisa e Capacitação em Desenvolvimento de Software, teve sua origem em 2007, fruto de uma parceria entre a UFOP e o Instituto Nacional de Pesquisas Espaciais (INPE). A sua principal missão é realizar pesquisas científicas neste tema, além de manter e evoluir a plataforma de software livre e de código aberto denominada TerraME (www.terrame.org). Atualmente, o TerraME serve como base tecnológica para dois produtos operacionais do INPE utilizados para definição e análise de políticas públicas para as regiões do Cerrado e da Floresta Amazônica, dois dos biomas brasileiros mais ameaçados. Este produtos são: (1) Arcabouço para Modelagem e Simulação de Mudanças de Uso e Cobertura  do Solo – LUCCME (luccme.ccst.inpe.br); e (2) Modelo de Estimativa de Emissão de Gases de Efeito Estufa por Mudanças de Cobertura da Terra – INPE-EM (inpe-em.ccst.inpe.br). O TerraME também é base tecnológica para pesquisas institucionais em diversos programas de pós-graduação do INPE, nos níveis de mestrado e doutorado, entre eles o curso de Ciência do Sistema Terrestre, o curso de Sensoriamento Remoto e o curso de Computação Aplicada. Ele também serviu como base tecnológica para cooperações científicas com Programa de Computação Científica (PROCC) da FIOCRUZ (ver projeto DengueME), com o Instituto de Geoinformática da Universidade de Munster, Alemanha e com o Environmental Change Institute (ECI) da Universidade de Oxford, Inglaterra. Os artigos científicos que resultam destas pesquisas podem ser acessados a partir do currículo lattes do Prof. Tiago Garcia de Senna Carneiro, coordenador do laboratório. Por meio destas parcerias, o TerraLAB estabelece uma forte conexão entre instituições de pesquisa de destaque e as pesquisas realizadas nas diversas graduações e na pós-graduação em Ciência da Computação  da UFOP. 

Para levar à frente estas iniciativas, o TerraLAB adquiriu e desenvolveu técnicas, métodos e processos para garantir qualidade aos produtos de software e velocidade aos seus times de desenvolvimento. Para isso, estreitou laços com a indústria de software e co-evoluiu com várias empresas parceiras. Neste sentido, o TerraLAB também se fez um ateliê de software que oferece aos estudantes da UFOP um ambiente de inovação que alinha o método/conhecimento científico ao saber produtivo, transformando os resultados de pesquisa em produtos e serviços de software. Nosso entendimento é de que a inovação nasce quando a ciência toca a vida! Neste contexto, o TerraLAB vem gerando startups como a Usemobile, Cachaça Gestor e MineInside (que já não existe por ter sido comprada).  Os trainees do processo seletivo de 2019.2 e 2020.1 produziram ao todo 7 aplicativos com versões WEB, Android e iOS. Alguns deles já estão disponíveis na Play Store, como é o caso do aplicativo da Orquestra Ouro Preto, acessível a partir de dispositivos Android e iOS. Todos os outros aplicativos já foram concluídos e se encontram em fase de testes alpha e beta.

Além destes esforços, para promover apropriação, por parte da sociedade, do conhecimento científico e tecnológico gerado e utilizado por nossas equipes, o TerraLAB mantém diversos canais de comunicações e redes sociais, todos acessíveis pelo link:  https://linktr.ee/terralab.ufop.

As empresas parceiras do TerraLAB

Desde 2019, o laboratório conduz um Programa de Trainee que permite aos estudantes vivenciar os cargos existentes no ecossistema de produção de software e ganhar experiência nos processos e tecnologias que representam o estado-da-arte na indústria de desenvolvimento de software. Para isso, recebe mentoria de profissionais de reconhecida autoridade e conta com a cooperação de diversas empresas, entre elas  destacamos: 

Usemobile: Especialista no desenvolvimento de aplicativos mobile para Android e iOS e aplicações web e desktop.
Stilingue: Plataforma multicanal para melhores experiências entre marcas e consumidores.  
Gerencianet: Conta digital focada em negócios para que empreendedores possam emitir e gerenciar recebimentos.
Memory: Desenvolve soluções tecnológicas, líder no desenvolvimento de sistemas para órgãos públicos. 
GS Ciência do Consumo: Empresa de inteligência com foco no aumento do faturamento para o varejo e a indústria.
Cachaça Gestor: Fornece sistema computacional que visa tornar a vida do produtor de cachaça mais eficiente.

Outras empresas nos oferecem apoio por meio de palestras e treinamentos pontuais, mas algumas  já negociam conosco modelos de colaboração mais abrangentes: 

Take.Blip: Especialista em facilitar a comunicação entre empresas e pessoas. Gestão e evolução de chatbots e contatos inteligentes. 
Tembici: Empresa líder da América Latina de tecnologia para micromobilidade que cria soluções para inspirar uma revolução do espaço urbano.
Nubank: Fintech que desenvolve soluções simples, seguras e 100% digitais, maior banco digital independente do mundo. 
iFood: Maior foodtech da América Latina. 
Como a indústria enxerga o TerraLAB

No final da página principal do site do TerraLAB, é possível ler o depoimento de nossos parceiros e dos trainees que já concluíram o treinamento. Além desses depoimentos, selecionamos os áudios de duas empresas parceiras que já avaliaram trainees do TerraLAB em seus processos seletivos. 
No primeiro áudio, o diretor de tecnologia (CTO) da empresa Usemobile, Patrick Brunoro, relata sua experiência com trainee do TerraLAB, após o primeiro mês de chegada dele na empresa. Durante um ano de treinamento,Arilton Aguilarfoi gerente de engenharia e product owner do TerraLAB e Koda Gabriel foi desenvolvedor de aplicativos móveis do TerraLAB. Atualmente, Aril (como é carinhosamente conhecido por nós) é Analista de Requisitos e Koda é Desenvolvedor iOs Jr na Usemobile.

No segundo áudio, o Gerente de Engenharia de Software da Tembici, Francisco Daniel Costa, relata suas impressões após entrevistar 18 trainees que passaram pelo nosso Programa de Trainee.

Captação de recursos e estágios remunerados 

Além da transferência de know-how, de nos oferecer mentorias, treinamentos e palestras, as empresas parceiras apoiam o TerraLAB nos oferecendo plataformas de aprendizado online e estágios remunerados; estimamos que, ao todo, estes investimentos representam o valor de 240 mil reais/ano provenientes da iniciativa privada.

 Sempre que possível, os trainees que se destacam recebem incentivos, na forma de remuneração direta e indireta (auxílio alimentação, plano de saúde e plano odontológico), para permanecerem no laboratório desempenhando seu treinamento e atividades de desenvolvimento e pesquisa. À medida que ganham experiência e vivência, cerca de 4 a 6 meses dependendo unicamente do estudante, esses trainees migram gradativamente para os ambientes produtivos das empresas. Aqueles que permanecem no laboratório por mais tempo, recebem treinamento em liderança e gestão de pessoas, desenvolvendo habilidades que vão além das habilidades técnicas.

Atualmente a empresa Gerencianet nos oferece três licenças da plataforma de aprendizado online Alura. As empresas Usemobilie e Cachaça Gestor nos oferecem uma licença da plataforma Udemy. Por meio destas licenças, os estudantes têm acesso a certificações e a cursos de elevada qualidade em diversas tecnologias e áreas do conhecimento.

A empresa Usemobile nos oferece dois estágios remunerados pelo segundo ano consecutivo. As empresas Stilingue e Memory nos oferecem quatro estágios remunerados, cada. As negociações com as empresas GS Ciência do Consumo e Tembici já estão em fase avançada. A Memory já sinalizou o desejo de ampliar a parceria. As negociações com a empresa iFood já estão em andamento. Neste contexto, esperamos, brevemente, poder oferecer mais estágios remunerados aos trainees do laboratório. 

Para que possamos ampliar a captação de recursos, desonerando e beneficiando as empresas parceiras, estamos em constante colaboração com a Coordenadoria de Convênios e o Nucleo de Inovação Tecnológica e Empreendedorismo da UFOP, para tirar proveito de tudo da fomenta e permite o Novo Marco Legal de Ciência, Tecnologia e Inovação (Lei nº 13.243/2016). Neste contexto, estamos preparando instrumentos legais e institucionais para facilitar e agilizar a captação de recursos financeiros, equipamentos, serviços de computação em nuvem e, é claro, viabilizar o pagamento de bolsas de estudos com menos burocracia. Um de nossos principais objetivos é viabilizar que as empresas parceiras possam reverter parte dos impostos que pagam em prol desta iniciativa. 

O número de trainees só cresce

O número de estudantes que se inscrevem no processo seletivo e  que realizam e concluem  o Programa de Trainee do TerraLAB é cada vez maior. Na edição de 2019.2 tivemos 17 inscritos, 9 selecionados e apenas 7 concluintes. Na edição de 2020.1, tivemos 39 inscritos, 29 selecionados e 16 concluintes. Na edição 2021.1, tivemos 38 inscritos e 22 selecionados que permanecem em treinamento. Atualmente, 54 pessoas permanecem ligadas ao laboratório e 34 desempenham atividades rotineiras. Queremos ampliar todos estes números, por isso, estamos reforçando nosso convite a estudantes, profissionais, pesquisadores e empresas para somarem forças na construção desse sonho conjunto.  

Venha fazer parte do nosso time

Venha fazer parte desse ambiente onde há espaço para todos aqueles que são automotivados, perseverantes e comprometidos com o próprio sucesso e com o sucesso dos projetos em que se envolvem. Convidamos os estudantes a vivenciar as diversas áreas de atuação envolvidas na produção de software, passando pelo desenvolvimento de  produtos, operação de serviços em nuvem, gestão de projetos e de processos produtivos. Aproveite tudo o que temos a oferecer em uma via de mão dupla. Aprenda as ferramentas, tecnologias e processos que estão em pauta na indústria e no mercado, assim como também tenha a chance de instaurar o novo e trazer suas próprias construções em uma contínua melhoria conjunta!

Se você é estudante da UFOP e se considera uma pessoa automotivada, autodidata, comprometida com o sucesso, proativa, descontraída e generosa, você tem o perfil que buscamos. Você poderá colher benefícios que só o ambiente profissionalizante do TerraLAB oferece para completar o conhecimento acadêmico oferecido pela UFOP, entre este benefício citamos: 

Contato com empresas de destaque na indústria;
Mentorias com profissionais experientes;
Estágios remunerados sem sair da universidade;
Aumento da sua empregabilidade;
Vivência dos diferentes cargos existentes no ecossistema de desenvolvimento de software;
Treinamento e ganho de experiência prática nos mais atuais processos e tecnologias para produção de software ;
Cumprimento da carga horária em atividades extra-curriculares (ATV100)  exigida pelos cursos de graduação.

Para isso, basta que você possa dedicar 20 horas semanais às atividades do TerraLAB e faça logo sua inscrição preenchendo este formulário. As inscrições estarão abertas até o dia 11 de junho e o processo seletivo terá início no próximo dia 14. Tanto o processo seletivo quanto o treinamento são realizados completamente online. Vem com a gente, faça parte do nosso time!
Se você representa uma empresa e se interessou por tudo o que viu aqui, entre em contato direto pelo e-mail do professor Tiago Carneiro.

Artigo escrito por Prof. Tiago Carneiro. Revisado por Paloma Bento."
A influências das cores em projetos de UX/UI,http://www2.decom.ufop.br/terralab/a-influencias-das-cores-em-projetos-de-ux-ui/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/05/UX-4-730x350.png,"Apsicologia é uma ciência que estuda a mente humana, os fatores que provocam diferentes decisões, atitudes, aprendizagem, inteligência, emoções e sensações. Isso faz com que seja interessante entendermos um pouco sobre, pois, assim podemos melhorar nossos produtos ou serviços pensando no comportamento de seu público alvo. Por isso, neste artigo abordaremos a psicologia das cores e sua aplicação na experiência do usuário ou, simplesmente, UX (de User eXperience).

A psicologia das cores é o estudo que busca compreender como as cores influenciam nas sensações e no comportamento. As cores podem conter diferentes significados que são influenciados por questões fisiológicas, culturais, de grupo e individuais.

Ela pode ajudar a transmitir uma mensagem mais clara e alavancar os negócios através das sensações que provocam nas pessoas. Pensando assim, podemos citar como exemplo as empresas do ramo alimentício que geralmente utilizam o vermelho e o amarelo que são cores que remetem à fome.

O que é necessário saber para aplicar a psicologia das cores?

Para aplicarmos a psicologia das cores em nosso projeto é importante termos em mente que apesar de haver alguns guias que podem ser bastante úteis para entendermos como cada cor influencia nosso comportamento. Precisamos levar em consideração o contexto, fatores culturais e experiências individuais, pois podem afetar a percepção das pessoas em relação a cada cor.

Um ótimo exemplo, é o caso de uma das franquias do McDonald ‘s na Turquia, que tiveram de abrir mão do seu clássico vermelho e amarelo devido à pressão causada pela rivalidade entre dois times de futebol. A loja da rede estava localizada próxima ao estádio dos Besiktas e a torcida não aceitou as cores, pois representava o time rival. Essa pode ser uma lição valiosa, de que precisamos estar atentos a todas as variáveis que podem influenciar um determinado público alvo.

Figura 1 – Loja da rede na região de Besiktas, Turquia. Fonte: Página da ESPN, A rivalidade que obrigou o McDonald’s a esconder suas cores¹

Como os profissionais que trabalham com UX podem se beneficiar deste conhecimento?

O profissional de UX é responsável por pensar em toda a experiência que os usuários terão durante o uso de um produto ou serviço e pode utilizar da psicologia das cores como parte do processo.  

Neste sentido, as cores podem servir como uma grande aliada nas sensações que os clientes irão experimentar com aquilo que lhe estiver sendo oferecido. Assim como também irão impactar na imagem de uma empresa perante seu público.

Tendo em mente o contexto, podemos utilizar alguns guias sobre as cores que serão bem úteis na hora de pensarmos o design e a experiência do usuário.

Figura 2: Guia emocional das cores.  Fonte: Acervo publicitário²


Alguns especialistas do site “No Film School” elaboraram um compilado contendo todos os significados ligados a determinadas cores, para facilitar a vida dos profissionais de diversas áreas:

Vermelho: raiva, paixão, fúria, ira, desejo, excitação, energia, velocidade, força, poder, calor, amor, agressão, perigo, fogo, sangue, guerra e violência.
Rosa: amor, inocência, saúde, felicidade, satisfação, romantismo, charme, brincadeira, leveza, delicadeza e feminilidade.
Amarelo: sabedoria, conhecimento, relaxamento, alegria, felicidade, otimismo, idealismo, imaginação, esperança, claridade, radiosidade, verão, desonestidade, covardia, traição, inveja, cobiça, engano, doença e perigo.
Laranja: humor, energia, equilíbrio, calor, entusiasmo, vibração, expansão, extravagância, excessivo e flamejante.
Verde: cura, calma, perseverança, tenacidade, autoconsciência, orgulho, imutabilidade natureza, meio ambiente, saudável, boa sorte, renovação, juventude, vigor, primavera, generosidade, fertilidade, ciúme, inexperiência, inveja, imaturidade e destruição.
Azul: fé, espiritualidade, contentamento, lealdade, paz, tranquilidade, calma, estabilidade, harmonia, unidade, confiança, verdade, confiança, conservadorismo, segurança, limpeza, ordem, céu, água, frio, tecnologia e depressão.
Roxo/Violeta: erotismo, realeza, nobreza, espiritualidade, cerimônia, misterioso, transformação, sabedoria, conhecimento, iluminação, crueldade, arrogância, luto, poder, sensibilidade e intimidade.
Marrom: materialismo, excitação, terra, casa, ar livre, confiabilidade, conforto, resistência, estabilidade e simplicidade.
Preto: não, poder, sexualidade, sofisticação, formalidade, elegância, riqueza, mistério, medo, anonimato, infelicidade, profundidade, estilo, mal, tristeza, remorso e raiva.
Branco: sim, proteção, amor, respeito, mesura, pureza, simplicidade, limpeza, paz, humildade, precisão, inocência, juventude, nascimento, inverno, neve, bom, esterilidade, casamento (culturas ocidentais), morte (culturas orientais), frio, clínico e estéril.
Prata: riqueza, glamour, fascínio, diferença, natural, liso, suave e macio.
Considerações finais

Compreender a psicologia das cores e aplicá-la na hora de escolher uma paleta de cores que transmita bem a mensagem do design, traz consigo um componente-chave do impacto psicológico que os produtos ou serviços devem ter sobre os usuários e, consequentemente, sua experiência.

As cores bem definidas e bem escolhidas elevam o design de “bom” a “ótimo”, enquanto a escolha de cores incorretas ou fora de harmonia pode prejudicar a experiência geral do usuário e como consequência interferir em sua capacidade de compreender as funcionalidades de uma aplicação. 

Embora tenhamos cores que sejam consideradas universais no design UX (como o preto, branco e cinza, utilizadas na maioria dos designs existentes) as cores com as quais são combinadas podem ter grande impacto sobre a compreensão do usuário de uma aplicação.

Para isso é necessário compreender o contexto social do usuário, por mais que a combinação esteja harmoniosa e bem definida, se o contexto cultural não for conhecido, todo o projeto poderá fracassar. Por isso, lembre-se: o que agrada o designer pode não agradar ao público alvo. Assim, estudar o usuário para atingir os melhores resultados nos negócios é essencial.  

Gostaria de aprender mais sobre UX? Você atua profissionalmente como designer de UX ou de UI? Gostaria de contribuir para essa discussão? Ou quem sabe conhecer mais sobre o TerraLAB? Comente, compartilhe esta publicação e não deixe de acompanhar nosso blog e redes sociais!

Artigo escrito por Eudes Rodrigues e Gustavo Lucas Moreira. Revisado por Walisson Farias, Daniel Keoma e Prof. Tiago Carneiro.

Referências

Como a Psicologia é Essencial em Seus Projetos de UX Design 

Entenda o que é Psicologia das Cores e descubra o significado de cada cor.

psicologia cores um guia avancado para profissionais 

harmonia-das-cores

Psicologia da Cores – Aulão completo e atualizado sobre Significado das Cores

Psicologia das Cores – Significado das cores no design

Psicologia das Cores no Marketing

A PSICOLOGIA DAS CORES – Significado das Cores no Marketing

How Colors Affect Conversion Rate

A Psicologia das Cores no Marketing

Psicologia das Cores – @Curso em Vídeo HTML5 e CSS3

Hack de carreira: como se tornar um UX designer (e por que você deveria)

A rivalidade que obrigou o McDonald’s a esconder suas cores | Blogs"
Resultados do Processo Seletivo 2021.1,http://www2.decom.ufop.br/terralab/resultados-do-processo-seletivo-2021-1/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/05/imagemdecabecalho-730x350.png,"Há três semanas atrás encerramos o nosso processo seletivo. No artigo Encerramento do processo seletivo 2021.1, explicamos como o processo foi conduzido, suas premissas e discutimos os principais desafios vividos por nosso time e pelos candidatos a trainees. Foram cinco semanas de muito trabalho que resultaram na aquisição de 22 novos colaboradores para o time TerraLAB. Eles atuaram nas áreas de Data Analytics, Geoinformatics, Marketing, Infrastructure, User Experience, Quality Assurance, Front-end Web, Front-end Mobile e Backend.

Nesse artigo, iremos mostrar os resultados concretos obtidos em cada uma dessas áreas ao longo do processo seletivo. Para isso, apresentamos vídeos gravados pelos gerentes responsáveis por essas áreas. Os gerentes são integrantes mais experientes do TerraLAB, isto é, são estudantes que já passaram pelo treinamento de nível operacional e atualmente realizam treinamento em atividades de gestão e ensino. Nos vídeos são apresentados os resultados alcançados pelos trainees em cada área de conhecimento do laboratório, esperamos que esses vídeos também evidenciem a maturidade de nosso processos de desenvolvimento de software, evidenciem nosso domínio sobre as práticas e ferramentas utilizadas na cultura DevOps e nosso domínio sobre as tecnologias de desenvolvimento mais atuais.

Os vídeos estão em nosso canal do YouTube e, para sua comodidade, listamos o link a seguir. 

Vídeo gravado por João Pedro Mendes – Gerente de User Experience.

Vídeo gravado por Carlos Magalhães – Gerente de Marketing.

Vídeo gravado por Emanuel Xavier – Gerente de projetos.

Vídeo gravado por Vinícius de Paula- Gerente de Desenvolvimento de Software.

Vídeo gravado por Diego Henrique – Gerente de Data Analytics.

Vídeo gravado por Guilherme Carolino – Gerente de Infraestrutura.

Considerações finais

Os resultados apresentados no final do processo seletivo confirmam o êxito dos trainees ao realizarem os desafios das suas respectivas áreas. Esse é apenas o começo de suas trajetórias no TerraLAB, que serão sempre marcadas por mais desafios e muito aprendizado. Durante seu período de capacitação dentro do laboratório, os estudantes experimentarão na prática a vivência do mercado de desenvolvimento de software e contarão com a colaboração de todo nosso time e das empresas parceiras.

Você ficou curioso sobre o processo seletivo? Deu aquela vontade gostosa de fazer parte do nosso time? Gostaria de conhecer mais sobre o TerraLAB? Comente, compartilhe esta publicação e não deixe de acompanhar nosso blog e redes sociais!"
Aplicativos para o Controle das Finanças Pessoais,http://www2.decom.ufop.br/terralab/aplicativos-para-o-controle-das-financas-pessoais/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/05/Aplicativos-para-o-Controle-das-Financas-Pessoais-2-730x350.png,"Este artigo sai um pouco do tema “Desenvolvimento de Software” e mostra como eles podem ser utilizados para a melhoria de vida, impactando o bem estar de famílias. Você pensou em utilizar aplicativos para controlar suas finanças pessoais? Neste artigo, tratamos desta questão como um exemplo vívido e muito simples dos benefícios que a tecnologia pode trazer. Escolhemos um exemplo de aplicativo que fosse útil a todos, sem exceções. A possibilidade de impactar positivamente a vida das pessoas justifica muito, ou quase tudo, daquilo que fazemos no TerraLAB. Vem com a gente!

A educação financeira nada mais é do que uma forma de entender a importância do dinheiro no dia-a-dia e conseguir monitorá-lo e controlá-lo adequadamente. Não é simplesmente economizar, mas é ter consciência dos riscos e oportunidades que envolvem este tema. Neste contexto, aplicativos para smartphones ou tablets podem lhe auxiliar a  decidir o que fazer com o seu salário.  Abaixo estão algumas sugestões de aplicativos para este fim:

O aplicativo Minhas Economias é muito bem avaliado na playstore da Google  (4.4) e teve cerca de 38 mil avaliações. Ele permite organizar entradas e saídas, além de categorizar os grupos que influenciam a renda mensal. Ele disponibiliza histórico de transações cadastradas, extrato mensal e gráficos de movimentação financeira. Também é possível estabelecer metas específicas para determinados objetivos e acompanhá-los ao longo do tempo, além de ser uma opção gratuita de apoio na administração financeira.

O aplicativo Mobiles Controle Financeiro também é muito bem avaliado (4.6) em aproximadamente 241 mil avaliações. Ele permite todos os itens que o aplicativo anterior oferece, além de gerar gráficos e relatórios personalizados com simples comandos. Este aplicativo permite o gerenciamento de cartões de crédito, controle de investimentos, leitura de SMS e notificações de bancos e geolocalização de despesas. Destacamos que é possível ter a versão gratuita com algumas limitações, porque só é possível usufruir na versão paga.

Acima de tudo, você deve compreender os conceitos que giram entorno da economia pessoal, para perceber que existem diversas opções para lhe direcionar a um controle de gastos. Para verificar este pressuposto, o TerraLAB buscou a Educadora e Consultora Financeira Kelly Ribeiro, para comentar sobre o uso de recursos digitais no controle pessoal financeiro.

“Para facilitar a organização financeira, podemos contar com os recursos computacionais, porém é preciso analisar bem quais utilizar. O desconhecimento de conceitos básicos sobre finanças pode ter o efeito contrário e acabar prejudicando o direcionamento de gastos, como é o caso dos aplicativos financeiros dos bancos.  Muitos deles possuem vínculos com instituições financeiras, que os utilizam para saber da sua situação e então te oferecem as “soluções”, que nem sempre são as melhores. Ressalto que em muitos casos eles criam um hábito desnecessário de “anotar sempre os seus gastos”. Diagnósticos de direcionamento financeiro, ou seja, entender quais gastos essenciais e não essenciais, uma vez ao ano ou quando o cenário muda, podem ser o suficiente para se preparar para os imprevistos econômicos. 

Basicamente, o orçamento mensal precisa conter os seguintes itens:

Se pagar – valor que você tem para gastar como quiser, ou seja passar o mês;
Reserva financeira – para casos de emergência e de oportunidades;
Objetivos – valor para seus objetivos de curto, médio e longo prazo;
Despesas – são as despesas mensais, que podem ser categorizadas por valor, Tipo(mensal ou bimestral, por exemplo) e formas de pagamento.

Neste caso uma boa e velha agenda ou então uma planilha de Excel simples, é suficiente. 

Veja abaixo o exemplo de uma planilha:

“Resumindo, não importa se é em agenda, caderno, planilha, papel de pão…o importante é que você escolha uma ferramenta simples, que você tenha familiaridade e que coloque em prática. Só melhoramos algo quando de fato conhecemos a situação que nos encontramos atualmente.“

Kelly Ribeiro é membro da Associação Brasileira de Educadores Financeiros (ABEFIN), Pós-Graduanda em Educação Financeira Metodologia (UNOESTE) e graduada em Engenharia de Produção – UNIPAC. Linkedin: kelly-ribeiro.

Artigo escrito por Josemar Coelho Felix. Revisado por Prof. Tiago Carneiro.

REFERÊNCIAS

[1]Pravaler. Educação financeira, qual sua importância de saber sobre finanças. Disponível em: https://www.pravaler.com.br/educacao-financeira-qual-a-importancia-de-saber-sobre-financas . Acessado: 27/04/2021

[2]ParanáBanco. Os 5 aplicativos de controle financeiro que você precisa conhecer. Disponível em: https://paranabanco.com.br/blog/educacao-financeira/os-5-aplicativos-de-controle-financeiro-que-voce-precisa-conhecer. Acessado: 27/04/2021"
Encerramento do processo seletivo 2021/1,http://www2.decom.ufop.br/terralab/encerramento-do-processo-seletivo-2021-1/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/05/WhatsApp-Image-2021-05-05-at-18.16.20-1-730x350.jpeg,"Após cinco semanas de muitos desafios, treinamento, aprendizado e trabalho em equipe, finda o processo seletivo do Programa de Trainee do TerraLAB, para o primeiro período letivo de 2021. Neste artigo, abordamos como o processo seletivo foi conduzido e apresentamos os resultados alcançados e as dificuldades vivenciadas pelos candidatos a trainees durante este processo. Desta forma, esperamos informar as empresas parceiras que apoiam o TerraLAB e encorajar estudantes que avaliam a possibilidade de fazer parte do nosso time. 

Ao invés de um processo seletivo que filtra ou de um processo seletivo que promova a competição entre os participantes, o TerraLAB possui um processo seletivo inclusivo, que promove a colaboração e o espírito de equipe. Uma vez que não temos limites de vagas, buscamos um processo seletivo que gere oportunidades para o máximo de pessoas, que respeite a diversidade e promova a inclusão etária, de gênero, de raça, de credo e de formação (precisamos de pessoas dos mais diversos cursos e habilidades). Com a ajuda das empresas parceiras, estamos cientes de que temos a capacidade de oferecer capacitação técnica de alta qualidade. Portanto, alinhado com nossos valores, o processo seletivo busca por pessoas automotivadas, comprometidas com o sucesso dos projetos nos quais se envolvem, dedicadas, autodidatas, generosas, gentis, descontraídas e proativas. 

Desta forma, o TerraLAB constrói e mantém um ambiente direcionado ao crescimento em conjunto da equipe, mostrando que é possível trabalhar e desenvolver o ser humano de maneira empática e amigável. Por isso, mesmo durante o processo seletivo, ninguém, em momento algum, trabalhou sozinho. Nossa preocupação é inserir cada participante dentro dessa cultura, deixando claro que nossas métricas de avaliação não se pautam apenas em produção, mas também privilegia o engajamento e a capacidade colaborativa de cada um. Esse é o pilar que garante a satisfação das nossas parceiras e da nossa equipe, orientando todos ao sucesso.

Trabalho em equipe

Conforme a cultura Agile – integrada a todos os processos do laboratório – cada participante foi alocado a um chapter específico, de acordo com as habilidades que gostaria de desenvolver. Chapter ou capítulo é termo organizacional da cultura Agile que agrupa pessoas com habilidades e responsabilidades similares, para que se fortaleçam tecnicamente ao colaborarem e possam também suprir ausências umas das outras. Nas equipes assim organizadas, é comum que haja um chapter para as diferentes áreas de conhecimento, desenvolvimento e produção da tecnologia da informação. Atualmente, a equipe do TerraLAB se estrutura nos seguintes chapters: Data Analytics, Geoinformatics, Marketing, Infrastructure, User Experience, Quality Assurance, Front-end Web, Front-end Mobile e Backend . 

Os participantes do chapters diretamente relacionado às atividades de desenvolvimento de software, Quality Assurance, Front-end Web, Front-end Mobile e Backend, também foram divididos em pequenas equipes denominadas squads. Cada squad é um pequeno time de desenvolvimento de software cujo objetivo é produzir aplicações ou serviços (SaaS – Software as a Service) dentro do foco de interesse e conhecimento do laboratório. Neste contexto, todo squad foi formado por, pelo menos, um indivíduo de cada um destes chapters. Alguns participantes decidiram em comum acordo com o TerraLAB e se voluntariaram para participar do processo seletivo em mais de um chapter. Diante desta decisão, estes participantes passaram a ser avaliados em uma função principal e foram bonificados por seu desempenho na função secundária.

Para fins de mentoria, treinamento e gestão do próprio processo seletivo, cada um dos chapters ou squads contou com um tutor. Este tutor, em geral, é um integrante experiente do TerraLAB que, do início do processo até o final do treinamento dos candidatos aprovados, passa a vivenciar atividades de gerência e ensino, responsabilizando-se pelo bem estar e desempenho de um chapter ou squad.

A seguir, descrevemos como foram conduzidos os desafios dentro de cada um destes grupos de pessoas, seus resultados e suas dificuldades.

Área de Desenvolvimento

“Como gerente técnico fiquei responsável por ajudar os trainees dos squads de desenvolvimento, esclarecendo dúvidas em relação a implementação dos desafios propostos pelo processo seletivo. A minha responsabilidade foi ajudar e adaptar os squads durante todo o processo seletivo.  Para o sucesso do trabalho foi utilizado a ferramenta GitLab, que criando pequenos treinamentos para formar os profissionais, tornou possível coletar métricas, além de organizar o fluxo de tarefas e entregáveis. Para conseguir auxiliar adequadamente os trainees no seu desenvolvimento, em conjunto com outros gerentes, foram elaborados tutoriais que ensinavam como utilizar bibliotecas e implementar as funcionalidades dos sistemas, exigido no desafio”, destaca Vinícius de Paula, gerente de engenharia, responsável pela área de desenvolvimento.

Todos os squads de desenvolvimento receberam o mesmo desafio de montar um sistema, descrito pelo cliente e coordenador e do TerraLAB. Cada um teve sua interpretação da descrição proposta e a partir desta, os times tiveram que buscar conhecimento para elaborar um backlog, criar protótipos de interface e validá-los com o cliente. Para conseguir realizar com sucesso esses feitos, o gerente de engenharia propôs prioridades de tarefas para cada sprint, ocasionando boas práticas no aprendizado do desenvolvimento do produto. 

Tela de login do aplicativo Web denominado GeoRef, desenvolvido pelo Squad 4 durante o processo seletivo.


Durante o processo foi possível notar que houve necessidade de incentivar o trabalho em equipe dos desenvolvedores Backend, principalmente no desenvolvimento de uma API unificada para atender a todos os front-ends, além de promover a interação dos desenvolvedores na construção da base de código. Para os desenvolvedores Front-end, a maior dificuldade foi a curva de conhecimento dos conceitos intrínsecos da linguagem e Frameworks utilizados. Nesta sessão de desenvolvimento destacou-se a necessidade de maior orientação e discussão de tópicos específicos da área de engenharia de software. Dessa maneira, o desafio de implementação dos mapas junto às funcionalidades do sistema tornou-se menos desafiador e mais compreensível. Em relação aos Product Owners, as principais dificuldades encontradas foram a adaptação à rotina de reuniões, e a utilização do GitLab para gerir os desenvolvedores e conseguir se adequar à cultura Agile do laboratório, mas com a iniciativa e vontade dos participantes, esse pequeno incômodo foi sendo superado aos poucos.

Tela de marcação de caminhamentos e pontos de interesse do aplicativo Android denominado GeoMap, desenvolvido pelo Squad 6 durante o processo seletivo.

Área de User Experience 

Os desafios da equipe de UX (Experiência do Usuário) foram divididos em cinco sprints de uma semana, onde a equipe trabalhou a todo momento de forma colaborativa em um desafio principal e um secundário.

O principal seria um projeto onde os candidatos estudaram todo o contexto do produto, realizaram o briefing com o usuário interno, pesquisa e benchmarking, elaboração de personas e por último a criação de um wireframe. A solução a ser desenvolvida seria criar um protótipo de um aplicativo para auxiliar profissionais de engenharia no levantamento de dados de campo.

Já o desafio secundário exigia que a equipe de UX auxiliasse os squads no desenvolvimento de UI (Interface do Usuário) por meio de validações e utilização das melhores práticas de UX. Ao final do processo seletivo todos os candidatos tiveram a oportunidade de aplicar na prática todo o conhecimento adquirido por meio dos cursos realizados na primeira semana do processo seletivo, o que trouxe grandes resultados em pouco tempo.

“Minha experiência durante o processo foi muito positiva, além de me preparar profissional e pessoalmente, tive contato com uma metodologia de mercado dentro da universidade, sendo única dentre as oferecidas e extremamente enriquecedoras. Espero que toda a bagagem adquirida possa me auxiliar futuramente na busca de aprimoramento de minhas habilidades sociais e técnicas.” –  disse Gustavo Lucas, trainee na área de UX.

Área de Quality Assurance

Na área de Quality Assurance, os participantes estudaram as técnicas de desenvolvimento ágil BDD – Behavior Driven Development, ou seja, Desenvolvimento Orientado por Comportamento, visando a melhora da qualidade dos produtos, ao elucidar os critérios de aceitação dos clientes e ao especificar cenários de testes equilibrados (muito robustos e tão baratos quanto possível). Estas técnicas foram utilizadas no desenvolvimento de testes funcionais e testes de aceitação para as aplicações Web e Mobile, assim como para a API do backend.

A maior dificuldade foi a de desenvolver a visão de cenários de testes capazes de prever cada necessidade do usuário, de forma a entregar uma aplicação robusta desde sua primeira iteração. Seguindo as práticas do BDD, houve também a necessidade de se criar cenários que não contavam necessariamente com um componente pronto ou funcional, mas que já previssem seu comportamento, suas falhas e a melhor forma de conduzir sua execução.

“O processo seletivo de 2021 foi algo muito gratificante para minha carreira, pois tive oportunidade de compartilhar meus conhecimentos com os trainees e vê-los se tornarem aptos a realizar a função de engenheiro de testes. O maior desafio que enfrentei na gerência foi a falta de conhecimento sobre atividades gerenciais, principalmente na organização e no trato com as pessoas, porém com a ajuda da equipe TerraLab pude desenvolver essas habilidades e cumprir as tarefas de gerente. Com pouco mais de 1 ano de TerraLAB já pude obter muito conhecimento, mas espero continuar desenvolvendo novas habilidades e compartilhando também” – depoimento de Bruno Augusto, gerente de Quality Assurance.

Área de Infrastructure

O processo seletivo da equipe de Infraestrutura consistiu em cinco sprints com cinco desafios, onde habilidades para fazer o deploy automático de componentes de software e manter um serviço em nuvem disponível foram aprendidas e exercitadas. Durante as sprints os trainees utilizaram ferramentas como docker e dockerfile, serviços de nuvem como o AWS Ec2, servidores Apache 2 e Ngnix e  implementaram pipelines CI/CD via GitLab.

  Os desafios foram realizados de forma individual, com intuito de gerar conhecimento para os trainees e simular de forma mais verossímil o trabalho em um sistema real. Todos foram realizados com dedicação e excelência pelo time, onde a maior dificuldade foi não estar trabalhando em um sistema real, havendo necessidade de criar simulações para todos os desafios.

“Participei do processo seletivo do TerraLAB nas áreas de UX e Infraestrutura e foi uma experiência bastante enriquecedora, pois tive a oportunidade de aprender coisas novas e colocar em prática através dos desafios ao lado de pessoas fantásticas interessadas em crescer e se desenvolver cada vez mais, só tenho a agradecer pela oportunidade.” – destaca Eudes Rodrigues, trainee nas áreas de UX e Infraestrutura.

Área de Data Analytics

O processo seletivo da equipe de Data Analytics consistiu em cinco sprints e três desafios onde foram introduzidos algumas técnicas da área da análise de dados. Os trainees foram incentivados a trabalharem em equipe, porém o último desafio foi individual com o objetivo de serem melhor avaliados.

Durante as sprints os trainees utilizaram técnicas como o web scraping para extrair dados necessários para um dos desafios, análise de séries temporais para prever uma determinada situação e uma análise livre para gerar diferentes tipos de insights a respeito de uma determinada base de dados.

Todos os desafios foram superados com excelência pelo time, ficou claro como o processo trainee tem a capacidade de ensinar, incentivar e os engaja-los a fazerem parte do TerraLAB.

“Participar do processo seletivo do Terralab foi uma experiência que possibilitou conhecer diferentes assuntos e ferramentas na área da Ciência de Dados, através de desafios práticos. Além disso, durante o processo a vivência de metodologias ágeis e da cultura organizacional laboratório permitiu criar uma visão de como é o mercado de trabalho.”- relata Lucas Natali, trainee na área de Data Analytics.

Área de Marketing

Durante o processo seletivo, os trainees da área de marketing realizaram cursos relacionados a área e foram apresentados ao atual plano de marketing do TerraLAB. A partir dos conhecimentos obtidos, os trainees produziram conteúdos para as redes sociais do lab e montaram uma nova estratégia de marketing de conteúdo.

Para a realização dessas atividades, os trainees se organizaram de maneira que fossem feitas colaborativamente e assim, todos puderam contribuir, o que foi determinante para o resultado final do processo.

“Aprender sobre uma área que faz parte da realidade de todas as empresas e poder desenvolver uma estratégia, que pode trazer impactos positivos, para o TerraLAB foi muito proveitoso e gratificante. Acredito que o aprendizado absorvido no processo seletivo vai ser um diferencial tanto na vida pessoal quanto na profissional daqui pra frente.” – depoimento de Paloma Bento, trainee na equipe de marketing.

Sequência de stories para Instagram feita pelos trainees de marketing, durante o processo seletivo. 

Considerações finais

Parabenizamos e agradecemos a cada participante que fez parte desse processo seletivo. A troca de conhecimentos e experiências promovida foi enriquecedora para o TerraLAB. A partir desse momento, os 22 aprovados entre os 38 candidatos iniciam sua jornada no lab, composta por mais desafios, que serão superados conjuntamente por todos nós.

Para o professor Tiago Garcia de Senna Carneiro, coordenador do TerraLAB, “Os principais desafios que o processo seletivo são: Projetar um processo seletivo inclusivo que crie oportunidades e amadureça pessoal ao invés de simplesmente filtrá-las; Manter os candidatos engajados aos longo do processo mesmo diante de desafios de nível elevado e de uma curva de aprendizado muito acentuada verificadas em algumas tecnologias; Mudar o mindset dos candidatos – de estudante para profissional – trazendo-os tão cedo quanto possível para uma cultura empresarial. Finalmente, de maneira muito prática foi preciso avaliar não só desempenho e qualidade técnica, mas também considerar com muito carinho a dedicação e as habilidades de comunicação e colaboração dos trainees.” 

Você ficou curioso sobre o processo seletivo? Gostaria de ficar atento aos acontecimentos do TerraLAB? Comente, compartilhe esta publicação e não deixe de acompanhar nosso blog e redes sociais para saber mais!"
"S.O.L.I.D: Conceitos e exemplos em Typescript – Parte 5, o “D”",http://www2.decom.ufop.br/terralab/s-o-l-i-d-conceitos-e-exemplos-em-typescript-parte-5-o-d/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/04/SOLID-13-730x350.png,"Um bom engenheiro de software precisa conhecer os princípios designados pelas letras que formam o acrônimo S.O.L.I.D:

S – Single Responsibility Principle 
O – Open-Closed Principle 
L – Liskov Substitution Principle 
I – Interface Segregation Principle 
D – Dependency Inversion Principle 

Este é o quinto e último artigo de uma série inteiramente dedicada a estes princípios. Caso ainda não tenha lido os artigos anteriores da série, sobre as letras “S”, “O” “L” e “I”, sugerimos a leitura antes que continue: Princípio da Responsabilidade Única, Princípio Aberto/Fechado,  Princípio da Substituição de Liskov e Princípio da Segregação de Interface respectivamente. 

A seguir iremos aprofundar em nossa explicações sobre a letra “D” – Princípio da Inversão de Dependência (The Dependency Inversion Principle).

DIP – Princípio da Inversão de Dependência

O Princípio da Inversão de Dependência estabelece que um sistema flexível é aquele cujo as dependências no código fonte sempre apontam para abstrações ao invés de componentes concretos. Estas dependências no código fonte são criadas através de palavras chaves como “include”, “require” ou “import”, como é o caso do Typescript.

Antes de nos aprofundarmos mais no princípio, é importante categorizar os dois tipos de componentes que podem existir em uma aplicação: 

Componentes de alto nível: São implementações que refletem as regras de negócio da aplicação. São sequências de passos que existem independente do tipo de tecnologia que é utilizada em um processamento. São a finalidade de um processo e não o meio.
Componentes de baixo nível: Também conhecido como componentes voláteis, são implementações que estão sujeitas a modificações frequentes. Geralmente, são códigos que realizam integração com bancos de dados (BD), com interfaces de programação de aplicações (APIs) ou com bibliotecas de código. É comum que estas tecnologias mudem e precisem ser atualizadas ou substituídas, com mais frequência do que a regra de negócio que uma aplicação implementa. São o meio para atingir o objetivo de um processo e não a finalidade. 

A ideia do DIP é que ao invés das dependências do código fonte apontarem para componentes de baixo nível, as mesmas sejam direcionadas para classes abstratas ou interfaces, que certamente são bem menos voláteis do que implementações concretas.

Estas implicações convergem em um conjunto bem específico de práticas de codificação:

Não faça referência a classes concretas que sejam voláteis: Ao invés disso dependa de classes abstratas ou interfaces;
Não implemente herança de classes concretas que sejam voláteis: Em linguagens estaticamente tipadas, herança é tipo de relacionamento mais forte e rígido; assim, deve ser usado com bastante cautela
Não sobrescreva (override) funções concretas: Ao utilizar ou chamar uma função concreta, seu código também está dependendo de um elemento concreto. O fato de você sobrescrever esta função não tira a dependência do elemento. Ao invés disso, você está criando um relacionamento de herança com ele. Uma boa solução para este cenário seria definir a função como abstrata e criar múltiplas implementações para a mesma. 
Nunca mencione o nome de qualquer classe ou componente concreto que seja volátil: Basicamente é o resumo do princípio em si.

Vamos entender melhor este princípio através de um exemplo prático em Typescript, discutindo como o mesmo pode ser violado:

import { Sequelize, QueryTypes } from 'sequelize';
 
interface User {
  nickname: string,
  password: string
};
 
export class Login {
  private _conn: Sequelize;
 
  private _createConnection() {
    this._conn = new Sequelize(
      'database',
      'username',
      'password',
      {
        host: 'localhost',
        dialect: 'mysql'
      }
    );
  }
  async execute(nickname: string, password: string) {
	    this._createConnection();
 
    if (!this._conn) {
      throw new Error(""DB connection not initialized"");
    }
 
    const users = await this._conn.query(`SELECT * FROM users WHERE nickname = ${nickname}`, {
      type: QueryTypes.SELECT,
      raw: true
    }) as User[];
 
    const user = users[0];
 
    if (user.password !== password) {
      throw new Error(""The given password is wrong. Try again"");
    }
 
    return ""Successfully logged"";
  }
}

A classe acima representa uma regra de negócio para realização de login de usuário em um sistema. 

Basicamente, o método principal recebe um nome de usuário e senha, tenta recuperar o registro do usuário no BD através do nickname informado, e verifica se o password recuperado é igual ao informado pelo usuário; se não for, um erro de falha de autenticação é retornado. O problema é que esta classe está indo contra grande parte dos princípios do S.O.L.I.D, a começar pelo número excessivo de responsabilidades que ela tem. Ao invés de implementar somente a regra de login, a classe também é responsável por implementar a integração com o BD através do pacote do Sequelize. Por essa razão, devemos dividir essas responsabilidades como é mostrado abaixo:

import { Sequelize, QueryTypes } from 'sequelize';
 
interface User {
  nickname: string,
  password: string
};
 
export default class SQLUserGateway {
  private _conn: Sequelize;
 
  constructor() {
    this._conn = new Sequelize(
      'database',
      'username',
      'password',
      {
        host: 'localhost',
        dialect: 'mysql'
      }
    );
  }
 
  async findUserByNickname(nickname: string): Promise<User> {
    const users = await this._conn.query(`SELECT * FROM users WHERE nickname = ${nickname}`, {
      type: QueryTypes.SELECT,
      raw: true
    }) as User[];
 
    return users[0];
  }
}

Acima, mostramos como a integração com o BD foi isolada em uma classe específica. Veja agora como fica a classe de Login:

import SQLUserGateway from './sql-user-gateway';
 
export class Login {
  async execute(nickname: string, password: string) {
 
    const userGateway = new SQLUserGateway();
 
    const user = await userGateway.findUserByNickname(nickname);
 
    if (user.password !== password) {
      throw new Error(""The given password is wrong. Try again"");
    }
 
    return ""Successfully logged"";
  }
} 

Apesar de cada classe possuir sua própria responsabilidade, ainda estamos ferindo o DIP, uma vez que a classe Login, um componente de alto nível,  possui uma dependência direta com a classe SQLUserGateway, um componente de baixo nível. Desta forma, qualquer mudança na integração com o BD, fato muito comum em projetos de software de longa duração, pode impactar na regra de negócio principal do login .
Para corrigir este problema, basta definir uma interface específica para a classe Login, contendo todos os métodos necessários para a execução do login (lembre do Princípio da Segregação de Interface). Assim, a classe Login não estará mais dependente de uma implementação concreta que poderia ser alterada a qualquer momento. Veja o código abaixo:

export interface User {
  nickname: string,
  password: string
};
 
export interface UserGateway {
  findUserByNickname(nickname: string): Promise<User>
}
 
export class Login {
  private _userGateway: UserGateway;
 
  constructor(userGateway: UserGateway) {
    this._userGateway = userGateway;
  }
 
  async execute(nickname: string, password: string) {
    const user = await this._userGateway.findUserByNickname(nickname);
 
    if (user.password !== password) {
      throw new Error(""The given password is wrong. Try again"");
    }
 
    return ""Successfully logged"";
  }
}

Repare que além de definirmos a interface para o gateway a ser utilizado na implementação da regra de negócio, não existe nenhuma relação de dependência do código com a classe que realiza a integração com o BD. Ao invés disso, o construtor da classe recebe um gateway do tipo UserGateway já instanciado. A classe SQLUserGateway, responsável por fazer a integração com BD, fica da seguinte forma:

import { Sequelize, QueryTypes } from 'sequelize';
import { User, UserGateway } from './login';
 
export default class SQLUserGateway implements UserGateway {
  private _conn: Sequelize;
 
  constructor() {
    this._conn = new Sequelize(
      'database',
      'username',
      'password',
      {
        host: 'localhost',
        dialect: 'mysql'
      }
    );
  }
 
  async findUserByNickname(nickname: string): Promise<User> {
    const users = await this._conn.query(`SELECT * FROM users WHERE nickname = ${nickname}`, {
      type: QueryTypes.SELECT,
      raw: true
    }) as User[];
 
    return users[0];
  }
} 

Agora é a classe SQLUserGateway que cria uma relação de dependência ao implementar a interface fornecida pela classe Login. O Princípio da Inversão de Dependência foi aplicado com sucesso, uma vez que a classe de mais baixo nível, SQLUserGateway, passou a depender da classe de mais alto nível, Login.

Para ilustrar como DIP foi aplicado no nosso exemplo, observe os dois diagramas abaixo, que representam o antes e o depois da relação de dependência entre os componentes:

Antes:

Depois:

Antes, a seta, que representa uma relação de dependência, sai do componente de alto nível, representado pela cor amarela, em direção ao componente de baixo nível, representado pela cor azul. Depois, após a aplicação do DIP, a seta pontilhada, que representa um relacionamento de implementação, sai do componente de baixo nível em direção ao componente de alto nível, invertendo a relação de dependência. A dependência do componente de alto nível com uma interface é um contrato com um componente abstrato, isto é, uma abstração. Por isso, mudanças nos componentes de baixo nível que implementam esse contrato não implicarão em necessidade de mudanças nos componentes de alto nível. 

Não foi mostrado no exemplo, mas em sistemas que aplicam o Princípio da Inversão de Dependência é necessária a criação de um componente cuja única responsabilidade é instanciar classes e injetá-las nos construtores dos componentes de alto nível. Geralmente são as classes “Main” do sistema, que ficam a cargo de gerar os containers da aplicação. O node.js possui um pacote muito bom para implementar injeção de dependência de forma automática. Caso tenha interesse, o nome dele é Awilix.

Considerações Finais

Como vimos, o DIP nos orienta a depender de abstrações e não de implementações, pois as abstrações mudam com menos frequência, além de facilitar a mudança de comportamentos e tecnologias e proporcionar uma melhor (mais fácil, mais rápida e mais suave) evolução do código.

O princípio se faz mais necessário quando estamos desenvolvendo aplicações com componentes com diferentes níveis de volatilidade. A aplicação do princípio não só irá manter o código mais limpo como também auxiliará em refatorações futuras.

Assim, finalizamos a série sobre S.O.L.I.D. Nos diga se gostou deste texto e se ele lhe foi útil? Você aplica os princípios S.O.L.I.D em seus projetos ou eles são novidades para você? Esperamos que tenha gostado da série. Até a próxima! 

Artigo escrito por Filipe Mata (Tech Lead na empresa Gerencianet). Revisado por Prof. Tiago Carneiro."
"S.O.L.I.D: Conceitos e exemplos em Typescript – Parte 4, o “I”",http://www2.decom.ufop.br/terralab/s-o-l-i-d-conceitos-e-exemplos-em-typescript-parte-4-o-i/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/04/SOLID-11-730x350.png,"Um bom engenheiro de software precisa conhecer os princípios designados pelas letras que formam o acrônimo S.O.L.I.D:

S – Single Responsibility Principle 
O – Open-Closed Principle 
L – Liskov Substitution Principle 
I – Interface Segregation Principle 
D – Dependency Inversion Principle 

Este é o quarto artigo de uma série inteiramente dedicada a estes princípios. Caso ainda não tenha lido os artigos anteriores da série, sobre as letras “S”, “O” e “L”, sugerimos a leitura antes que continue: Princípio da Responsabilidade Única, Princípio Aberto/Fechado e  Princípio da Substituição de Liskov, respectivamente. 

A seguir iremos aprofundar em nossa explicações sobre a letra “I” – Princípio da Segregação de Interface (The Interface Segregation Principle).

ISP – Princípio da Segregação de Interface

Este princípio foi proposto por Robert C. Martin, mais conhecido Uncle Bob, enquanto ele realizava uma consultoria para a empresa Xerox, algumas décadas atrás. Na época, a Xerox possuía um sistema de impressora que executava uma série de tarefas. O problema com esse sistema estava concentrado em uma única classe que continha uma série de responsabilidades, sendo assim utilizada por uma infinidade de tarefas. Dá pra ter uma ideia do receio que qualquer desenvolvedor desse sistema tinha ao realizar uma manutenção nessa classe. Qualquer bug no código, por menor que fosse, comprometeria uma boa parte do sistema.

Para demonstrar melhor este cenário, veja o diagrama abaixo:

Imagine que a classe OPS representa a superclasse da Xerox que fornecia uma série de operações para tarefas distintas. Neste cenário, a tarefa representada pelo componente Task1 precisaria somente da operação op1(), a tarefa Task2 somente da operação op2() e a tarefa Task3 somente da operação op3().

Agora, imagine que a classe OPS foi implementada em uma linguagem estaticamente tipada, como Java ou C++. O código da Task1 fica dependente das operações op2() e op3(), mesmo que estas operações não sejam executadas no fluxo da tarefa Task1. Assim, qualquer mudança no código do método op2() irá forçar as tarefas Task1 e Task3 a serem compiladas e reimplantadas, mesmo que os métodos utilizados por estas tarefas não tenham sido alterados.

Este era o cenário da Xerox na época, e a solução proposta por Robert Martin foi a aplicação de uma camada de interfaces entre a superclasse, exemplificada pela classe OPS, e as tarefas que executavam as operações fornecidas por esta classe, conforme ilustra a figura abaixo. Mais tarde essa solução ficou conhecida como o Princípio da Segregação de Interface.

Repare que agora cada tarefa do sistema possui sua própria interface, contendo somente os métodos que serão utilizados pela tarefa. Assim, a tarefa Task1 irá depender somente da interface Task1Ops, e consequentemente da operação op1(), que é a operação que realmente importa para a tarefa Task1. Portanto, qualquer mudança nos métodos op2() e/ou op3() na classe OPS não irá forçar uma recompilação e uma nova implantação (deploy) da classe Task1.

Exemplos Práticos

Imagine a seguinte interface:

export interface Carro {
  acelerar();
  frear();
}

Agora temos duas classes para implementar a interface Carro: 

export class Fiat implements Carro {
  acelerar() {
    console.log('O Fiat está sendo acelerada');
  }
 
  frear() {
    console.log('Você acabou de frear o Fiat');
  }
}
 
export class Ford implements Carro {
  acelerar() {
    console.log('O Ford está sendo acelerada');
  }
 
  frear() {
    console.log('Você acabou de frear o Ford');
  }
}

Até aqui tudo certo. Temos as classes Ford e Fiat implementando todos os métodos da interface Carro. Obrigatoriamente todo carro deve acelerar e frear. Nesse sentido, o ISP está sendo aplicado da maneira correta. 

Agora imagine que algum desenvolvedor teve a brilhante ideia de implementar um carro bem específico com funções bem peculiares. A primeira ação dele foi alterar a interface carro, como pode ser visto:

export interface Carro {
  acelerar();
  frear();
  viajarNoTempo();
}
 
export class DeLorean implements Carro {
  acelerar() {
    console.log('O DeLorean está sendo acelerado');
  }
 
  frear() {
    console.log('Você acabou de frear o DeLorean');
  }
 
  viajarNoTempo() {
    console.log('Vamos de volta para o futuro com o DeLorean');
  }
}

Aparentemente está tudo certo com a nova classe implementando o clássico DeLorean de “De volta para o futuro”, mas veja o que o Typescript nos alerta com relação às outras classes:

Aqui o compilador está nos forçando a implementar o método viajarNoTempo() nas classes Fiat e Ford, por mais que este comportamento não exista nestes tipos de carro. Assim, nosso inexperiente desenvolvedor teve outra brilhante (ou não) ideia de implementar este método de uma maneira bem estranha:

export class Fiat implements Carro {
  acelerar() {
    console.log('O Fiat está sendo acelerada');
  }
 
  frear() {
    console.log('Você acabou de frear o Fiat');
  }
 
  viajarNoTempo() {
    throw new Error(""Este comportamento não funciona para este carro"");
  }
}
 
export class Ford implements Carro {
  acelerar() {
    console.log('O Ford está sendo acelerada');
  }
 
  frear() {
    console.log('Você acabou de frear o Ford');
  }
 
  viajarNoTempo() {
    throw new Error(""Este comportamento não funciona para este carro"");
  }
}

 Nosso desenvolvedor não sabe, mas ele acabou de ferir o Princípio da Segregação de Interface. Ele fez isso no momento em que ele forçou as classes Fiat e Ford a implementar um método que elas não precisam. Pior, a implementação não faz sentido algum, já que ela só lança uma exceção.

O grande problema dessa implementação é que, ao longo do tempo, pode ser que o método viajarNoTempo() sofra algumas alterações como, por exemplo, receber um argumento referente ao ano para o qual o dono do carro deseja viajar. Todas as classes que implementam a classe Carro precisarão ser alteradas para satisfazer esta mudança, quando na verdade somente a classe DeLorean implementa, de fato, o método de viagem no tempo. Somado a isso, existe o fato de que, se este código estivesse escrito em linguagens como C++ ou Java, todas as classes que implementam a interface Carro deveriam ser re-compiladas. Tudo isso parece um trabalho bobo, mas imagine que ao invés de apenas 3 classes implementando a interface Carro, tenhamos 1000 ou 10000 classes.

Podemos resolver este problema segregando as interfaces do nosso exemplo, de modo que a interface Carro contenha somente os métodos acelerar() e frear(), e tenhamos mais uma interface responsável por disponibilizar somente comportamentos referentes a viagem no tempo. Veja o exemplo:

export interface Carro {
  acelerar();
  frear();
}
 
export interface MaquinaDoTempo {
  viajarNoTempo();
}

Agora, as classes Fiat e Ford não são obrigadas a implementar o método viajarNoTempo() ou qualquer outro que não esteja definido no contrato da interface Carro. No entanto, como o DeLorean é ao mesmo  tempo um carro e uma máquina do tempo, podemos fazer algo como a seguinte implementação:

export class DeLorean implements Carro, MaquinaDoTempo {
  acelerar() {
    console.log('O DeLorean está sendo acelerado');
  }
 
  frear() {
    console.log('Você acabou de frear o DeLorean');
  }
 
  viajarNoTempo() {
    console.log('Vamos de volta para o futuro com o DeLorean');
  }
}

 Ao realizar estas alterações voltamos a respeitar o ISP, já que isolamos as interfaces da aplicação através das diferenças de responsabilidade e comportamentos, e, portanto, passamos a ter interfaces específicas ao invés de interfaces genéricas.

Considerações Finais

Vimos que o Princípio da Segregação de Interface está relacionado a coesão de interfaces, definindo que classes ou componentes clientes não devem ser forçados a depender de métodos que eles não precisam. 

Vale ressaltar, também, que esse princípio está restrito ao tipo de linguagem que você está utilizando no desenvolvimento das suas aplicações. Se você está desenvolvendo em linguagens tipadas dinamicamente como Python ou Ruby, declarações de tipo e dependências de interface não existem no código fonte. Ao invés disso, elas são inferidas em tempo de execução e, portanto, não existem dependências de código que forcem a recompilação e um novo deploy. Por isso é que linguagens dinamicamente tipadas criam sistemas mais flexíveis e menos acoplados do que quando desenvolvemos em linguagens estaticamente tipadas como o Java. Assim, o ISP é mais vinculado a uma questão de linguagem do que de design.

Nos diga se gostou deste texto e se ele lhe foi útil? Você aplica os princípios S.O.L.I.D em seus projetos ou eles são novidades para você? Fique de olho em nossas redes sociais, o próximo artigo desta série irá se aprofundar no quinto e último princípio do acrônimo, o “D”, ou mais especificamente, o Princípio da Inversão de Dependência. Aguardamos você lá! 

Artigo escrito por Filipe Mata (Tech Lead na empresa Gerencianet). Revisado por Prof. Tiago Carneiro."
"S.O.L.I.D: Conceitos e exemplos em Typescript – Parte 3, o “L”",http://www2.decom.ufop.br/terralab/s-o-l-i-d-conceitos-e-exemplos-em-typescript-parte-3-o-l/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/04/SOLID-9-730x350.png,"Se seu objetivo atual é tornar-se um bom engenheiro de software, é essencial que você conheça os princípios designados pelas letras que formam o acrônimo S.O.L.I.D:

S – Single Responsibility Principle
O – Open-Closed Principle
L – Liskov Substitution Principle
I – Interface Segregation Principle
D – Dependency Inversion Principle

Este é o terceiro artigo de uma série inteiramente dedicada a estes princípios. Caso ainda não tenha lido os artigos anteriores da série, sobre as letras “S” e “O”, sugerimos a leitura antes que continue: Princípio da Responsabilidade Única e Princípio Aberto/Fechado, respectivamente.

A seguir iremos aprofundar em nossa explicações sobre a letra “L” – Princípio da Substituição de Liskov (Liskov Substitution Principle).

LSP – Princípio da Substituição de Liskov

Este pode ser considerado um dos princípios chaves do S.O.L.I.D, já que o mesmo define o conceito de tipos abstratos e interfaces, sendo, assim, crucial para a aplicação dos outros princípios do S.O.L.I.D.

A criadora do princípio é uma das maiores referência da Ciência da Computação no mundo, a doutora Barbara Liskov, a primeira a mulher a obter um PhD em Ciência da Computação nos Estados Unidos, e também conhecida por inventar o conceito de Tipo Abstrato de Dados (TAD), que é um intimamente ligado com o princípio LSP.

Em 1988, Liskov escreveu o seguinte para definir subtipos:

“O que é desejado aqui é algo como a seguinte propriedade de substituição: Se para cada objeto O1 do tipo S existe um objeto O2 do tipo T, de modo que, para todos os programas P definidos em termos de T, o comportamento de P fica imutável quando O1 é substituído por O2, então S é um subtipo de T.”

Apesar de complexo, o que este princípio diz é que, em uma aplicação um objeto de um dado tipo pode ser facilmente substituído por outro objeto de um tipo derivado sem qualquer tipo de impacto. Claramente estamos falando dos conceitos de abstração e definição de contratos por interface. Para demonstrar de uma melhor maneira este princípio, vamos imaginar um programa de um banco que exibe um relatório completo das tarifas de seus clientes. No relatório temos as informações de documento e tarifa do cliente. Veja o exemplo abaixo:

export interface Cliente {
  recuperarDocumento();
  calcularTarifa();
}

export class Program {
  private _clientes: Cliente[];

  constructor(clientes: Cliente[]) {
    this._clientes = clientes;
  }

  exibirRelatorio() {
    console.log('Documento | Tarifa');
    this._clientes.forEach((cliente) => {
      const documento = cliente.recuperarDocumento();
      const tarifa = cliente.calcularTarifa();

      console.log(`${documento} | ${tarifa}`);
    })
  }
}


Repare que o programa interage com um tipo que representa o cliente do banco. É esperado que o programa possa extrair o documento e calcular a tarifa desse cliente, independente de quais características específicas este cliente possua. Ao fazer isto, estamos fechando um contrato do programa que emite o relatório com a interface do tipo cliente. Assim, o programa consegue lidar com quaisquer tipos que respeitem o contrato da interface do cliente. A seguir podemos ver alguns exemplos de classe que representam tipos específicos de cliente, o cliente pessoa física e o cliente pessoa jurídica:

export class ClientePF implements Cliente {
  private _cpf: string;

  constructor(cpf: string) {
    if (cpf.length !== 11) {
      throw new Error(""O CPF de um Cliente pessoa física deve conter exatamente 11 dígitos"");
    }

    this._cpf = cpf;
  }

  recuperarDocumento() {
    return this._cpf;
  }

  calcularTarifa() {
    /** regras para calcular tarifa de um cliente PF */
  }
}

export class ClientePJ implements Cliente {
  private _cnpj: string;

  constructor(cnpj: string) {
    if (cnpj.length !== 14) {
      throw new Error(""O CNPJ de um Cliente pessoa jurídica deve conter exatamente 14 dígitos"");
    }

    this._cnpj = cnpj;
  }

  recuperarDocumento() {
    return this._cnpj;
  }

  calcularTarifa() {
    /** regras para calcular tarifa de um cliente PJ */
  }
}


Temos duas classes que representam tipos diferentes de clientes, cada uma com suas regras de negócio específicas, porém ambas respeitam o contrato da interface Cliente. Assim, o programa está preparado para receber em seu construtor qualquer uma das duas implementações.

Neste exemplo utilizamos o conceito de interfaces para demonstrar o LSP, no entanto poderíamos facilmente tê-lo demonstrado através do uso de tipo abstrato de dados, como foi feito no artigo anterior.

Violação do LSP – Problema do Quadrado/Retângulo

Um quadrado é um retângulo? Este é um caso clássico de violação do LSP. Um programa interage com um tipo abstrato de dados, Retângulo, que fornece uma interface com métodos para editar a altura e a largura do retângulo. Bem, um quadrado também possui largura e altura, então poderíamos definir um tipo Quadrado como subtipo de Retângulo, como é mostrado no diagrama abaixo. Mas seria que poderíamos mesmo?

Na verdade, Quadrado não é um subtipo de Retângulo, pois a altura e largura de um retângulo podem mudar de forma independente. O mesmo não acontece com um quadrado, já que, obrigatoriamente, a largura e altura de um quadrado devem ter sempre o mesmo valor. Uma vez que o programa irá interagir com o tipo Retângulo, algumas coisas podem dar errado:

class Programa {
  private _retangulo: Retangulo;

  constructor(retangulo: Retangulo) {
    this._retangulo = retangulo;
  }

  exec() {
    this._retangulo.setLargura(2);
    this._retangulo.setAltura(5);

    console.log('Resultado: ', this._retangulo.getArea() === 10); // Verdadeiro ou Falso?
  }
}


Se passarmos uma instância de Quadrado no construtor do programa, como mostra o código abaixo, o resultado a ser exibido na tela será falso.

let retangulo: Retangulo = new Quadrado();
let p = new Programa(retangulo);
p.exec();


Algo que poderia ser feito, neste caso, é adição de uma condição no programa para verificar se o retângulo recebido no construtor é um quadrado ou não. Porém, uma vez que o comportamento do programa depende do tipo que ele está recebendo no construtor, estes tipos não são substituíveis e, portanto, é uma violação clara do LSP.

Você pode estar pensando que uma alternativa válida seria sobrescrever o método getArea() na classe Quadrado. No entanto, ainda que este método fosse sobrescrito, isto não impediria que o programa definisse a altura e a largura do quadrado de forma independente. Pior, se o método que getArea() levasse em consideração somente uma propriedade do retângulo, altura ou largura, não faria sentido criar um setter para a propriedade não aproveitada. Nos dois casos, o programa imprimiria falso, pois a área do quadrado não seria igual a 10, se utilizasse a largura como parâmetro válido a área calculada seria 4 e se utilizasse a altura a área seria 25.  Isso deixa mais evidente que Quadrado não é um subtipo de Retângulo.

Considerações Finais

O Princípio da Substituição de Liskov é um dos mais fantásticos do S.O.L.I.D, pois, como vimos, é através dele que conseguimos alcançar maior flexibilidade na construção de aplicações, através da substituição de tipos, com uso de tipos abstratos de dados ou contratos por interface. É um princípio de suma importância para entendimento e aplicação dos outros princípios do S.O.L.I.D.

Nos diga se gostou deste texto e se ele lhe foi útil? Você aplica os princípios S.O.L.I.D em seus projetos ou eles são novidades para você? Fique de olho em nossas redes sociais, o próximo artigo desta série irá se aprofundar no terceiro princípio do acrônimo, o “I”, ou mais especificamente, o Princípio da Segregação de Interfaces. Aguardamos você lá!

Artigo escrito por Filipe Mata (Tech Lead na empresa Gerencianet). Revisado por Prof. Tiago Carneiro."
"S.O.L.I.D: Conceitos e exemplos em Typescript – Parte 2, o “O”",http://www2.decom.ufop.br/terralab/s-o-l-i-d-conceitos-e-exemplos-em-typescript-parte-2-o-o/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/04/SOLID-6-730x350.png,"Se você ainda não ouviu falar do acrônimo S.O.L.I.D e, principalmente, se o seu objetivo atual é tornar-se um bom engenheiro de software, é essencial que você conheça cada um dos princípios designados pelas letras que formam esse acrônimo:

S – Single Responsibility Principle 
O – Open-Closed Principle 
L – Liskov Substitution Principle 
I – Interface Segregation Principle 
D – Dependency Inversion Principle 

Este é o segundo artigo de uma série inteiramente dedicada aos princípios S.O.L.I.D. Caso ainda não tenha lido o primeiro artigo da série, sobre a letra “S”, sugerimos  a leitura antes que continue: Princípio da Responsabilidade Única 

A seguir iremos aprofundar em nossa explicações sobre a letra “O” – Princípio Aberto/Fechado (Open/Closed Principle).

OCP – Princípio Aberto/Fechado

Este princípio foi criado em 1988 pelo acadêmico francês Bertrand Meyer, criador da linguagem de programação Eiffel e do conceito de Design por contrato. Ele diz:

“Um artefato de software deve ser aberto para extensões, mas fechado para modificações”

Na rotina de trabalho de um engenheiro de software, construir aplicações novas é quase sempre visto como algo muito bom, mas nem sempre esse olhar se aplica quando se fala em manutenção de código.

Provavelmente, em algum momento, você já deve ter ouvido algum relato sobre um grupo de desenvolvedores que já esteve em apuros para dar manutenção em códigos de aplicações legadas. Isto é, código de aplicações que foram desenvolvidas no passado (às vezes muito distante) e que continuam a atender as necessidades de seus usuários. 

A manutenção de aplicações legadas acontece porque, muitas vezes, seu código deveria manter suas regras de negócio intactas ao longo do tempo, mas acaba sofrendo uma série de modificações para atender as inúmeras mudanças de requisitos e escopos que comumente ocorrem no ciclo de vida de um software. E aí, o que era mil maravilhas no início do projeto, acaba virando um inferno depois de alguns anos ou até mesmo meses de trabalho.

O problema não está nas mudanças de requisitos ou escopo, até porque este tipo de situação é mais do que comum no mundo do desenvolvimento de software. O problema em geral reside na maneira como estas alterações são refletidas no código. Se o seu sistema possui uma classe com única responsabilidade (lembre o SRP), que já funciona perfeitamente bem, entregando exatamente aquilo que ela foi proposta a entregar, no momento em que você acrescenta um função nessa classe para atender a outro requisito que nada tem a ver com o propósito da classe, você não só atribui mais uma responsabilidade a classe como também corre o risco de comprometer o funcionamento das regras de negócio originais da classe. É aí que o OCP, ou Princípio Aberto/Fechado, deve entrar em ação.

Um bom design de aplicação deve oferecer padrões que possibilitem que o desenvolvedor realize o menor número de manutenções possíveis. Isto pode ser feito através de dois conceitos chave: Extensão e Abstração.

A ideia é que, se a aplicação possui uma classe, por exemplo, que já tem suas regras bem definidas, ao receber novas demandas de requisitos que fogem do escopo da classe, porém estão intimamente interligados com o que a classe faz, nós iremos estender o comportamento dessa classe em classe mais específicas, de modo que a classe estendida abstraia as funcionalidades que serão implementadas nas classes concretas. Para ficar mais claro, observe o código abaixo:

import { promises as fs } from 'fs';
 
interface ReportRegister {
  workedHours: number;
  employeeName: string;
}
 
export class HourReport {
  private reportData: Array<ReportRegister>;
 
  constructor(data: Array<ReportRegister>) {
    this.reportData = data;
  };
 
  async export() {
    let content: string = 'Name | Worked Hours\n';
 
    this.reportData.forEach((reportReg) => {
        content = content + `${reportReg.employeeName} | ${reportReg.workedHours}\n`;
    });
 
    const currentDateTime = new Date().getTime();
 
    await fs.writeFile(`./report-${currentDateTime}.txt`, content, 'utf8');
  }
}


Neste código temos uma classe que representa um relatório de horas trabalhadas por um funcionário (lembre-se da classe HourReporter do artigo anterior).

Nesta classe é possível observar um método para exportação do relatório no formato de um arquivo de texto. Isto aconteceu porque, provavelmente, o Departamento Pessoal da empresa solicitou essa funcionalidade. Porém, depois de um tempo, a Diretoria de Operações percebe que o relatório no formato de texto não é o mais adequado para os padrões da empresa. Será necessário gerar relatórios no formato PDF.

Nesse sentido, a primeira ideia que pode vir na cabeça do desenvolvedor é alterar o método export() da classe HourReport, de modo que o mesmo passe a manipular e escrever arquivos PDF ao invés de arquivos em formato de texto. 

A ideia parece simples e até poderia resolver o problema da Diretoria de Operações, porém alguns fatores devem ser levados em conta:

O desenvolvedor precisou fazer alterações em um código que já funciona para atender a um requisito que pode mudar a qualquer momento. Assim, se na hora de implementar a integração com a biblioteca de manipulação de PDF o desenvolvedor deixou algum erro passar, todas as regras de negócio envolvendo a estrutura e os dados do relatório estariam comprometidas também.
Hoje a Diretoria de Operações deseja um relatório no formato PDF, mas pode ser que amanhã ou depois ela queira retomar os relatórios no formato TXT bem como solicitar relatórios no formato DOC. A cada vez que um novo formato de documento for solicitado pela Diretoria de Operações, o desenvolvedor terá que fazer alterações na classe HourReport, correndo o risco de inserir uma falha na mesma.

Para resolver este problema, podemos aplicar os conceitos de Extensão e Abstração para atender o OCP. Veja qual seria a melhor maneira de escrevermos a classe HourReport.

export abstract class HourReport {
  protected reportData: Array<ReportRegister>;
 
  constructor(data: Array<ReportRegister>) {
    this.reportData = data;
  };
 
  toString() {
    let content: string = 'Name | Worked Hours\n';
 
    this.reportData.forEach((reportReg) => {
        content = content + `${reportReg.employeeName} | ${reportReg.workedHours}\n`;
    });
 
    return content;
  }
 
  /** 
   * ...
   * Aqui podem ter outros métodos que implementam regras de negócio específicas de relatórios de horas
   * ...
   */
 
  abstract export();
}


Repare que isolamos as regras de negócio mais críticas referente a criação e manipulação de relatórios dentro da classe HourReport, que agora é uma classe abstrata. Assim, definimos que o método export, que representa um requisito que tende a mudar com mais frequência, passa a ser um método abstrato.

Tudo que precisamos fazer agora é criar extensões para a nossa abstração, ou seja, plugins que implementem o método export() de maneiras diferentes, atendendo as demandas da Diretoria de Operações. Veja como pode ser implementado o plugin para gerar relatórios de horas no formato TXT:

import { promises as fs } from 'fs';
 
export class TxtHourReport extends HourReport {
 
  async export() {
    const currentDateTime = new Date().getTime();
    await fs.writeFile(`./report-${currentDateTime}.txt`, this.toString(), 'utf8');
  }
}


O mesmo pode ser feito para qualquer outro formato que a Diretoria de Operações queira. Veja os exemplos abaixo:

export class DocHourReport extends HourReport {
  
  async export() {
    /** Manipula e escreve um novo arquivo no formato .docx */
  }
}
 
export class PDFHourReport extends HourReport {
  
  async export() {
    /** Manipula e escreve um novo arquivo no formato .pdf */
  }
}


Com os plugins criados, a aplicação irá instanciar a classe específica baseada na escolha de formato do usuário da aplicação. Assim, se o analista do Departamento Pessoal solicitar um relatório no formato PDF, a aplicação irá instanciar a classe PDFHourReport. 

Considerações Finais

Como visto acima, a aplicação do Princípio Aberto/Fechado – OCP nos direciona a um design que separa as regras de negócio mais críticas da aplicação daqueles comportamentos que tendem a sofrer modificações frequentes. Esta separação se dá através da criação de plugins para as regras de negócio da aplicação. Assim, sempre que houver necessidade de implementação de uma funcionalidade que se adeque às estas regras de negócio, como um novo formato de relatório, no exemplo acima, iremos criar um plugin para esta nova funcionalidade, ao invés de alterar um código que já funciona. Isso garante mais qualidade e maior facilidade na manutenção futura de códigos.

Nos diga se gostou deste texto e se ele lhe foi útil? Você aplica os princípios SOLID em seus projetos ou eles são novidades para você? Fique de olho em nossas redes sociais, o próximo artigo desta série irá se aprofundar no terceiro princípio do acrônimo, o “L”, ou mais especificamente, o Princípio da Substituição de Liskov. Aguardamos você lá!

Artigo escrito por Filipe Mata (Tech Lead na empresa Gerencianet). Revisado por Prof. Tiago Carneiro."
"S.O.L.I.D: Conceitos e exemplos em Typescript – Parte 1, o “S”",http://www2.decom.ufop.br/terralab/s-o-l-i-d-conceitos-e-exemplos-em-typescript-parte-1-o-s/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/03/SOLID-3-730x350.png,"Para você que não conhece, S.O.L.I.D é um acrônimo para 5 princípios de Orientação a Objeto que definem o que é, de fato, um bom design de aplicação. 

Estes princípios foram definidos nos anos 2000 por Robert C. Martin e Michael Feathers, no intuito de promover técnicas e conceitos que facilitassem o desenvolvimento e manutenção de sistemas complexos de computação.

Caso você não conheça, Robert C. Martin, mais conhecido como Uncle Bob é uma das referências em engenharia e arquitetura de software no mundo, sendo um dos assinantes do Manifesto Ágil, além de criador de conceitos consagrados na comunidade como Clean Code e Clean Architecture. Não distante, Michael Feathers, também é reconhecido como um dos grandes nomes do mundo do desenvolvimento de software, foi, de fato, o criador do acrônimo S.O.L.I.D.

Se você já ouviu falar de Clean Code e Clean Architecture saiba que estes princípios formam a pedra angular de todos estes conceitos. Assim, se o seu objetivo é tornar-se um bom engenheiro de software, é de suma importância o entendimento destes princípios antes de aprofundar em qualquer modelo arquitetural mais rebuscado. 

Cada letra do acrônimo representa os seguintes conceitos:

S – Single Responsibility Principle (Princípio da Responsabilidade Única)
O – Open-Closed Principle (Princípio Aberto/Fechado)
L – Liskov Substitution Principle (Princípio da Substituição de Liskov)
I – Interface Segregation Principle (Princípio da Segregação de Interface)
D – Dependency Inversion Principle (Princípio da Inversão de Dependência)

Neste artigo, iremos aprofundar no primeiro princípio, fazendo uso de alguns exemplos práticos em Typescript do que deve e do que não deve ser feito em designs que tenham a pretensão de seguir o Princípio da Responsabilidade Única, ou SRP. 

SRP – Princípio da Responsabilidade Única

“Cada módulo de uma aplicação deve ter uma, e somente uma, razão para mudar”

Erroneamente muitos programadores, ao se depararem com este princípio, assumem que cada módulo deve realizar apenas uma ação.Isto ocorre pois existe um conceito que fala que cada função de uma classe deve fazer apenas uma coisa. Do ponto de vista da refatoração de sistemas isto está correto, porém não é o que o Princípio da Responsabilidade Única é.

Primeiramente devemos entender que um módulo nada mais é do que a maneira que os códigos da sua aplicação são armazenados ou agrupados. Pode ser por arquivo ou simplesmente através de funções ou estruturas de dados. 

Cada módulo do seu sistema é implementado para satisfazer as necessidades de um determinado grupo de usuários ou stakeholders (qualquer pessoa interessada no projeto). Essas necessidades geralmente são definidas na forma de requisitos e este grupo específico de usuários são os atores do sistema.

Uma vez que estes conceitos estejam claros, podemos entender que o SRP define que cada módulo da sua aplicação, feito exclusivamente para um ator específico do sistema, deve mudar apenas para atender as necessidades deste ator. Se um determinado módulo se propõe a satisfazer as necessidades de dois atores distintos, o SRP será quebrado, já que ao satisfazer as mudanças de requisitos de um ator os requisitos do outro ator podem ficar comprometidos. O módulo deixa de ter apenas uma responsabilidade.

Abaixo é mostrado um exemplo clássico de violação do SRP:

class Employee {
  public calculatePay() {}
  public reportHours() {}
  public save() {}
}


A classe Employee representa os funcionários de uma determinada empresa. Nela podemos observar 3 métodos atendendo 3 necessidades distintas:

O método calculatePay(), que tem por objetivo calcular o pagamento do funcionário, atende a uma demanda do setor financeiro da empresa, ou seja, neste caso, a classe Employee é responsável por mudanças requisitadas por um ator que representa a diretoria de finanças da da empresa.
O método reportHours(), responsável por gerar relatórios de horas trabalhadas ao departamento pessoal, atende aos requisitos estabelecidos por um ator que representa a diretoria de operações da empresa.
O método save(),  que define como um funcionário é cadastrado na empresa, é especificado por DBAs, já que os mesmos são as pessoas mais hábeis para garantir a persistência dos dados de forma performática. Assim, a classe Employee passa a ser responsável por mudanças requisitadas pela diretoria de tecnologia da empresa.

A classe Employee possui 3 responsabilidades distintas e com isso alguns problemas podem aparecer. Se, por exemplo, a diretoria de finanças especificar uma mudança na forma como as horas trabalhadas são calculadas, para suprir a uma mudança no cálculo do pagamento, o relatório de horas pode ser gerado com erros para a diretoria de operações. 

Algo similar acontece quando, por exemplo, a diretoria de tecnologia resolve definir uma nova forma de armazenamento para os dados do funcionário. Neste cenário, uma mudança no método save(), pode acarretar em modificações indesejadas em toda a classe e, consequentemente, em resultados errados tanto para a diretoria de finanças quanto para a diretoria de operações.

Uma das soluções para este problema é definir uma classe EmployeeData, sem métodos, contendo apenas as propriedades de um funcionário. Tal classe seria compartilhada entre 3 classes distintas, cada uma sendo responsável por apenas um dos diferentes atores do nosso exemplo. Veja o exemplo abaixo:

class EmployeeData { }
 
class PayCalculator {
  public calculatePay(data: EmployeeData) {}
}
 
class HourReporter {
  public reportHours(data: EmployeeData) {}
}
 
class EmployeeRepository {
  public saveEmployee(data: EmployeeData) {}
}


Agora as classes PayCalculator, responsável por atender os requisitos da diretoria de finanças, HourReporter, responsável por atender os requisitos da diretoria de operações, e EmployeeRepository, responsável por atender as especificações da diretoria de tecnologia, são isoladas uma das outras. Assim, mudanças em qualquer uma das classes não impactarão em nada nas outras classes e, portanto, cada classe do nosso exemplo passam a  possuir uma e somente uma responsabilidade.

A única desvantagem dessa solução é a quantidade de instâncias que precisamos criar para iniciar a aplicação do exemplo. Para contornar este problema pode-se utilizar o padrão de projeto de software “Facade” , como mostrado no exemplo abaixo:

class EmployeeData { }
 
class PayCalculator {
  public calculatePay(data: EmployeeData) {}
}
 
class HourReporter {
  public reportHours(data: EmployeeData) {}
}
 
class EmployeeRepository {
  public saveEmployee(data: EmployeeData) {}
}
 
class Employee {
  private data: EmployeeData;
  private payCalculator: PayCalculator;
  private hourReporter: HourReporter;
  private employeeRepository: EmployeeRepository;
 
  constructor(data: EmployeeData) {
    this.data = data;
    this.payCalculator = new PayCalculator();
    this.hourReporter = new HourReporter();
    this.employeeRepository = new EmployeeRepository();
  }
  
  public calculatePay() {
    return this.payCalculator.calculatePay(this.data);
  }
 
  public reportHours() {
    return this.hourReporter.reportHours(this.data);
  }
 
  public save() {
    return this.employeeRepository.saveEmployee(this.data);
  }
}


Repare que voltamos com a classe Employee para o nosso exemplo, com uma estrutura bem similar àquela mostrada no início. No entanto, agora, ela tem o  único propósito de ser uma fachada para as outras classes. A classe Employee possui 3 métodos, mas ela delega as regras de negócio mais críticas, ou seja, suas responsabilidades, para as classes PayCalculator, HourReporter e EmployeeRepository. 

Considerações Finais

Como visto no exemplo, o Princípio da Responsabilidade Única – SRP nada tem a ver com o número de funções ou métodos que um componente ou classe possui, mas sim com o número de responsabilidades que o mesmo tem. Cada uma das classes do nosso exemplo possui uma e somente uma razão para mudar e este é o propósito do SRP.

Nos diga se gostou deste texto e se ele lhe foi útil? Você aplica os princípios SOLID em seus projetos ou eles são novidades para você? Fique de olho em nossas redes sociais, o próximo artigo desta série irá se aprofundar no segundo princípio do acrônimo, o “O”, ou mais especificamente, o Princípio Aberto/Fechado. Aguardamos você lá! 

Artigo escrito por Filipe Mata (Engenheiro de Software na empresa Gerencianet). Revisado por Prof. Tiago Carneiro."
Como simular a AWS no seu computador e utilizar o serviço de filas SQS,http://www2.decom.ufop.br/terralab/como-simular-a-aws-no-seu-computador-e-utilizar-o-servico-de-filas-sqs/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/03/Como-simular-a-AWS-no-seu-computador-e-utilizar-o-servico-de-filas-SQS-9-730x350.png,"Introdução

A AWS (Amazon Web Services) é uma plataforma de serviços de computação em nuvem oferecida pela amazon.com. Seus produtos, oferecidos no sistema de pay-as-you-go, são utilizados globalmente tanto por indivíduos e empresas de pequeno porte, quanto por multinacionais e governos. Atualmente, a AWS fornece mais de 200 diferentes serviços e produtos, incluindo serviços de armazenamento, redes, analytics, mobile e IoT. Alguns dos mais populares são: EC2, S3, lambda e SQS. 

É muito comum que aplicações que executam na AWS sejam divididas em diversos componentes. Assim, cada componente tem uma responsabilidade específica dentro da aplicação. Na verdade, esta é uma boa prática de Engenharia de Software, utilizada na maioria dos projetos de software, pois permite que cada componente seja desenvolvido de forma independente, por uma equipe dedicada, que evolua em seu próprio ritmo e seja fracamente acoplado aos demais componentes que formam a arquitetura desta aplicação. Componentes podem ser entendidos como módulos executáveis de código, no Windows seriam distribuídos arquivos executáveis (.exe) ou como DLL (Dynamic Link Library) e, no sistema operacional Linux, seriam distribuídos como arquivos do tipo “.so” (Shared Object) .

 Em aplicações onde se deseja robustez e alto desempenho é comum que, para promover o fraco acoplamento entre os componentes, a comunicação entre eles ocorra por meio de  filas de mensagens colocadas na entrada e/ou na saída de cada componente.  Os serviços de fila trazem os seguintes benefícios:

Resiliência: A aplicação pode continuar funcionando mesmo que um ou mais componentes falhem, as mensagens continuarão a ser processadas pelos componentes ativos até que os demais sejam restabelecidos;
Escalabilidade: A aplicação pode escalar de acordo com a demanda, isto é, a quantidade de mensagens na fila determina o número de réplicas de componentes que serão utilizadas para consumí-las. Assim, evita-se que as mensagens esperem muito para serem atendidas e o desempenho de toda aplicação degrade;
Segurança: O desacoplamento dos componentes traz segurança, já que não há requisições diretas entre eles; e
Equilíbrio: A assincronicidade no atendimento às mensagens possibilita que um componente muito rápido não sobrecarregue outro mais lento.

Este artigo tem como objetivo introduzir você ao serviço de fila SQS oferecido pela nuvem AWS. No entanto, outras nuvens oferecem serviços similares que operam segundo os mesmo princípios e possuem a mesma missão. Assim, mesmo que você esteja interessado(a) em outras nuvens, também será beneficiado(a) pela leitura deste artigo. Além disso, mostraremos como simular a AWS localmente, em seu computador pessoal, para que você possa aprender a utilizar o SQS em seus projetos.

O serviço Simple Queue Service (SQS) 

O serviço Amazon Simple Queue Service oferece sistema de filas seguras, duráveis e de alta disponibilidade que permite integrar e desacoplar sistemas de software e componentes distribuídos. As principais vantagens da utilização desse serviço são:

Segurança: Você controla quem pode enviar e receber mensagens em uma fila do Amazon SQS; 
Disponibilidade: O uso de infraestrutura redundante fornece acesso altamente simultâneo às mensagens e alta disponibilidade para produzir e consumir mensagens;
Tutorial

A seguir mostramos os passos que devem ser seguidos para a instalação e configuração do Localstack para simulação da nuvem AWS com o serviço SQS. Para isso, partimos do pressuposto de que você está utilizando o sistema operacional Linux.

Instalando o Localstack

Para fazer uma demonstração de uma aplicação com troca de mensagens, usaremos o localstack. Este projeto fornece um framework para teste de aplicações em nuvem (no momento, para as aplicações da AWS). O jeito mais fácil de instalá-lo é utilizando pip (os outros jeitos são detalhados no repositório do github). Os requisitos necessários são python 2 ou 3, pip e Docker.

pip install localstack

Após a instalação, é necessário executar o localstack…

localstack start

Agora, vamos utilizar o aws-cli para criar uma fila pelo terminal. Este cliente pode ser instalado por qualquer gerenciador de pacotes (apt, pip): 

https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html

Obs: Se nunca tiver configurado a AWS no computador, tente chamar aws configure e setar quaisquer valores (com uma region válida), como por exemplo:

AWS Access Key ID: key-id
AWS Secret Access Key: secret-ak
Default region name: us-east-1
Default output format None

Após a instalação do cliente, crie uma fila para teste. Ao passar o endpoint como localhost, direcionamos a chamada para o localstack que, neste momento, já deve estar executando em seu computador.

aws --endpoint=""http://localhost:4566"" sqs create-queue --queue-name=test_queue

O retorno de sucesso é:

{
    ""QueueUrl"": ""http://localhost:4566/000000000000/test_queue""
}
Desenvolvendo uma aplicação simplificada

Vamos criar uma aplicação simples com dois módulos que se comunicam através da nossa fila. O componente 1 recebe uma mensagem através da linha de comando e a envia através da fila.

 
const AWS = require('aws-sdk');
const queueUrl = 'http://localhost:4566/000000000000/test_queue';
 
const sendMessage = async (messageBody = 'empty') => {
   const sqs = new AWS.SQS({region: 'us-east-1'});
   const a = await sqs.sendMessage({
       QueueUrl: queueUrl,
       MessageBody: messageBody
   }).promise();
};
 
sendMessage(process.argv[2]);

O segundo componente fará a leitura da fila esperando alguma mensagem.

const AWS = require('aws-sdk');
const queueUrl = 'http://localhost:4566/000000000000/test_queue';
 
const run = async () => {
   const sqs = new AWS.SQS({region: 'us-east-1'});
 
   while(true) {
       try {
           const a = await sqs.receiveMessage({
               QueueUrl: queueUrl,
               WaitTimeSeconds: 20
           }).promise();
           console.log('Message received: ', a.Messages[0].Body);
 
           await sqs.deleteMessage({
               QueueUrl: queueUrl,
               ReceiptHandle: a.Messages[0].ReceiptHandle
           }).promise();
       } catch (err) {
           console.log(err);
           break;
       }
   }
};
 
run();

Para fazer o teste, iniciar o componente 2 na linha de comando com o comando a seguir:

node second-component.js

Depois, envie uma mensagem  para testá-lo, utilizando o comando a seguir:

node first-component.js ""aqui vai a mensagem""
Usando mais de dois componentes

Em alguns casos, pode haver necessidade de um componente enviar a mesma mensagem para mais de um outro componente. Podemos fazer a chamada do envio mais de uma vez, porém, isso se torna inviável quando o sistema aumenta de tamanho.

Para fazer isso de forma natural, contornando o problema de aumento de tráfego de mensagens, podemos usar outro serviço da AWS: O  Simple Notification Service (SNS). Vamos direcionar a mensagem a um tópico do SNS, que irá entregar para todos os componentes inscritos no mesmo. Esse padrão de envio de um para muitos é muitas vezes chamado de “Fan Out” Design Pattern.

Conheça o serviço de notificação simples (SNS)

O Amazon Simple Notification Service (SNS) é um serviço gerenciado que provê a entrega de mensagens de publicadores (publishers) para subscreventes (subscribers), em acordo com (Publisher-Subscriber Design Pattern). Os publicadores comunicam-se de forma assíncrona com os subscreventes enviando mensagens a um tópico no qual os subscreventes estarão inscritos.
Para  utilizar o SNS, conectando 2 componentes, vamos criar uma fila para o segundo componente e para um novo tópico, no qual as duas filas estarão inscritas.

Criação da segunda fila:

aws --endpoint=""http://localhost:4566"" sqs create-queue --queue-name=another_test_queue

Criação do novo tópico:

aws --endpoint=""http://localhost:4566"" sns create-topic --name=test_topic

O retorno de sucesso deve ser:

{
    ""TopicArn"": ""arn:aws:sns:us-east-1:000000000000:test_topic""
}

Para preparar as filas para receber mensagens enviadas àquele tópico, faça:

aws --endpoint=""http://localhost:4566"" sns subscribe --topic-arn=arn:aws:sns:us-east-1:000000000000:test_topic --protocol=sqs --notification-endpoint=http://localhost:4566/000000000000/test_queue 
&&
aws --endpoint=""http://localhost:4566"" sns subscribe --topic-arn=arn:aws:sns:us-east-1:000000000000:test_topic --protocol=sqs --notification-endpoint=http://localhost:4566/000000000000/another_test_queue

Após estes passos, basta enviar as mensagens no tópico e todas as aplicações inscritas nele receberão a mesma mensagem simultaneamente.

O código fonte do segundo exemplo pode ser visto em  https://github.com/bermr/sqs-demo.

Considerações Finais

Neste artigo, explicamos as motivações e os passos necessários para o uso do sistema de filas da AWS, o serviço SQS, e para o uso do sistema de distribuição de notificações, o SNS. Para ensinar a prática, nós também discutimos o código fonte de uma aplicação exemplo que utiliza esses serviços com o objetivo de obter uma comunicação rápida e resiliente. Convidamos você a deixar um feedback sobre o texto e contar quais são as suas experiências com essa arquitetura de software.

Fontes
https://aws.amazon.com/
https://en.wikipedia.org/wiki/Amazon_Web_Services
https://docs.aws.amazon.com/pt_br/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html
https://docs.aws.amazon.com/sns/latest/dg/welcome.html
https://github.com/localstack/localstack
https://github.com/bermr/sqs-demo

Artigo escrito por Bernardo Marotta de Rezende (Tech Lead na empresa Gerencianet). Revisado por Prof. Tiago Carneiro."
Serviços de software oferecidos de graça para estudantes,http://www2.decom.ufop.br/terralab/servicos-de-software-oferecidos-de-graca-para-estudantes/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/03/Servicos-de-software-oferecidos-de-graca-para-estudantes-730x350.png,"Diversas empresas oferecem alguns de seus produtos de maneira gratuita para estudantes. É uma relação de benefício mútuo. O estudante pode aproveitar um serviço de qualidade e que pode ajudá-lo durante sua graduação, sem ter custos. Já as empresas, por sua vez,  contribuem na formação dos beneficiários, estimulam a reciprocidade e impulsionam suas marcas. Elas gradativamente promovem e ampliam a comunidade de usuários destes produtos.

Pensando nisso, mostraremos neste artigo 4 serviços pagos que podem ser obtidos de maneira gratuita por estudantes. Para cada um deles, apresentaremos quais ferramentas são oferecidas e faremos um breve tutorial de como um estudante pode obtê-las de forma gratuita.

GitHub Student Developer Pack

O GitHub é uma plataforma de hospedagem e versionamento de código amplamente conhecida entre a comunidade de desenvolvedores de software. Buscando oferecer para estudantes um grupo de serviços amplamente utilizados nessa área, a empresa criou o GitHub Student Developer Pack. Neste pacote, são oferecidas assinaturas gratuitas das mais diversas plataformas voltadas para a área de desenvolvimento. Entre elas estão: Heroku, Digital Ocean, Unity, Bootstrap Studio, Canva e GitHub Pro.

Para obter acesso a esse pacote de serviços, você deve ter uma matrícula ativa em alguma instituição de ensino. Para iniciar seu cadastro, você deve possuir uma conta no GitHub. Caso não possua, crie uma utilizando seu email institucional. Caso já possua, mas sua conta não esteja vinculada a seu email institucional, você deve fazer isso acessando as configurações de sua conta e procurando a seção “Emails”, assim como na figura abaixo.

Com um email institucional já vinculado a sua conta, acesse o site GitHub Student Developer Pack. Nesse site, clique no botão “Get Your Pack”. A partir desse momento, você será redirecionado para a seguinte página:

Na parte inferior dela, há um formulário no qual você deverá responder às perguntas, que são referentes à sua instituição de ensino. Após submeter as respostas, você já terá seu acesso garantido. Para resgatar seus benefícios, basta acessar a página Student Offers.

Pacote Office

O Pacote Office é um conjunto de aplicativos criado pela Microsoft. Ele é composto de programas voltados para as mais diversas funções como gerenciamento de banco de dados, manipulação de planilhas e edição de texto. Seus serviços são amplamente utilizados nas instituições de ensino e no mercado.

O que muitos não sabem é que a Microsoft disponibiliza esse pacote de graça para estudantes. O Office Student é a versão do pacote completamente online e gratuita para alunos. 

Para obter essa versão, você deve acessar o site Office Student e inserir seu email institucional no campo destacado na imagem abaixo.

Após realizar esse passo, você será redirecionado para uma página na qual você deve preencher um formulário. Logo após concluí-lo, seu acesso ao pacote Office estará liberado.

Microsoft Azure for Students

O Azure é um dos serviços de computação em nuvem mais conhecidos, e oferece mais de 200 produtos que são distribuídos, em sua maioria, na forma de software como serviço (SaaS). Apesar dele permitir o uso de diferentes tipos de linguagens e sistemas operacionais, o seu principal diferencial é o suporte a sistemas criados com a plataforma .Net e Windows.

O programa Microsoft Azure for Students permite que você utilize alguns dos serviços da nuvem de graça por 1 ano e ainda lhe oferece US$100 (cem dólares) para aprender e testar algumas soluções mais robustas. Além disso, a plataforma te oferece cursos e certificações gratuitas que podem fazer a diferença em qualquer entrevista de emprego na área de TI.

Para fazer parte do programa é necessário ter um e-mail institucional ativo da sua universidade, e se cadastrar passando pela fase de Verificação de Identidade por Telefone e a Verificação Acadêmica.

Depois disso, você só precisa esperar pelo email da Microsoft lhe avisando da liberação da conta. Você também pode participar através do GitHub Student Developer Pack.

AWS Educate

A plataforma AWS é a maior fornecedora de serviço de computação em nuvem disponível, e possui diversos servidores espalhados pelo mundo. Através do AWS Educate a plataforma pretende educar e formar profissionais para atender o mercado de trabalho gerado pelos serviços em nuvem. Estima-se que, ao redor do mundo, este tipo de serviço em nuvem gera cerca de 18 milhões de empregos.

Dentro da plataforma é possível realizar cursos sobre machine learning, cybersecurity, data science e muitos outros, todos com oferecendo certificações ao final. Além disso, após concluir alguns cursos, podemos nos candidatar a muitas vagas de emprego oferecidas dentro da própria AWS Educate.

Esta plataforma não é feita somente para estudantes, existem formas de conseguir acesso como professor, instituição ou recrutador. Para entrar como estudante você deve ter um e-mail institucional e estar devidamente matriculado em alguma universidade.

Depois dessa etapa é só esperar um email da AWS Educate e você poderá utilizar a plataforma.
Neste artigo mostramos para você 4 recursos que podem ser obtidos de graça por estudantes. Gostou do texto? Ele foi útil para você? Conte para gente nos comentários!

Tutorial escrito por Carlos Magalhães Silva e Vinícius de Paula Silva. Revisado por Prof. Tiago Carneiro."
As Revoluções Industriais e a Evolução do Guia SCRUM,http://www2.decom.ufop.br/terralab/as-revolucoes-industriais-e-a-evolucao-do-guia-scrum/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/03/As-Revolucoes-Industriais-e-a-Evolucao-do-Guia-SCRUM-730x350.png,"De fato o planeta passou por várias revoluções, talvez um termo que soa melhor atualmente seria adaptações. Começaremos fazendo um breve histórico das revoluções industriais que estiveram presentes nos momentos mais importantes da humanidade. Esses ocorridos estruturam as empresas no presente e futuro. Vamos iniciar com um breve histórico das principais mudanças que ocorreram da primeira à quarta evolução da indústria.

Indústria 1.0

Ocorreu por volta de 1784 e foi baseada em mecanismo usando água e metal.

Indústria 2.0

Acontece quase 100 anos depois, em torno de 1870, introduzindo produção em massa com a ajuda de energia elétrica.

Indústria 3.0

Um pouco mais tarde, já nas décadas de 60 e 70 inicia-se a revolução digital com o uso de eletrônicos e tecnologia da informação para automatizar a produção.

Indústria 4.0

E nesse exato momento passamos pela quarta revolução industrial com sistemas cibernéticos e físicos, com algoritmos inteligentes, se conectando através da internet.

Existem sinais do ambiente atual que demonstram que precisamos de algum tipo de mudança, adaptabilidade ou flexibilidade. No contexto das revoluções podemos citar 5 deles. 

 Atualmente temos uma alta competitividade na maioria dos setores, causados por empresas startups, geralmente com menos de 20 anos, que são rápidas, possuem um cenário de menor número de colaboradores, com um perfil jovem e engajado. Elas possuem a estrutura organizacional mais horizontal, que muitas vezes supera os sistemas altamente hierárquicos, promovendo a tomada de decisão de forma mais ágil e comunicação mais fluida. 

Depois disso, as novas tecnologias sempre ganham espaço, até mesmo a comunicação e decisões a serem tomadas podem ser revolucionadas com novas ferramentas ou algoritmos auxiliares para uma determinada estratégia. O objetivo é dar ao talento individual uma forma de maximizar a sua produtividade. Um quarto momento é a necessidade de nos mantermos sempre conectados, essa conexão passa por uma alta adaptação de garantia de estarmos sempre ligados uns aos outros, permitindo tomadas de decisão simplesmente pelo uso de alguma plataforma. 

Por fim, o gerenciamento em escala, que abre uma comunicação honesta e sem medo para as futuras lideranças conseguirem “servir” as suas equipes. Com isso um ambiente de bastante transparência, mudança de projetos está sendo preparado para absorver a Indústria 4.0.

Nesse novo contexto, a organização precisa ser robusta, com planejamentos estratégicos bem definidos. O que significa que terá um plano mas poderá adaptá-lo a cada instante, sempre promovendo uma cultura e engajamento das pessoas. 

Tudo isso faz sinergia com a perspetiva do framework Scrum, desde a sua essência e ainda mais agora com sua atualização em novembro de 2020. O que apresentaremos aqui é simplesmente um breve resumo de como o guia Scrum está em termos de princípios, valores, papéis, artefatos e as suas principais adaptações em relação a 2017.

A íntegra do que iremos abordar você pode consultar no próprio site da Scrum.org, onde também é possível consultar várias postagens a respeito das mudanças que ocorreram, sob a perspectiva de várias abordagens. 

O seu propósito e definição mantém a base original de ser um framework leve para ajudar pessoas e organizações a gerenciar suas entregas de valor, através de adaptações para solucionar problemas complexos.

Em termos teóricos ele se baseia em três pilares, transparência, inspeção e adaptação. Ou seja, todos os papéis, eventos, artefatos, propósitos e valores, são para dar visibilidade ao trabalho em questão, permitir uma validação antes, durante e depois do processo e a qualquer momento realizar ajustes nas demandas a serem executadas.

Os valores se baseiam em comprometimento, foco, respeito, abertura e coragem. Todos eles são essenciais para a revolução industrial que estamos passando. Essa ideia de certa forma ultrapassa a teoria, chegando a ser uma necessidade para o mundo moderno.

O time Scrum entende o Dono do Produto, como um representante do cliente diante da organização, de partes interessadas e do próprio time. O Scrum Master, na posição de um líder servidor e facilitador da análise de negócio e de processos diante de todas as partes envolvidas. E por fim o time de desenvolvimento, autônomo e multifuncional para executar as demandas.

Os eventos Scrum também seguem a tradição:

Sprint: Intervalo de tempo onde uma parte do projeto é executada
Sprint Planning: Encontro onde se deve buscar responder – O que? Pra quê? Por que? Como? Quem? Se deve fazer determinado conjunto de demandas.
Daily Scrum: Um evento periódico, na grande parte diariamente, onde o time revisa a meta e analisa a estratégia de execução do que foi planejado.
Sprint Review: Momento em que o time scrum, analisa o que foi entregue, e apresenta em termos de produto os itens construídos e rapidamente levantam os possíveis débitos técnicos.
Sprint Retrospective: De uma forma mais geral todos analisam, processo, comportamento, entregas, satisfação, desempenho e qualquer outro assunto que acharem pertinente. Em busca de pontos fortes e de melhoria para manter a evolução contínua do time.

Os artefatos se conectam e também não mudaram:

Product Backlog: O produto ou projeto como um todo, a entrega por completo, o que realmente irá dar visibilidade e garantir uma entrega de valor sólida e viável.
Sprint Backlog: A demanda escolhida pelo time a cada intervalo de tempo (sprint) é parte mínima do Product Backlog.
Increment: A entrega realizada somada a todas as entregas anteriores, ou seja, todo o conjunto pronto do Product Backlog.

Por fim, vamos trazer as 8 principais mudanças do Scrum, pois de forma analítica e interpretativa, alguns analistas vão dizer que mudou e outros não. Portanto é somente um apanhado optativo que estamos apresentando. Vamos lá:

A versão de 2017 tinha 19 páginas e a de 2020 tem 13. Com isso nota-se que ele foi literalmente reescrito, com o intuito de ficar mais leve e possibilitar que outras áreas, que não sejam Engenharia de software, possam utilizá-lo.
Adição do Objetivo do Produto. O time agora terá um objetivo claro e de grande valor, para orientar. Mesmo que cada sprint tenha uma meta, uma missão maior deverá ser perseguida.
Três compromissos em vez de um. Objetivo do Produto para o Product Backlog. Objetivo da sprint para o sprint backlog. Definição de Feito para o incremento.
Development Team agora é somente Developers, o termo team foi removido com a ideia de dar foco a um objetivo. O que remete a ideia de não ter sub-times nem hierarquia entre os membros.
A estrutura da daily não tem mais as três perguntas. Não que não possa ter, porém o estilo do encontro é livre limitado a 15 minutos de uma reunião de status do andamento em direção aos objetivos.
O time scrum se auto organiza literalmente. Dispensando a necessidade de gerentes e o dono do produto é como um integrante do time.
A Sprint Planning terá que responder a três questões. Por que esse sprint é viável? O que pode ser feito na sprint? Como será feito o trabalho?
Os papéis estão distribuídos por responsabilidades em vez de uma descrição detalhada do trabalho.

De fato, o que sabemos é que o planeta está em constante mudança de revoluções industriais, um framework de processo e uma rotina de trabalho que de uma hora para outra exige uma necessidade do trabalho ser totalmente em casa. A projeção é que o engajamento,  a comunicação objetiva, o apoio e liderança servidora serão como um guia para as adaptações.

Artigo escrito Pedro Saint Clair Garcia. Revisado por Prof. Tiago Carneiro.

REFERÊNCIAS

[1] Guia do Scrum, disponível em https://www.scrum.org/resources/scrum-guide SCRUM Guide. Acessado em 16/08/2020.

[2] Princípios e Práticas para Desenvolvimento de Software com Produtividade, disponível em https://engsoftmoderna.info/, acessado 16/08/2020, Versão atual: 2020.1.4 – ISBN: 978-65-00-01950-6 (impresso) e 978-65-00-00077-1 (e-book)."
Workshop TerraLAB 2021/1,http://www2.decom.ufop.br/terralab/workshop-terralab-2021-1/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/03/workshop-terralab-2021_1-17-730x350.png,"No dia 6 de março de 2021, sábado, a partir das 8 horas da manhã,  o TerraLAB irá realizar um Workshop com o tema: “Carreiras e práticas atuais na indústria de software”. Serão 12 palestras divididas em 30 minutos de apresentação e 10 minutos de perguntas e respostas. 

É necessário fazer a inscrição neste formulário para participar. O evento será realizado na plataforma Google Meet. O link da sala será enviado por e-mail aos inscritos. Para acessá-la, deve-se utilizar o e-mail institucional da UFOP. 

Os nossos palestrantes são profissionais de vasta experiência e reconhecidos pelo mercado de Tecnologia da Informação e Desenvolvimento de Software.

Tiago Carneiro que atua como  Professor Associado e é coordenador do laboratório TerraLAB do Departamento de Computação (DECOM) da UFOP
Vitor Paiva que atua como Engenheiro de software Sênior na empresa Gerencianet
Amanda Pinto Founder da empresa DevOps BootCamp
Conrado Carneiro que atua como CEO e é fundador da empresa Usemobile
Filipe Santos que atua como Engenheiro de Software Pleno na empresa Gerencianet
Lucas Uchôa que atua como Team Lead e Engenheiro Android na empresa iFood
Glícia Braga que atua como Supervisora de recursos na empresa Gerencianet
Pedro Garcia que atua como Agilista na empresa Take.Blip
Francisco Costa que atua como Developer na empresa Tembici
Gabriel Machado que atua como Feature Lead e Engenheiro de Software Sênior na empresa Nubank
Paulo Henrique Mendonça que atua como CIO na empresa Stilingue
João Vitor Mattos que atua como Diretor de Tecnologia na empresa Cachaça Gestor

A empresas que estarão nos apoiando e oferecerão palestras no evento são, portanto:

Gerencianet:  A Gerencianet é uma uma conta digital focada em negócios, para que empreendedores possam emitir e gerenciar recebimentos. Atualmente, é responsável pela emissão de mais de 60 milhões de cobranças anuais e possui 210 mil clientes cadastrados em todo o Brasil. 
Origine:  A Origine é uma empresa com o foco de desenvolver uma cultura de inovação e melhoria da gestão e de processos, por meio da mentalidade ágil e de métodos mais eficientes e eficazes para entrega de produto ou serviço e mais geração de valor para o cliente.
Usemobile: A Usemobile nasceu em 2015, na cidade de Ouro Preto.  É uma empresa especialista no desenvolvimento de aplicativos mobile, para Android e iOS, aplicações web e desktop com o foco de oferecer soluções inteligentes e adequadas para pessoas e empresas que precisam de tecnologia.
iFood: O iFood é a maior foodtech da América Latina. Com 8 anos de atuação, tem o propósito de revolucionar o universo da alimentação por uma vida mais prática e prazerosa. Atualmente, o iFood atende milhões de usuários no Brasil e possui uma ampla rede de restaurantes e mercados parceiros. O iFood conta com a participação da Movile – líder global em marketplaces móveis – e da Just Eat.
Take.Blip: Há 21 anos, a Take.Blip é destaque no mercado latino-americano de tecnologia mobile e mensageria. Além disso,  a empresa é uma das candidatas a atingir o status de unicórnio segundo o estudo Corrida dos Unicórnios 2021. Em 2016, lançou uma plataforma de desenvolvimento de chatbots. Tal plataforma ajudou grandes empresas a inserirem suas marcas nos aplicativos de mensagens. Recentemente a Take.Blip foi considerada a 2ª melhor empresa para trabalhar em Minas Gerais pela Great Place to Work (2020), a 11ª melhor empresa média para trabalhar no Brasil (2020) e a 15º melhor empresa para trabalhar no segmento de Tecnologia (2020).
Tembici: A Tembici é líder na América Latina em tecnologias para micromobilidade que criam soluções para inspirar uma revolução do espaço urbano. A empresa utiliza tecnologia e engaja parceiros públicos, privados e a sociedade civil para que o convívio das pessoas com as cidades seja mais eficiente, inteligente e agradável.  Em 2021, a Tembici foi apontada como uma das startups brasileiras candidatas a atingir o status de unicórnio pelo estudo Corrida dos Unicórnios 2021.
Nubank: O Nubank é uma fintech que desenvolve soluções simples, seguras e 100% digitais para a vida financeira de seus clientes. Hoje, a empresa é o maior banco digital independente do mundo e conta com mais de 20 milhões de clientes em todo o Brasil. O Nubank busca ter processos justos e transparentes na conduta, diretos e objetivos na comunicação, e tratamos cada cliente como uma pessoa. Contra burocracia, papelada, agências e centrais de atendimento caras e ineficientes. 
Stilingue: A Stilingue é uma plataforma com Inteligência Artificial brasileira, que escuta, resume e prioriza a voz do cliente em um só lugar, fazendo com que marcas fiquem um passo à frente, enxerguem oportunidades e melhorem as experiências de seus consumidores. A tecnologia é admirada pelos mais modernos Departamentos de Marketing e Customer Service da América Latina, mais precisa que grandes nomes estrangeiros de Inteligência Artificial e a mais usada no Brasil, segundo pesquisa dos profissionais de redes sociais. Recentemente a Stilingue anunciou a captação de recursos no valor de R$18 milhões, liderada pela DGF Investimentos. O valor será destinado a inovação em produtos, com o desenvolvimento de novos módulos e o aprimoramento da plataforma.
Cachaça Gestor: A Cachaça Gestor fornece um sistema computacional que visa tornar a vida do produtor de cachaça mais eficiente. O objetivo da empresa é aumentar a produtividade do empreendimento de seus clientes e garantir um controle melhor e mais dinâmico dos processos de produção e gestão. O sistema também fornece informações importantes para o negócio, como dicas para boas práticas na produção de uma cachaça de qualidade e orientações sobre a produção da bebida e certificação.
DevOps Bootcamp: O DevOps Bootcamp é um projeto de capacitação, mentoria em educação corporativa e de profissionais de tecnologia, focada em inovação, automação, segurança, gestão e aplicação das melhores ferramentas de DevOps e Cloud Native do mercado. A empresa reconhece que a Tecnologia da Informação é o caminho alcançar a excelência e satisfazer as necessidades mais desejadas das organizações em tecnologia e seus profissionais, levando a automação dos processos.

O Workshop é também a primeira atividade obrigatória para os inscritos no Processo Seletivo Trainee 2021/1, que permanece aberto até dia 05 de março. 

Há oportunidades para estudantes de todos os cursos da UFOP. Em 2020, capacitamos 22 estudantes e 95% deles já foram absorvidos por empresas.

Venha prestigiar o evento! Venha fazer parte do nosso time!!!!.

Para se inscrever no Workshop TerraLAB 2021 clique aqui.

Para se inscrever no Processo Seletivo Trainee 2021 clique aqui."
Aberto o processo Trainee TerraLAB 2021/1,http://www2.decom.ufop.br/terralab/aberto-o-processo-trainee-terralab-2021-1/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/02/Programa-trainee-terralab-2021_1-10-730x350.png,"Inscrições de 22 de fevereiro a 5 de março de 2021

Aprenda as tecnologias mais pedidas no mercado e desenvolva suas habilidades trabalhando com profissionais experientes e qualificados. Você desenvolverá aplicativos mobiles e web, ambos integrados a serviços na nuvem! Dentre outras tecnologias, você vai ter contato com React, React Native, Node JS, Jest, Cucumber e Selenium, Apium, AWS e mais.

Graduandos e pós-graduandos da UFOP: Aumente a sua chance de conseguir uma vaga de emprego e domine as tecnologias utilizadas na indústria de software.

Nosso objetivo é aumentar sua empregabilidade e, ao mesmo tempo, reduzir o custo e tempo da seleção e treinamento de recursos humanos por parte da indústria.

Os estudantes serão capacitados em um ambiente profissionalizante, similar ao de uma fábrica de software, onde o estudante pode vivenciar os papéis existentes no ecossistema dessa indústria.

Nominalmente, os estudantes podem vivenciar os seguintes papéis: Analista de Negócio, Gerente de Projeto, Analista de Sistema, Engenheiro de Teste, Engenheiro de Software, Engenheiro DevOps, Designer de UX, Cientista de dados, Analista de Geoprocessamento, Analista de TI, Análise de marketing, Analista contábil e Analista jurídico.

Coordenação: O laboratório TerraLAB é coordenado pelo professor Dr. Tiago Garcia de Senna Carneiro, do Departamento de Computação (DECOM) da Universidade Federal de Ouro Preto (UFOP).

Mentoria: Os estudantes selecionados receberão mentoria de profissionais experientes que atuam em empresas parceiras. Estas empresas, de antemão, expressaram interesse em contratar, em médio prazo, os estudantes que apresentarem bom desempenho na realização das atividades propostas nesta iniciativa.

Processo seletivo: Após o período de inscrição, no dia 6 de março, sábado, faremos um workshop onde as empresas parceiras apresentarão seus cases e ambientes de desenvolvimento aos estudantes inscritos. Depois, os estudantes terão aproximadamente 5 semanas para realizarem auto-treinamento por meio dos cursos online oferecidos pelas empresas parceira sobre as atuais tecnologias por elas utilizadas para o desenvolvimento de frontends (web e mobile), backends (em nuvem) e testes (web, mobile e backend). Durante esse tempo, serão propostos desafios para os estudantes, que devem utilizar os conhecimentos adquiridos para resolvê-los. Um deles será o desenvolvimento de um aplicativo, utilizando-se das tecnologias descritas acima.

Pré-requisitos: É essencial que os candidatos sejam pessoas automotivadas e comprometidas com o sucesso dos projetos com os quais se envolvem. Apesar dos projetos pedirem demandas técnicas específicas, os candidatos receberão treinamento e mentoria para desenvolvimento de suas tarefas. No ambiente de fábrica de software, é clara a necessidade de profissionais que atuam para apoiar o processo de produção de software e que não são programadores, este é o caso do Analista de TI que mantém a rede de comunicação e servidores linux em funcionamento, além do Designer Gráfico que cuida das peças (imagens, ícones, telas e textos) de comunicação associadas aos produtos e o Gerente de Projetos que tem como incumbência administrar os resultados, prazos, custos e recursos dos projetos. Então, se você ainda não é um programador no paradigma orientado por objetos, pense em nos ajudar desempenhando um desses papéis.

Inscrição: De 22 de fevereiro a 5 de março de 2021. Os interessados devem enviar currículo LATTES e fornecer todas as informações solicitadas no seguinte formulário .

Bolsa: Neste semestre, os estudantes deverão se voluntariar e receberão como contrapartida a mentoria e o treinamento nas tecnologias supracitadas. Após esse período de tempo, os estudantes com bom desempenho poderão receber bolsas para a continuidade dos projetos que iniciaram neste semestre.

Carga Horária: 20h/semanais

Atuação remota: Todas as atividades que serão realizadas durante o processo seletivo ocorrerão de forma remota. Os encontros serão síncronos, via Google Meet, após o término do período de inscrição.

Número de vagas : Não há um número máximo de vagas em aberto, todos os estudantes que tiverem bom desempenho no processo seletivo serão convidados a tomar parte nesta iniciativa. No passado, o laboratório TerraLAB já teve 36 colaboradores atuando simultaneamente em diversos projetos.

As inscrições para o processo seletivo podem ser feitas do dia 22 de fevereiro ao dia 5 de março de 2021.

A proposta está aberta aos estudantes de todos os cursos de graduação e pós-graduação da UFOP.

A primeira atividade obrigatória do processo seletivo é a presença durante todo o Workshop TerraLAB que irá ocorrer no dia 6 de março de 2021, sábado. 

Ajude-nos a divulgar essa iniciativa!"
5 extensões para melhorar a produtividade no Visual Studio Code,http://www2.decom.ufop.br/terralab/5-extensoes-para-melhorar-a-produtividade-no-visual-studio-code/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/02/Group-10-730x350.jpg,"Na rotina de um desenvolvedor, o retrabalho pode tornar as suas atividades um pouco cansativas e improdutivas ao ter que identificar erros simples quando precisa-se realizar entregas dentro do prazo. Isso pode causar grandes problemas, por isso é necessário buscar ferramentas que possam auxiliar no desenvolvimento como um todo, buscando agilidade e produtividade nos processos.

Pensando nesse assunto, este artigo tem como objetivo apresentar 5 extensões que facilitam a vida do desenvolvedor, além de tornar o trabalho ainda mais prazeroso. Falaremos sobre a função de cada extensão, e logo após um breve tutorial de como instalar no VS Code, de forma rápida e fácil.

Primeiro você precisa ter o visual studio code instalado na máquina, se ainda não utiliza, você pode fazer o download no site oficial, clicando aqui.

Obs.: Baixe sempre a versão estável do software.
Após realizar a instalação do VS code ou caso já utilize, você precisa abrir o programa e selecionar o botão extensões, no painel esquerdo do ambiente, no qual irá surgir outro painel contendo todas as extensões disponíveis, conforme a imagem abaixo:

Será por aqui que iremos pesquisar as nossas extensões e instalar cada uma no editor de textos.

1ª Javascript (ES6) Code Snippets

Essa extensão se torna essencial para todo desenvolvedor front-end, pois fornece trechos não só do código JavaScript como também do Vue, React e Type Script e HTML com muita praticidade. 
Pesquise pelo nome javascript code snippets e selecione a primeira opção. Ao lado vai aparecer toda a documentação da extensão, que pode ser consultada sempre que tiver dúvidas. Para instalar no editor de texto, basta clicar em install, e pronto, já pode utilizar os recursos em seu código.

2ª – Color HighLight

Essa extensão também serve para quem atua como front-end, pois a mesma pode ajudar na verificação das cores, que vai ser criado no código do CSS e Sass. Para isso, basta que digite os caracteres alfanuméricos referentes a cor desejada, para que o sistema sinalize qual a cor o código digitado se refere.

Confira mais sobre a extensão aqui.

3ª – Dracula Official

Para poder personalizar o ambiente de desenvolvimento, muitos optam por utilizar temas que alteram todo o visual do editor, sendo o Drácula um dos mais utilizados pela comunidade. Para instalar essa extensão, basta pesquisar por Drácula Official e selecionar a primeira opção:

Após a instalação, o seu código ficará conforme a figura abaixo, trazendo uma melhor visibilidade e facilidade para encontrar qualquer variável ou comando através da cor indicada.

É possível escolher cores variadas de acordo com o gosto de cada Desenvolvedor.   Confira mais sobre a extensão aqui. 

4º – Tabnine

O Tabnine foi criado com base em um modelo de inteligência artificial, que usa Machine Learning para sugerir o que provavelmente será digitado, em tempo real. Essa extensão fornece uma pequena parte do código-fonte reutilizável (snippets), além de reconhecer o código de qualquer linguagem. A mesma mostra também a probabilidade do uso do snippet.

Após instalado, você deve tentar codificar algo, para que o sistema retorne as possíveis sugestões de continuidade do código.

Confira mais sobre a extensão aqui.

5º – Debugger for Chrome

Essa extensão é ideal para verificar linha por linha até encontrar qualquer bug, que esteja difícil de resolver. A mesma está voltada para desenvolvedores front-end, e permite usar o Debugger do Chrome integrado ao VS code. Pode ser executada ao mesmo tempo e na mesma tela do editor, sem que haja a necessidade de abrir o navegador para isso.

Essa extensão é restrita à linguagem Javascript e depois de instalada, basta definir os pontos de interrupção dentro do seu código e executá-lá pressionando o botão debug ou F5 para poder depurar o seu código, linha por linha.

Você pode encontrar a documentação completa clicando neste link.
Essas e outras extensões podem ser encontradas no próprio site do Visual Studio. Certamente com o uso dessas extensões, aliadas a outros tipos de métodos e técnicas, você será muito mais produtivo e terá um visual agradável pra poder trabalhar no dia a dia.

E você tem alguma dica de extensão para usar no VS Code? Deixe seu comentário, compartilhe e contribua com o nosso blog!

Referências

Marketplace de extensões do Visual Studio Code

Debugger for Chrome

10 extensões para aumentar a sua produtividade no Visual Studio Code

Top Extensões Úteis para VSCode – 2020

Artigo escrito por Walisson Farias França e revisado por Carlos Magalhães Silva e Prof. Tiago Carneiro."
Como o TerraLAB e suas equipes são estruturados?,http://www2.decom.ufop.br/terralab/como-o-terralab-e-suas-equipes-sao-estruturados/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2021/02/Como-o-TerraLAB-e-suas-equipes-sao-estruturados_-1-730x350.png,"Existem diversas maneiras de se organizar uma equipe de desenvolvimento de software. Não existe “a melhor maneira” de estruturar equipes de software. Muitas vezes a natureza do projeto tem impacto direto sobre esta estrutura, como é fácil imaginar que equipes de desenvolvimento de jogos possuem papéis/funções muito diferentes de equipes de desenvolvimentos de sistema de informação geográfica ou de desenvolvimento de simuladores. 

Por outro lado, a estrutura das equipes têm impacto direto no fluxo de trabalho e informações. Ao estruturar uma equipe é importante ter em mente a escalabilidade do processo de desenvolvimento de software, isto é, é sempre bom planejar uma estrutura que permita à equipe aumentar sua produtividade pela simples adição de novos membros. Contudo, quanto maior a equipe, maiores são as chances de falhas na comunicação acontecerem, papéis centralizadores podem se tornar gargalos desse processo e impedir a almejada escalabilidade. 

É neste contexto que este artigo tem como objetivo apresentar como a equipe do TerraLAB está estruturada, atualmente. É provável que o entendimento  desta estrutura interesse aqueles estudantes e profissionais que estejam planejando construir a sua própria startup. A equipe do laboratório se estrutura conforme mostra a figura abaixo.

Estrutura Geral da Equipe

Figura 1. Estrutura da Equipe do laboratório TerraLAB.

A equipe se divide em duas dimensões transversais em esquadrões (SQUADs)  ou em capítulos (CHAPTERS). Nos capítulos, as pessoas são agrupadas por suas habilidades, cada capítulo é um grupo de especialistas em uma área específica do conhecimento. Na indústria de desenvolvimento de  software as equipes são comumente agrupadas nos capítulos de experiência do usuário (UX – User eXperience), de desenvolvimento (DEVS – Development), de garantia de qualidade (QA – Quality Assurance), de operações (DEVOPS – Development and Operations) e de agilidade (SM – Scrum Master). Alguns nichos de atuação poderão demandar outros capítulos como os capítulos de geoprocessamento, de modelagem e de jogos.  Tudo realmente depende do ramo de atuação desta equipe. Cada capítulo possui um líder  (CHAPTER LEAD) que é responsável por auxiliar tecnicamente os demais colaboradores de seu capítulo e lhes oferecer feedbacks sobre sua atuação. Estes líderes devem manter alinhamento e atender às demandas gerência técnica (HEAD ENGENHARIA).  

Na rotina de trabalho, quando algum desafio surge e a equipe precisa ser mobilizada para produzir uma solução em resposta a esse desafio, primeiro, identifica-se as habilidades necessárias para o desenvolvimento desta solução, depois, monta-se um esquadrão selecionando pessoas dos diferentes capítulos e entrega-lhes a missão de vencer o desafio. Assim, que desafio é vencido o esquadrão se desfaz. É comum que os colaboradores participem de mais de um esquadrão, afinal muitos desafios podem demandar mais esforço de algumas especialidades que de outras. 

Por outro lado, nos esquadrões, é comum existir integrantes de diferentes habilidades. Na imagem, os esquadrões se organizam verticalmente. Os esquadrões devem se auto-organizar para desenvolver uma solução que atenda às necessidades do cliente que a encomendou. Somente deixando esse cliente satisfeito ele terá cumprido sua missão.  Muitas vezes, o cliente é externo à empresa na qual a equipe trabalha e, outras vezes, o cliente é interno.  Independente da origem do cliente, uma coisa é certa, muitas vezes o cliente não sabe exatamente o que ele quer ou o que ele  precisa, são coisas muito diferentes. É comum que o cliente vá respondendo esta pergunta a si mesmo à medida que experimenta versões evolutivas das soluções desenvolvidas pelos esquadrões. Isto pode ser um problema, visto que para cumprir sua missão é crucial que qualquer esquadrão receba especificações exatas sobre o desafio que precisa ser vencido, sobre quais requisitos a solução a ser construída precisa satisfazer e sobre as que restrições tecnológicas, legais ou negociais são impostas a esta solução. É no meio deste impasse que, tanto para guiar os clientes quanto para informar esquadrões que surgem os donos de produtos – os Product Owners. 

Cada desafio a ser vencido pertence a um Product Owner (PO) que dele deve conhecer todos os detalhes. Para isso, os POs devem se alinhar com seus clientes, entender profundamente suas necessidades, gerir suas expectativas e intermediar todas as comunicações entre os clientes e os esquadrões que os atenderão, fazendo que a solução produzida convirja rapidamente para algo que satisfaça as necessidades reais do cliente. Para os esquadrões, os POs são os clientes, pois os POs precisam aprovar todas as entregas feitas pelos esquadrões antes que elas cheguem aos verdadeiros clientes. Para um cliente, o PO é a equipe! Isto significa, se a solução que esse cliente recebeu não o satisfez, então, toda a equipe falhou! Portanto, para desempenhar bem o seu papel, um PO precisa se comunicar bem no jargão do cliente para evitar ao máximo ruídos de comunicação e, então, traduzir as demandas do cliente para o esquadrão. No sentido complementar, o PO também precisa ser capaz de conversar tecnicamente com o esquadrão de forma a especificar com muita clareza aquilo que precisa ser desenvolvido e, também, para ser capaz de entender os impedimentos e restrições técnicas ou tecnológicas encontradas pela equipe e, então, explicá-las para o cliente. Desta maneira, os POs são peças cruciais na engrenagem das equipes de desenvolvimento de software, traduzindo todas as comunicações entre clientes e esquadrões.

Há ainda uma outra peça neste quebra-cabeça. Uma vez que qualquer solução consome recursos e tem prazos estimados para seu desenvolvimento, os POs também precisam se manter alinhados com os gerentes de projetos (PM – Project Manangers). Estes últimos possuem como principais responsabilidades controlar prazos e custos, alocando recursos com eficácia.  Os gerentes de projeto são catalisadores, devem fazer com que o fluxo de trabalho flua da maneira mais eficiente e eficaz possível, sem desperdícios e gerando resultados. Eles precisam resolver todas as questões não técnicas que estejam impedindo ou atrasando o desenvolvimento de uma solução. Eles determinam o ritmo dos trabalhos, engajando, motivando, tranquilizando e harmonizando as equipes. Gerentes de projetos e donos de produto estabelecem uma relação cliente-fornecedor.  À medida que o gerente de projetos, no papel de cliente, cobra dos donos de produto entregas de qualidade dentro dos prazos  e custos estabelecidos, os donos de produto solicitam e justificam recursos que os gerentes de projeto devem adquirir e aplicar com sabedoria. Assim, donos de produtos e gerentes de projetos precisam se manter alinhados com o cliente sobre questões de naturezas diferentes, uma técnica e outra negocial.

Uma coisa a mais é importante ressaltar sobre o funcionamento de um esquadrão. Apesar dos integrantes terem atribuições únicas, realizar as tarefas para o desenvolvimento de uma solução, isto é, realizar a missão, é dever compartilhado por todos. Ninguém deve ficar à toa se há algo que precisa ser feito para atingir a conquista! Ninguém fica ocioso enquanto houver tarefa pendente! Por isso, todos os integrantes de um esquadrão devem sempre buscar o autodesenvolvimento, de maneira que em algum tempo, eles se habilitem a realizar qualquer tarefa de desenvolvimento e operação e, então, se tornem o tipo de profissional que o mercado busca com avidez: fullstack e praticante da cultura Devops!  

Finalmente, na dimensão do capítulo que se estrutura horizontalmente na figura acima,  os colaboradores são agrupados com seres pares em habilidades. Nesta dimensão, todos os desenvolvedores podem se ajudar tirando dúvidas com colegas e fortalecendo o conhecimento instalado no capítulo, muitas vezes um colega pode cobrir a ausência do outro ou reforçar sua presença. É um ambiente onde todos colaboram e co-evoluem.

 No TerraLAB, além dos papéis já citados (UX, DEV, QA, PO, DEVOPS e SM) e que comumente aparecem nas equipes de desenvolvimento de software, há o capítulo formado por especialistas em Geoinformática. Isto é um reflexo do nicho de atuação do laboratório junto a seus parceiros. Porém, há ainda outras duas gerências, Jurídica e de Marketing, que oferecem suporte a todos os esquadrões e os esforços de seus membros são compartilhados por todas as demandas do laboratório.  Abaixo, falaremos um pouco mais sobre estas funções. Clique nos links a seguir caso você queira saber um pouco mais sobre as funções de:

UX(O que é User Experience e quais etapas compõe o seu processo)
PO(A vida de Product Owner não é fazer piada)
PM(O papel e os desafios do gerente de projetos)
O que faz a equipe de Geoprocessamento?

A equipe de geoprocessamento é responsável de dotar os produtos e serviços do TerraLAB de Inteligência Espacial, isto é, eles devem ser capazes de desenvolver funcionalidades que permitam que nossos software transformem dados espaciais (mapas, imagens de satélite, dados GPS) em informações úteis a seus usuário. Para isso, a equipe deve conhecer bem os conceitos da GeoInformática e dominar os métodos e ferramentas computacionais disponíveis nos Sistemas de Informação Geográfica (SIG), como os aplicativos QGIS, TerraView, ArcGIS e Spring.

Mas além disto, é de especial interesse do TerraLAB que sua equipe de Geoprocessamento seja capaz de programar para que possam desenvolver inovações e estender as funcionalidades presentes nos atuais SIGs pagos ou gratuitos. Por meio de pequenos programas chamados scripts, essa equipe deveria ser capaz de reutilizar as funcionalidades disponíveis em bibliotecas de código encontradas na Internet para criar novos métodos e ferramentas computacionais de interesse de um projeto ou cliente/parceiro.

O que faz a equipe de Marketing?

A equipe de marketing é responsável por desenvolver a imagem/sentimento que os colaboradores, parceiros e clientes terão do TerraLAB e de seus produtos/serviços. Para isso, ela deve planejar, promover e realizar campanhas de publicidade, ou eventos, sobre importantes acontecimentos ligados ao TerraLAB. Assim, o Marketing deve gerir todas as mídias e redes sociais do TerraLAB, incluindo: Blog, Wiki, Instagram, Linkedin, Facebook, Twitter, etc. Tudo deve ser realizado pensando sempre nas perspectivas de três atores essenciais à existência do laboratório, seus colaboradores, seus clientes e seus parceiros.

O que faz a equipe Jurídica?

“O jurídico é um dos departamentos com maior permeabilidade dentro de uma empresa, já que faz interface com diversas áreas – desde compras e vendas até marketing e finanças. É responsável por orientar a empresa sobre as melhores práticas a seguir, o departamento jurídico é um dos setores mais importantes quando falamos do contexto estratégico do negócio.” 

(https://www.aurum.com.br/blog/departamento-juridico-em-uma-empresa/, 2020).

Recentemente, o setor jurídico das melhores empresas deixou de ser passivo, deixou de ser acionado apenas quando ações judiciais ou outros processos jurídicos aparecem, e passou a ser um setor ativo, tornando a rotina jurídica organizada e mantendo a empresa em conformidade com a lei. O jurídico precisa ter uma mentalidade empreendedora e ter uma visão global do negócio. No desenvolvimento ou no lançamento de um produto ou serviço, o jurídico deve realizar a análise do contexto legal e também fornecer um posicionamento ou previsão sobre os impactos futuros de tais medidas. É preciso conhecimento aprofundado em legislação, é preciso aperfeiçoar a construção de um raciocínio crítico e analítico para embasar e mapear possíveis riscos à imagem da empresa, à saúde e ao bem-estar dos colaboradores. É preciso cuidar dos impactos das decisões da empresa em contratos, registro de patente, etc.

O TerraLAB é um laboratório de uma universidade, portanto, não é uma organização (não tem CNPJ), apenas a UFOP o é. Portanto, é missão desse setor buscar caminhos viáveis para captação de projetos, recursos humanos e recursos financeiros. Para facilitar todas essas captações, busca-se responder às seguintes questões: Como o próprio TerraLAB deveria ser caracterizado dentro da estrutura da UFOP: Como um projeto de extensão? Como um núcleo de pesquisa? Ambos? Quais normas da UFOP são relevantes neste contexto? Quais leis regem estas questões? Como receber doações de empresas parceiras? É possível a dedução de impostos das empresas doadoras? Como fixar e pagar colaboradores? São muitos os desafios à frente. Esperamos que setor jurídico nos ajude a construir um modelo viável para a inovação.

Considerações Finais

Neste artigo apresentamos como o TerraLAB está estruturado para conferir escalabilidade a sua equipe de desenvolvimento de software e produtividade a todos os seus colaboradores. Neste artigo, também descrevemos quais são as funções atribuídas a alguns especialistas que formam a equipe. Esperamos ter contribuído para todos aqueles estudantes e profissionais que pensam em algum momento construir sua própria startup. Diga-nos se esse texto lhe foi útil e compartilhe conosco idéias para melhorar a produtividade e bem estar nossa equipe.

Artigo escrito por Prof. Tiago Carneiro."
TerraLAB: Retrospectiva e encerramento do ano de 2020!,http://www2.decom.ufop.br/terralab/terralab-retrospectiva-e-encerramento-2020/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/12/retangularSemTitulo-730x350.png,"Oano de 2020 trouxe muitos desafios para a humanidade, sem exceções. Todas as organizações tiveram que se adaptar. Muitas delas precisaram redefinir suas estratégias de negócios e objetivos devido ao cenário de incertezas decorrente da pandemia. No TerraLAB não foi diferente. Quando a quarentena teve início, estávamos começando um processo seletivo que recebeu quase meia centena de novos estudantes. Uma expansão no quadro de colaboradores nunca antes experimentada pelo laboratório.

O aumento do número de alunos, por si só, já traria um desafio interessante para o TerraLAB, uma vez que trabalhar com mais alunos, de diferentes cursos e em estágios distintos dentro deles, não seria tarefa simples. Com a impossibilidade de trabalharmos pessoalmente, essa tarefa se mostrava ainda mais complexa. Apesar de todo esse quadro desfavorável, não só conseguimos finalizar bem o processo de seleção dos novos estudantes, como conseguimos atingir e cumprir metas relevantes ao longo do ano.

Portanto, para fecharmos as atividades do blog do TerraLAB no ano, trazemos esse artigo exibindo dados referentes às atividades que realizamos em 2020. Apresentaremos os projetos realizados, estatísticas das redes sociais, parcerias firmadas, dentre outras informações, que julgamos importante compartilhar com os leitores do blog que nos acompanharam durante todo esse tempo.

Projetos Desenvolvidos

Esse ano tivemos a possibilidade de desenvolver muitos projetos. O GeoSpotted, primeiro projeto iniciado ano passado e que serviu de aprendizado para os alunos, foi finalizado mas não chegou a ser lançado, tendo em vista que a pandemia inviabilizou totalmente sua aplicação.  Desde o início de 2020, passamos a desenvolver outras cinco aplicações: Orquestra Ouro Preto, Segurança da Mulher, WindSun, CSI: Covid e Geolarica. Os primeiros quatro projetos mencionados são frutos de parcerias com a Orquestra Ouro Preto, com a Ouvidoria da UFOP e com o professor Rodrigo Cesar Pedrosa Silva (departamento de Ciência da Computação da UFOP). O projeto Geolarica foi usado como aplicação-escola onde os estudantes aprenderam a realizar testes e validação, garantindo a qualidade do framework de componentes também desenvolvidos pelo TerraLAB em 2020. Este framework é reutilizado em todos os projetos, assim, atingimos um novo e mais elevado nível em Engenharia de Software. Vale ressaltar que todos os projetos citados possuem versões para dispositivos móveis e Web. Em outras palavras, nesse ano trabalhamos no desenvolvimento de 12 aplicativos, se considerarmos também o GeoSpotted! 

Todas elas já se encontram em estágio avançado de desenvolvimento. O aplicativo Orquestra Ouro Preto, inclusive, já foi lançado e está a disposição para quem quiser conferir nosso trabalho, além de prestigiar as excelentes músicas da Orquestra. WindSun e CSI: Covid estão em processo de lançamento, subindo pela pilha de testes  das lojas virtuais: Google e Apple. O projeto Segurança da Mulher, em comparação com os demais projetos, ainda está um pouco atrasado em virtude da complexidade que ele nos apresentou em termos de UX (User eXperience), mas já está com sua versão mobile praticamente finalizada e com a versão Web bastante adiantada. 

Como projetos adicionais, podemos citar também a campanha de marketing realizada para o lançamento do aplicativo Orquestra Ouro Preto. Por meio do blog e redes sociais do TerraLAB, promovemos durante três semanas essa aplicação que foi lançada, oficialmente, durante uma live da Orquestra. Outra realização relevante foi a reconstrução do site do TerraLAB, que agora possui um layout muito mais moderno e profissional, com uma cara bem mais condizente com o trabalho realizado no laboratório. 

Processo Trainee 2020.1

Outra realização importante que merece atenção especial foi o Processo Trainee TerraLAB 2020.1. As inscrições ocorreram entre os dias 02 e 13 de março, com a inscrição de 48 estudantes e a participação efetiva de 39 graduandos e pós-graduandos em Ciência da Computação, Engenharia de Controle e Automação, Engenharia Mecânica, Engenharia de Minas, Arquitetura e Urbanismo, Direito, Engenharia de Materiais, Jornalismo e Estatística. 

O marco inicial do processo de seleção foi o Workshop TerraLAB 2020.1, que contou com o apoio das empresas parceiras do laboratório – Cachaça Gestor, Gerencianet, Stilingue e Usemobile. Neste evento, os alunos puderam assistir palestras enriquecedoras sobre arquitetura de softwares, desenvolvimento para Android, uso do AngularJS e NodeJS em aplicações WEB, gestão de pessoas, processos ágeis, etc. O evento contou ainda com a participação da equipe de Quality Assurance do iFood e um ex-aluno da UFOP que, por dez anos, atuou como Engenheiro de Software da Google, compartilhando experiências muito interessantes para todos. 

O processo seletivo teve duração de 8 semanas, bem superior às 3 estipuladas previamente. Essa extensão se deu em virtude da pandemia, que veio à tona logo no início das atividades. Dessa forma, todo o processo trainee foi realizado de maneira remota, assim como temos trabalhado até então. Durante essas semanas, os alunos foram introduzidos à metodologia ágil de desenvolvimento de software, com orientações e treinamentos sobre as tecnologias necessárias para atuar na área. Divididos em 5 equipes (squads), os trainees vivenciaram uma experiência semelhante ao mercado de trabalho, trabalhando no desenvolvimento de 5 aplicativos, com versões WEB e Mobile para cada um deles. Quatro desses projetos são os que estão sendo finalizados agora, os já citados CSI: Covid, Geolaria, Segurança da Mulher e WindSun. Por meio de desafios semanais, aprenderam a utilizar tecnologias como React, React-Native e NodeJS através de cursos online e mentorias oferecidos por nossos parceiros.

Dos 39 alunos que participaram do processo seletivo, 29 foram aprovados e passaram a fazer parte do laboratório. Contabilizando os egressos, o TerraLAB conta hoje com 22 alunos. Para muitos, os desafios trazidos pela pandemia significaram mudança de rumos e o atraso de sonhos. Todos têm o nosso apoio. Todos! Sobre os egressos, vale ressaltar o crescimento deles e quanto o TerraLAB foi decisivo para o seu sucesso. Por estarmos sempre em parceria com as empresas que atuam na área, o laboratório não apenas consegue recursos para a manutenção de suas atividades através dessas colaborações, como estreita a distância entre os alunos e essas empresas. Podemos citar o caso dos alunos Arilton Aguilar e Koda Gabriel. Integrantes do laboratório desde o ano passado, experimentaram um crescimento profissional impressionante nesse período, tanto que, no início de dezembro, tiveram seus trabalhos reconhecidos e foram contratados por uma das nossas empresas parceiras. Diversos outros exemplos poderiam ser dados aqui a respeito de egressos que tiveram contribuição decisiva do laboratório para o êxito dos mesmos, já que essa é a principal função do TerraLAB: Ser um celeiro de talentos para o mercado de trabalho. Contudo, vamos deixar isso para um artigo futuro, onde traremos números e depoimentos relatando o quão importante foi a contribuição do TerraLAB na formação desses profissionais.

Parcerias Firmadas

Como mencionado acima, o TerraLAB possui parceria com diversas empresas da computação da região. Todas elas já reconhecem a importância do laboratório nas áreas de inovação tecnológica e na formação de mão de obra diferenciada, o que tem motivado investimentos por parte delas em nosso trabalho. 

Esses investimentos vêm em forma de recursos para o laboratório (equipamentos em geral) e bolsas para os estudantes. Isso faz com que os alunos se tornem ainda mais motivados com as atividades do laboratório, nos tornando capazes de assumir projetos que exigem maior responsabilidade e comprometimento. Dentre as empresas que estão conosco nessa empreitada, citamos a Usemobile, fundada por ex-alunos que fizeram parte do TerraLAB (outros exemplos de sucesso dos egressos do TerraLAB), a Stilingue, empresa que se aproximou muito do laboratório depois do nosso bom rendimento nos desafios propostos por ela, a Gerencianet que nos apoia fornecendo ferramentas de aprendizado remoto, além da empresa Cachaça Gesto que também é fundada por egressos do TerraLAB.

Podemos citar também a parceria com a Ouvidoria da UFOP, para o desenvolvimento do aplicativo Segurança da Mulher. Além da criação dessa importante ferramenta na luta em prol da segurança das mulheres, este parceiro nos ofereceu uma importante contribuição jurídica para as necessidades do TerraLAB. Outra parceria muito benéfica para o laboratório foi a criada com a Orquestra Ouro Preto. Desenvolvemos uma aplicação que promove a divulgação do belo trabalho realizado pela Orquestra, fazendo com que os fãs possam acompanhar suas atividades de maneira muito mais prática e cômoda. Com isso, mais uma vez, conseguimos recursos necessários para o laboratório continuar suas atividades.

Por último, mas não menos importante, mantemos uma parceria com o Laboratório CSI  que também integra o Departamento de Computação (DECOM) da UFOP, nas figuras dos professores Eduardo Luz, Gladston Moreira e Rodrigo Pedrosa . Com essa parceria desenvolvemos os projetos CSI: Covid, aplicação extremamente importante dentro do contexto atual da pandemia, por permitir o diagnóstico da COVID-19 a partir da análise de imagens de raio-x dos pulmões dos pacientes e projeto WindSun que fornece prognósticos sobre o clima. A contribuição desses profissionais foi decisiva para o desenvolvimento dos motores de inteligência artificial (AI) utilizados nessas aplicações, propiciando aos estudantes do TerraLAB o contato com essa importante tecnologia.

Visibilidade do TerraLAB.

Como em qualquer atividade, nos dias de hoje é extremamente importante saber “vender o seu peixe”. Tendo ciência disso, após o ingresso de novos alunos no primeiro semestre do ano, criamos nossa própria equipe de marketing. Essa equipe traçou uma estratégia que buscava atingir nossos três públicos alvos: empresas da área de tecnologia, professores e pesquisadores da área de Computação e alunos da universidades, para que futuramente se interessem em fazer parte da nossa iniciativa. 

Para chamar a atenção dessas pessoas, passamos a divulgar, semanalmente, três artefatos de marketing. Às segundas-feiras, temos o TerraOffice, que traz sempre postagens, nas redes sociais, sobre o dia-a-dia do laboratório. Às quartas, temos sempre um novo artigo no blog do TerraLAB (como esse). Para nós, esse é o artefato mais importante, pois por meio dele damos visibilidade e compartilhamos o conhecimento técnico e científico adquirido pelos alunos do laboratório. Temos 48 artigos lançados no blog, sendo 34 deles publicados de março de 2020 até o presente momento, após o ingresso dos alunos aprovados no último processo trainee.  E por fim, às sextas-feiras, temos o TerraTips, que mostra sempre dicas relacionadas com tecnologia, normalmente relacionadas aos textos do blog.

Com essa abordagem adotada desde o início do ano, aliada às mudanças realizadas no site do TerraLAB, fizeram com que o alcance e o número de visualizações aumentassem bastante tanto no site quanto nas nossas redes sociais. As Figuras 1, 2 e 3 mostram estatísticas sobre o número de acessos ao site e blog do TerraLAB. Essas informações são referentes aos meses de agosto (momento onde colocamos no ar o novo site do laboratório) até o presente momento. Podemos notar que o aumento do número de acessos ao site foi muito grande, saindo de 656 em agosto para 2854 em novembro. Por ainda estarmos finalizando apenas a primeira metade de dezembro, temos “apenas” 696 acessos, número que já é superior ao obtido em todo o mês de agosto. Mesmo assim, vale ressaltar que a média diária de acesso no mês de dezembro já é superior aos demais meses, ultrapassando a marca de 100 acessos por dia (107).

Figura 1 – Gráfico Estatísticas de Acesso Site TerraLAB.

Figura 2 – Números Absolutos de Acessos e Média de Acessos por Dia no Site do TerraLAB.

Figura 3 – Gráfico Referente ao Número de Visitas ao Blog do TerraLAB. 


Quanto ao blog, o número de acessos tem crescido constantemente conforme vemos na Figura 3. Até o momento, desde agosto temos 12851 visitas, sendo 7439 visitantes distintos. Esses números mostram o sucesso que obtivemos com as mudanças realizadas no layout do site e o quanto os artigos produzidos por nossos colaboradores têm atraído cada vez mais o público, seja pela relevância dos temas abordados ou/e qualidade dos textos produzidos.

Em relação às redes sociais, a produção de conteúdo semanalmente fez nossos números de seguidores e interações crescer de maneira geral. As Figuras 4, 5, 6 e 7 mostram informações a respeito das estatísticas do linkedin do TerraLAB desde março deste ano até o momento. Nesse período, o perfil do laboratório teve um aumento de mais de 85% no seu número de seguidores, saindo de 194 para 356 atualmente. Consequentemente, percebemos um aumento no número de visualizações da página, o que era esperado em função dos novos seguidores que adquirimos. Dentre os atuais seguidores que temos, sua maioria é composta por profissionais de Ouro Preto e região (44,94%) e Belo Horizonte e região (26,27%) – Figura 7, sendo a maior parte deles profissionais em início de carreira (53,1%) – Figura 6.

Figura 4 – Gráfico Novos Seguidores do Perfil no Linkedin do TerraLAB.

Figura 5 – Gráfico Número de Visualizações na Página do Linkedin do TerraLAB.

Figura 6 – Nível de Experiência Profissional dos Seguidores do Perfil do Linkedin do TerraLAB.

Figura 7 – Localidade dos Seguidores no Perfil do Linkedin do TerraLAB


A Figura 8 ilustra, mais uma vez, o aumento da visibilidade do TerraLAB nas redes sociais devido à estratégia de marketing que adotamos. Como podemos ver, percebemos aumentos nos números de perfis alcançados (6,4%), de interações (34,2%) e no total de seguidores (2,1%), no último mês, em nosso perfil do Instagram. Assim como percebemos no Linkedin, a maior parte de nossos seguidores se encontram em Ouro Preto e região.

Figura 8 – Estatísticas do Instagram do TerraLAB.


Com engajamento menor do que no Linkedin e Instagram, o TerraLAB também possui uma página no Facebook e uma conta no Twitter. As Figuras 9, 10, 11 e 12 trazem estatísticas dos perfis citados nessas redes (a Figura 9 tem informações sobre a página do Facebook e as demais sobre a conta do Twitter). Diferentemente do evidente aumento mostrado nos números do Instagram e Linkedin, Twitter e Facebook não apresentaram a mesma evolução. Esse comportamento pode ser justificado por não serem as redes sociais entendidas, por nós, como prioridade para a divulgação do nosso trabalho, inicialmente. Outro fator que pode ser levado em consideração é que esses dados se referem aos últimos 30 dias apenas, uma parcela de tempo muito curta para ser representativa de todo o trabalho que fizemos nos últimos meses. De todo o modo, essas informações servem como um norte sobre onde podemos trabalhar para potencializar ainda mais o alcance do nosso trabalho.

Figura 9 – Estatísticas da Página do Facebook do TerraLAB.
Figura 10 – Estatísticas Twitter do TerraLAB.
Figura 11 – Número de Visitas do Twitter do TerraLAB.

Figura 12 – Impressões do Twitter do TerraLAB.


Esperamos que essas informações sejam relevantes para todos, principalmente para os integrantes atuais do laboratório. Esses resultados são motivo de muito orgulho para  nós e mostram que estamos no caminho certo, contribuindo com a comunidade onde estamos inseridos e promovendo relevante crescimento para todos os estudantes envolvidos no trabalho do TerraLAB.

E você, fiel leitor, gostou das informações compartilhadas nesse texto? Teria alguma sugestão sobre como poderíamos melhorar o trabalho feito dentro do TerraLAB? Sinta-se à vontade para comentar, deixar críticas construtivas e elogios, caso considerem que sejamos merecedores!

Artigo escrito  Ramon Silveira Assis Barros. Revisado por Prof. Tiago Carneiro."
O que é User Experience e quais etapas compõem seu processo.,http://www2.decom.ufop.br/terralab/o-que-e-user-experience-e-quais-etapas-compoem-seu-processo/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/12/ux-730x350.png,"User eXperience (Experiência do Usuário) ou simplesmente UX é toda relação que uma pessoa tem com um produto ou serviço. Este termo está diretamente ligado às sensações que o usuário vivencia ao utilizar um  produto que, além de resolver seu problema, deve oferecer uma experiência satisfatória. Exemplificando, podemos dizer que UX inicia desde o momento em que acessamos um site de vendas, selecionamos o item que desejamos, colocamos no carrinho de compras, pagamos, até o momento que recebemos o item em casa, abrimos, tocamos e usamos.

Neste artigo vamos falar um pouco sobre o processo de UX, especificamente das etapas de briefing, investigação, benchmarking, desenvolvimento das personas, prototipagem, testes de usabilidade e sua implementação.

Briefing

Dito como um elemento fundamental o briefing é quem vai definir o que cada equipe irá fazer durante todo o projeto. Ele geralmente é feito em uma reunião, onde são coletadas várias informações relevantes para desenvolver uma ideia ou projeto. Além disso, ele deve ser detalhado e curto para que não cause confusão entre as pessoas que irão utilizá-lo. É muito importante que o mesmo tenha bem delimitado o objetivo que o cliente ou empresa deseja atingir com aquela ação específica ou projeto. O briefing é a bússola do projeto. É nele que será discutido qual o problema a ser trabalhado e quais as principais funcionalidades. No mais, tudo que tiver nele será um guia para todo o projeto. 

Podemos dizer que o briefing geralmente busca responder às seguintes perguntas:

Qual o problema a ser resolvido?
Quais são os possíveis usuários do produto?
Qual o contexto em que estão inseridos?
Quais são as possíveis soluções?
Como se chegou a esse problema?
Quais as principais funcionalidades?
Investigação

Nesta etapa definimos quem são nossas personas, isto é, os públicos alvos do produto em desenvolvimento (detalharemos esse conceito à frente). Para isso é preciso observar, registrar, analisar e identificar as características mais relevantes que distinguem um grupo do outro, ou seja, o que difere um usuário do outro. Além disso, é preciso identificar quais são as principais dores de nossos usuários. Para isso, existem algumas abordagens que podemos obter respostas, sendo elas:

Análise quantitativa: Análise na qual buscamos colher dados numéricos sobre a opinião de uma grande quantidade de usuários sobre determinado assunto. Podemos fazer a análise quantitativa através de questionários onde identificamos a faixa etária dos usuários do nosso produto, estados e cidades onde é mais conhecido nosso produto etc.

Análise qualitativa: Análise na qual devemos lidar diretamente com o usuário, pois devemos obter dados que dizem respeito ao motivo do mesmo usar o produto e o porquê utiliza de tal maneira. Este método nos demonstra o que resolver e como resolver com base na opinião do usuário.

Benchmarking

Benchmarking significa fazer uma análise dos concorrentes. Desta forma é possível identificar quais são as melhores ferramentas ou práticas que os mesmos estão utilizando, para que possamos implementar em nosso projeto, empresa ou, até mesmo, melhorar algo em nossa estratégia para que possamos atuar melhor e sermos mais competitivos no mercado. Nesta etapa podemos identificar o que nossos principais concorrentes entregam para os usuários, ou seja, como o problema é resolvido, além de sabermos quais são os pontos fortes e fracos dos concorrentes. Devemos ter sempre em mente que por mais que um projeto seja inovador é preciso levantar quais produtos são mais similares ao que iremos desenvolver, pois dessa maneira é possível identificar soluções e pegar um atalho na fase de desenvolvimento do produto.

Personas

Personas são personagens fictícios criados para representar os diferentes tipos de usuários. Elas buscam retratar o usuário ideal do produto, pois são criadas baseadas em dados extraídos das etapas de investigação e benchmarking, não em suposições. O objetivo é traçar um perfil que extraia as principais características dos usuários, com a finalidade de elaborar estratégias que atinjam o público alvo.

Existem algumas ferramentas gratuitas que podemos utilizar para criar personas para nossos projetos, aqui temos duas:

Make my persona;
Gerador de personas.
Prototipagem

Etapa de validação do produto, onde simulamos seu funcionamento. Para fazermos a prototipagem temos três níveis de fidelidade:

Baixa fidelidade: Nele é possível encontrar um protótipo com baixo grau de detalhamento, apresentando apenas as funcionalidades. Neste nível de fidelidade, buscamos validar apenas essas funcionalidades sem a existência de interfaces elaboradas de interação com os usuários.

Média fidelidade: Conhecidos como wireframes, buscamos definir e validar o conteúdo da interface e criar simulações de uso não muito complexas. Protótipo em que é possível simular o que seria o comportamento esperado do produto com um pouco mais de proximidade ao resultado final esperado.

Alta fidelidade: Apresenta uma interface com mais proximidade ao que seria o produto final. Neste protótipo é possível validar a interação do usuário e sua experiência com o produto. É o que mais se aproxima de um Produto Mínimo Viável (MVP).

O ideal para qualquer produto é passar por essas três fases, pois em cada uma delas é possível validar um tipo de requisito com mais eficiência, além de minimizar os riscos de retrabalho.

Algumas das principais ferramentas utilizadas para prototipagem são:

Axure;
Figma;
Balsamiq;
Proto.io;
Draw.io
Marvel;
Sketch;
Adobe XP.
Testes de usabilidade

Teste de usabilidade é um teste qualitativo que, ao ser realizado, tem como intuito medir o grau de satisfação e performance com relação à interface do produto. No caso, buscamos identificar a quantidade de erros que o usuário possa cometer no decorrer do teste e, também, se o mesmo consegue executar uma dada tarefa, ou seja, cumprir um determinado objetivo. Nesta etapa é preciso recrutar usuários identificados como público alvo do produto que será testado e, no decorrer do teste, enquanto o usuário executa a tarefa solicitada, o analista observa, ouve e anota. Contudo, é importante deixar claro para os participantes do teste que, o que está sendo testado é o produto e não ele.

Implementação

Etapa final do processo na qual é feita a entrega do produto. Nela é possível realizar alterações no projeto com base nos dados coletados nas etapas anteriores. 

Lembre-se que pessoas mudam de comportamento e gostos então isso torna a prática UX mutável, fazendo com que um projeto sempre estará em um ciclo onde todas as etapas se repetem.

E você, já pensou em ser um profissional de UX? Você conhecia as etapas da prática de UX? Conta pra gente qual delas você mais gostou e claro, se teve dúvida sobre alguma delas, não deixe de comentar também.

Considerações Finais

Neste artigo pudemos entender melhor o que é UX e conhecermos de forma simples quais são as etapas que compõem seu processo de desenvolvimento. Vimos que o briefing é uma etapa fundamental por ser a bússola de todo esse processo e que as análises quantitativa e qualitativa da investigação são de extrema importância para colhermos informações que serão de grande serventia para formularmos nossas personas de acordo com nosso público alvo. Descobrimos também, que a prototipagem é essencial pois avaliarmos diferentes requisitos em níveis distintos de fidelidade para que, por fim, possamos executar os testes de usabilidade com prováveis usuários e fazermos melhorias no projeto, para em seguida realizarmos a entrega do produto final.

Escrito por Daniel Rodrigues. Revisado por Prof. Tiago Carneiro.

Referências
As etapas da prática de UX
As etapas da prática de UX

https://medium.com/design-rd/as-5-etapas-de-um-processo-de-solu%C3%A7%C3%A3o-eb4eec0cde37

https://medium.com/skillsweb/6-livros-para-quem-deseja-conhecer-a-%C3%A1rea-de-ux-f97356ccf2ed"
O Papel e os Desafios do Gerente de Projetos,http://www2.decom.ufop.br/terralab/o-papel-e-os-desafios-do-gerente-de-projetos/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/11/imagemArtigo1-730x350.png,"Projeto, de uma maneira geral, é tudo aquilo que envolve o desenvolvimento de algo novo, inédito, dentro de um prazo bem determinado. Em outras palavras, projetos estão diretamente ligados à inovação, que pode ser entendida como a implementação de um novo produto (bem ou serviço), processo, modelo de marketing, método organizacional nas práticas de negócio, na organização do local do trabalho ou nas relações externas. Por trabalhar com algo nunca feito antes, gerenciar projetos torna-se uma atividade desafiadora e complexa, visto que, diferentemente da gestão tradicional que lida com rotinas bem estabelecidas, a gestão de projetos é pautada em estimativas, que trazem consigo as incertezas.

Por lidar com algo tão complicado, o gerente de projetos, pessoa designada para liderar a equipe responsável por alcançar os objetivos do projeto, precisa ter diversas competências técnicas e comportamentais para que seja bem sucedido no cumprimento de suas atividades. É papel dele garantir a entrega do projeto dentro dos prazos estabelecidos, gerindo adequadamente as incertezas ao longo do desenvolvimento do projeto, para que elas não comprometam a entrega do mesmo. Tudo isso mantendo a equipe motivada e os clientes satisfeitos, respeitando os interesses da organização da qual ele faz parte. 

Sabendo disso, neste artigo falaremos sobre o papel do gerente de projetos, conciliando os conceitos presentes na literatura com as experiências práticas observadas dentro do TerraLAB. Tentaremos mostrar como adaptamos essa importante função dentro das nossas particularidades enquanto organização.

Esfera de influência do Gerente de Projetos

Por se tratar de uma função estratégica dentro de uma empresa, o gerente de projetos possui um vasto campo de influência. Segundo o PMBOK (Project Manegement Body of Knowledge), essa influência do seu trabalho abrange, de maneira geral, os projetos, a organização (empresa), o setor de atuação da organização, a gestão de projetos propriamente dita e os profissionais de outras expertises.

Em relação ao projeto, é ele o responsável por mostrar o caminho do sucesso aos colaboradores. Realiza a comunicação entre as partes interessadas (stakeholders), gerindo as expectativas dos clientes, motivando e orientando a equipe, e atendendo às exigências dos patrocinadores. Para tanto, ele tem que ser capaz de se comunicar muito bem com todas essas pessoas, sendo essa habilidade tratada como uma das mais relevantes para exercer essa função. Uma boa comunicação inclui habilidades apuradas de expressão verbal, escrita e até mesmo gestual. Deve procurar manter canais para realização de feedbacks, transmitindo as informações de forma concisa, clara e completa. Em outras palavras, ele deve atuar sempre tentando eliminar ou, no mínimo, buscando mitigar ruídos de comunicação entre todos os envolvidos no projeto. 

No caso particular do TerraLAB, temos um colaborador no papel de gerente de projetos. Entretanto, a função é compartilhada, em muitos aspectos, com outras duas pessoas, sendo elas o professor encarregado pela iniciativa (TerraLAB) e outro colaborar com boa experiência dentro do laboratório. Essa equipe, que pode ser tratada como um Escritório de Gerenciamento de Projetos (EGP), mantém um canal de comunicação constante, onde as decisões sobre os projetos desenvolvidos são constantemente tomadas. Essas decisões são comunicadas aos squads pelo gerente de projetos, assim como é responsabilidade dele trazer informações dos times aos EGP. Dentro de cada squad, temos os Product Owners (PO’s) que são a principal ponte de comunicação entre os squads e o gerente de projeto. É fundamental que o gerente crie uma relação próxima e de confiança com esses colaboradores, para que tenha acesso, prontamente, ao que se passa dentro dos squads.

No âmbito de uma empresa (organização), o gerente de projetos interage com outros gerentes de projetos. Essa interação é importante, pois outros projetos, muitas vezes, podem demandar recursos semelhantes que podem ser compartilhados ou priorizados para determinado projeto, de acordo com o objetivo global da empresa. O gerente de projetos trabalha com o patrocinador para resolver questões estratégicas e de políticas internas que possam afetar a equipe, a viabilidade ou a qualidade do projeto. Dentro da realidade do TerraLAB, é relativamente comum termos que concentrar esforços no desenvolvimento de determinado projeto, quando esse tem prazo de entrega curto ou alguma relevância estratégica para o laboratório. O papel do gerente de projetos, nesse caso, é deixar isso claro aos colaboradores tomando medidas de controle como, por exemplo, alocando colaboradores de outros squads à equipe, quando necessário.

Também é papel do gerente de projetos manter-se atualizado sobre as tendências do setor. Ele deve ser capaz de coletar essas informações, analisá-las, julgar se podem ser incorporadas aos projetos desenvolvidos e como fazer isso. Essas tendências vão desde o desenvolvimento de novos produtos e tecnologias, novos nichos de mercado e ferramentas de suporte técnico até forças econômicas que podem afetar projetos em curso. No TerraLAB, essa função é exercida pelo professor responsável pelo laboratório, haja visto que é o profissional mais qualificado do time para exercer essa função. Os demais colaboradores, por mais que assumam responsabilidades consideráveis, ainda estão em treinamento e não se encontram maduros o suficiente para realizarem essa atividade, no momento.

O gerente de projetos deve sempre defender o valor da abordagem de gerenciamento de projetos para a empresa. Deve se portar como um embaixador de valores como pontualidade, qualidade e gerenciamento de recursos. Seu papel é garantir que as diretrizes de gerenciamento de projetos estejam inseridas na cultura organização da empresa. Um exemplo disso, no TerraLAB, é a padronização do uso dos repositórios do GitLab. Buscamos uma padronização que nos proporcionará a obtenção de métricas de desenvolvimento, importantes para a gestão dos projetos em andamento do laboratório. Em diversas reuniões, ressaltamos a importância disso junto aos colaboradores, conforme se espera de profissionais na posição de gerente de projetos.

Competências do Gerente de Projetos

Segundo o PMBOK, as competências desejadas para um gerente de projetos podem ser distribuídas em três conjuntos de habilidades-chaves apresentadas na figura abaixo. 

Habilidades de gerenciamento de projetos técnico é a capacidade que esse profissional possui de colocar os conhecimentos em gerenciamento de projetos em prática nas iniciativas as quais ele faz parte. Isso reforça a necessidade desses profissionais estarem sempre se atualizando sobre as boas práticas da área e se mostrarem capazes de não apenas adquirir esse conhecimento, como colocá-lo em prática, adequadamente, em seus projetos. Dentre as diversas habilidades técnicas apresentadas pelos gerentes de projetos de destaque, podemos citar a capacidade de concentrar-se nos elementos críticos do projeto (fatores essenciais para o sucesso do projeto, cronograma, relatórios financeiros, etc.), capacidade de adaptação de ferramentas técnicas, métodos tradicionais e ágeis para cada projeto, entre outros.

As habilidades de gerenciamento estratégico e de negócios, comumente chamadas de conhecimento de domínio, estão relacionadas à capacidade do gerente de projetos em identificar a visão de alto nível da empresa, alinhando suas práticas gerenciais com o objetivo da organização como um todo. Em outras palavras, o gerente de projetos deve ser capaz de compreender claramente os propósitos da organização em que atua e fazer com que suas ações gerenciais, dentro dos projetos de sua responsabilidade, convirjam nessa mesma direção. Essas habilidades podem incluir conhecimentos de outras funções, como finanças, marketing e operações. Para que isso seja feito de maneira adequada, o gerente de projetos deve ter o melhor entendimento possível dos assuntos dos seus projetos. Por mais que eles possam ser das mais variadas expertises, isso é importante para que ele seja capaz compreender a inter-relação entre os projetos e a organização.

Já as habilidades de liderança estão relacionadas com a capacidade de orientar, motivar e guiar suas equipes. O maior ativo de uma empresa são as pessoas que a constituem. São elas que fazem as coisas acontecerem. Entretanto, precisam estar motivadas e se sentindo importantes dentro das equipes. É papel crucial do gerente de projetos manter seus colaboradores engajados. Para tanto, ele deve apresentar um conjunto amplo de capacidades, como poder de negociação, resiliência, boa comunicação, ser capaz de resolver problemas e administrar conflitos, ter pensamento crítico, dentre outros. De certo modo, deve ser um indivíduo político, sendo capaz de influenciar pessoas e exercer sua autoridade quando preciso. Dentro dos limites éticos, das políticas organizacionais, dos protocolos e procedimentos, devem fazer o que precisa ser feito para cumprir os objetivos dos projetos. 

Liderança pode ser exercida de diversas formas, de acordo com as características pessoais do gerente de projetos, da equipe que deve ser liderada e até mesmo da empresa. Gerentes de projetos de destaque são capazes de exercer liderança de diversas formas, se adaptando às necessidades encontradas.

No TerraLAB, essas habilidades foram muito exigidas ao longo dos últimos meses. Desde março, trabalhamos em cinco projetos, cada um com suas particularidades e desafios. Apesar de sermos um projeto de extensão como muitos existentes na UFOP, nosso ritmo de trabalho se assemelha muito ao de uma empresa, uma vez que temos clientes e prazos reais a cumprir. Isso acaba exigindo muito dos nossos colaboradores em termos de comprometimento e profissionalismo, pois trata-se de um trabalho sério.

Nossa equipe é formada exclusivamente por estudantes, que possuem particularidades muito próprias para serem administradas. Eles possuem atenção dividida com outras atividades relacionadas à universidade, como aulas, estágios e outros projetos. No atual momento, com o isolamento social imposto pela pandemia, alguns tiveram até necessidade de trabalhar para conseguir uma renda extra. Com toda essa demanda, não só o gerente de projetos como todo o EGP precisaram atuar continuamente buscando motivá-los e mantê-los focados nas atividades do laboratório. Como estamos impossibilitados do convívio diário pela pandemia, esse trabalho se tornou ainda mais difícil. 

Para contornar todas essas adversidades, procuramos manter uma comunicação ativa, reforçando sempre a importância do trabalho que desenvolvemos aqui. Além disso, com esses canais de comunicação livres, mantemos sempre conhecimento dos problemas e dificuldades de cada estudante, para podermos atuar rapidamente na resolução deles. Com o objetivo de mantê-los engajados, sempre deixávamos claro o quanto ainda podem crescer, como pessoas e profissionais, trabalhando em nossos projetos e somando a nossa equipe. Ao longo do desenvolvimento desses projetos, os próprios estudantes perceberam esse crescimento, que ficaram ainda mais evidentes com os feedbacks da gerência e dos clientes. Atuando dessa forma, temos conseguido cumprir, até aqui, todas as metas estabelecidas. No entanto, temos ciência que sempre poderemos evoluir muito em termos de gerência de projeto. 

Apesar da vasta literatura disponível sobre gestão de projetos, a própria literatura reconhece que boa parte dos conhecimentos nessa área são adquiridos na prática. Nenhum projeto é igual ao outro e sempre podemos aprender coisas novas em cada um deles. Portanto, é importante que o gerente de projetos tenha a mente aberta para mudanças e procure sempre se atualizar.

Vocês, leitores, se interessam por esse assunto? Já pensaram em se tornarem gerentes de projeto em algum momento? Possuem alguma experiência na área que gostariam de compartilhar conosco? Deixem nos comentários que leremos com todo o prazer! 

Escrito por Ramon Silveira Assis Barros. Revisado por Prof. Tiago Carneiro.

REFERÊNCIAS
Guia do Conhecimento em Gerenciamento de Projetos – PMBOK (Project Manegement Body of Knowledge); Sexta Edição; 2017;
CARVALHO, M. M.; Fundamentos em Gestão de Projetos; Terceira Edição; Editora Atlas SA; 2011;
Curso gestão de projetos – Veduca;"
Gerenciamento de estados globais no React – Como utilizar Context API,http://www2.decom.ufop.br/terralab/gerenciamento-de-estados-globais-no-react-como-utilizar-context-api/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/11/post1-730x350.jpg,"Acomponentização é uma das bases do desenvolvimento de uma aplicação React e um dos principais atrativos da biblioteca. Ela permite que diferentes funcionalidades sejam separadas em componentes, que podem, ou não, ter uma relação entre si. Esse processo facilita o reuso e permite que cada um deles controle e tenha acesso às suas próprias informações, o que possibilita que atualizações em seus dados não afetem diretamente outras partes da aplicação.

Imagine o seguinte cenário, uma aplicação em que o usuário possa escolher o tema de um site, por exemplo, é necessário que diversos componentes da aplicação saibam qual tema foi escolhido. Provavelmente, essa informação será definida como um state dentro do componente que permite a escolha do tema. O termo state se refere a uma variável de estado que armazena qualquer valor de interesse do desenvolvedor. Neste cenário, o desafio surge porque outros componentes da aplicação não têm acesso direto a esse state para que definam seu tema, portanto, é preciso que essa informação seja compartilhada de maneira global dentro do projeto. 

Nesse pequeno exemplo, vimos como é possível que o contexto de uso de um state definido dentro de um componente seja muito maior que aquele no qual ele foi criado. Noutras palavras, apesar do processo de componentização isolar diferentes funcionalidades de um projeto, pode ser necessário que elas acessem dados compartilhados.

Pensando nesse assunto, este artigo tem como objetivo apresentar uma solução para o compartilhamento de states entre componentes de uma aplicação desenvolvida em React. Primeiro, mostraremos como funciona a arquitetura de componentes do React. Depois, faremos um tutorial explicando como fazer o uso da Context API para gerenciar states na sua aplicação.

Hierarquia dos componentes no React

O React possui uma arquitetura onde seus componentes são organizados hierarquicamente, conforme ilustra a figura abaixo. Naturalmente, por padrão , os componentes filhos podem acessar as props dos componentes país. Props são atributos que podem ser passados diretamente para um componente. Porém, como esse compartilhamento é feito através de herança, não é possível que componentes que estejam em um mesmo nível (irmãos) acessem diretamente dados uns dos outros. Além disso, um componente pai também não tem acesso direto a dados dos filhos.

Para contornar essas limitações, o React oferece uma solução muito eficaz para quebrar essa hierarquia e permitir a existência de states globais, a Context API. Através dela é possível compartilhar states diretamente entre dois componentes, mesmo que eles não façam parte de uma mesma hierarquia. Na figura abaixo, as setas apontam para o componente que acessa os dados do componente em sua origem. O lado esquerdo da figura ilustra as permissões de acesso padrão. O lado direito da figura ilustra o uso da Context API para permitir que o componente 3 acesse diretamente os dados do componente 1.

A Context API é uma biblioteca que pode ser usada para autenticação, armazenamento de dados de formulários, configurações de temas ou outras funcionalidades que uma aplicação possa precisar.

Antes de começarmos, para realizar esse tutorial você já deve possuir um projeto React ativo. Se esse não for o caso, confira o seguinte tutorial:

Iniciando um projeto no React
Utilizando a Context API

A biblioteca Context API é instalada automaticamente junto com o próprio React. O React oferece uma função que inicializará o nosso gerenciador de estados globais. Portanto, vamos criar o arquivo myContext.js e nele importar apenas o React.

/* src/contexts/myContext.js */

import React from 'react';]

Feito isso, criaremos nosso context através da função createContext() e, logo após, o exportaremos, para que ele possa ser acessado futuramente. Um context é a ferramenta responsável por armazenar os states que serão compartilhados e acompanhar eventuais mudanças em seus valores para realizar novas renderizações. 

/* src/contexts/myContext.js */

const MyContext = React.createContext();
 
export default MyContext;

A partir de agora, temos um context declarado na nossa aplicação. Porém, ele ainda não está visível para os demais componentes da aplicação e não possui um state a ser compartilhado. Para que isso seja possível, devemos configurar quais states estarão presentes nele e quais componentes da aplicação poderão ter acesso a seus valores. O arquivo recomendado para realizar a configuração do context é o arquivo src/App.js, pois ele costuma possuir acesso a todos os componentes da aplicação.

Para iniciar esse processo, importaremos no App.js o arquivo que possui a declaração do nosso context.

/* src/App.js */

import MyContext from './contexts/myContext'

Em seguida, faremos a declaração dos states que vão ser compartilhados na aplicação. Nesse exemplo, serão criados os states nome, email e  idade, além de suas respectivas funções “set”, que também serão compartilhadas no nosso context, a fim de fazer mudanças nos valores dos mesmos, se necessário.

/* src/App.js */

//declaração dos states
const[nome, setNome] = useState('')
const[email, setEmail] = useState('')
const[idade, setIdade] = useState('')

Criados os states agora vamos exportar um componente cuja tag pai será um provedor dos states que desejamos compartilhar ao longo do nosso projeto e os filhos serão aqueles componentes que desejamos que tenham acesso a esses dados. 

A tag pai recebe o nome do context “MyContext”, que já havia sido importado no arquivo anteriormente, seguido de “.Provider”. Isso indica que estaremos utilizando aquele context como o provedor de dados que desejamos acessar globalmente na aplicação. Além disso, essa tag terá como atributo uma propriedade chamada value, que receberá como valores os states que iremos compartilhar, além de suas funções set. A figura abaixo mostra o código completo da função principal da nossa aplicação.

/* src/App.js */

function App(){
 
    const[nome, setNome] = useState('')
    const[email, setEmail] = useState('')
    const[idade, setIdade] = useState('')
 
    return (
        <MyContext.Provider value={{nome, setNome, email, setEmail,   idade, setIdade}}>
      //exemplos de possíveis componentes que terão acesso aos states
            <LandingPage/>
            <Menu/>
            <Form/>
            <Register/>
            <Login/>
        </MyContext.Provider>
    )
}
export default App

Nesse momento,  já definimos os states que iremos compartilhar e, na ultima linha da figura acima, os exportamos para que possam ser acessados naqueles componentes em que forem necessários. Então, para concluirmos, basta importá-los nos outros componentes do projeto.

Dessa forma, criaremos um arquivo e nele faremos a importação dos nossos states. Para isso, utilizaremos uma função do React chamada useContext, e novamente importaremos o nosso context, que foi definido anteriormente.

/* src/components/form.js */

import React, { useContext } from 'react'
import MyContext from '../contexts/myContext'

Finalmente, para poder acessar os states desejados, utilizaremos o “MyContext” como atributo da função useContext e a desestruturação para “retirá-los” do context e trazê-los ao componente que estamos trabalhando.

/* src/components/form.js */

const { nome, setNome, email, setEmail,   idade, setIdade} = useContext(MyContext)

Pronto, a partir de agora já estamos gerenciando estados de maneira global dentro da nossa aplicação. Todos esses valores podem ser acessados e modificados em quaisquer componentes que tenham acesso a ele (conforme definido no App.js). Todo componente proveniente do useContext causa uma nova renderização quando seu valor é alterado, logo, o valor de um context é alterado em todos os componentes que têm acesso a ele quando é atualizado. Isso o mantém idêntico em todas as partes da aplicação.

Considerações finais

Este tutorial mostrou como é feito o uso da biblioteca Context API para o gerenciamento de estados no React. Demonstrando como fazer sua inicialização, importar a configuração em outros arquivos, determinar quais states serão compartilhados e acessar seus valores em outras partes da aplicação. Gostou do tutorial? Ele foi útil para você? Conte para gente, nos comentários, se você teve alguma dúvida ou se já utilizou a Context API em algum projeto seu!

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software.

Referências 

Context

Working with the React Context API

Tutorial escrito por Carlos Magalhães Silva. Revisado por Vinícius de Paula Silva e Prof. Tiago Carneiro."
Como utilizar geolocalização em seu aplicativo React Native,http://www2.decom.ufop.br/terralab/como-utilizar-geolocalizacao-em-seu-aplicativo-react-native/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/11/blogPostPrincipal-730x350.png,"Neste momento, temos à nossa disposição diversos satélites ao redor de nosso planeta integralmente disponíveis para nos ajudar, a partir da tecnologia do GPS (Sistema de Posicionamento Global, do inglês Global Positioning System), enviando informações sobre horário, posicionamento e navegação. É claro que nem sempre essas informações encaminhadas a partir dos satélites chegarão para nós de forma exata, já que os serviços podem sofrer interferências de prédios ou até mesmo nuvens. Imagine se o iFood não fosse capaz de obter sua localização através de seu smartphone por causa de uma nuvem de chuva em sua região? Ele não conseguiria mostrar os locais próximos a sua localização e isso certamente seria um incômodo para os usuários do aplicativo. Porém, para sorte dos desenvolvedores, não é necessário que se preocupem com isso. Tecnologias como o A-GPS (assistente GPS), disponível de forma embarcada na maior parte dos dispositivos comercializados, utilizam a internet para ajudar os smartphones a se localizarem melhor quando os satélites não dão conta, nos informando a localização do aparelho com margem de erro em cerca de 5 metros. 

A maioria dos smartphones equipados com IOS ou Android, independente da empresa que o fabricou, possuem APIs (Interfaces de Programação de Aplicações, do inglês Application Programming Interface) disponibilizadas pelo sistema operacional que automatizam a comunicação de nosso software com o kernel, e depois o hardware do aparelho. Quando precisamos utilizar o GPS, por exemplo, precisamos destas interfaces em nosso software. Apesar delas simplificarem nosso trabalho, existem ainda outras APIs, focadas não somente em geolocalização, que acrescentam mais um nível de simplificação para o nosso código e ainda oferecem várias outras facilidades e funcionalidades, necessárias a qualquer aplicativo. Devemos deixar claro que essas APIs são disponibilizadas pela comunidade de desenvolvedores, e nem sempre podem ser utilizadas de forma gratuita.

Conseguir explorar as APIs de geolocalização disponíveis nos permite criar diversas aplicações diferentes, como o já citado serviço de entrega, aplicativos de prática de esportes, jogos, relacionamentos, viagens, e até mesmo para marketing. Porém, lembre-se que, para realizar operações de geolocalização em seu aplicativo será necessário solicitar que o usuário permita que você acesse estas informações no smartphone dele. Estas solicitações são as mesmas requeridas ao tentar acessar os documentos ou a câmera do aparelho, e normalmente são responsáveis pela fuga de usuários, visto que podem ser consideradas invasivas. Dessa forma, é necessário que seu time converse bastante com o cliente sobre a funcionalidade, e esclareça para os usuários como as informações disponibilizadas por eles serão utilizadas na aplicação.

Este artigo mostra de maneira bem sucinta e prática, com código simplificado e algumas imagens, como utilizar umas dessas APIs de geolocalização para desenvolvimento de aplicações móveis Android por meio do framework React Native. Para sua comodidade, oferecemos um projeto exemplo que lhe permitirá analisar melhor nossa implementação e no qual este artigo se apoia. Antes de continuar, presumimos que você já tenha tido contato com o React Native, caso ainda não, recomendamos acesso ao link a seguir: Iniciando com React Native.

1 – Obtendo a geolocalização do smartphone

Para obter a geolocalização, vamos utilizar a biblioteca “react-native-geolocation-service”. É possível utilizá-la tanto no Android quanto no iOS, foi testada em vários projetos no laboratório e oferece fácil implementação. Instale-a com o seu gerenciador de pacotes favorito.

Vamos adicionar as seguintes linhas no arquivo AndroidManifest.xml, que vão informar ao Android que em alguma parte de nosso código poderemos solicitar permissões referentes a localização do usuário. Dentro da tag manifest insira:

/* android/app/src/main/AndroidManifest.xml */
<uses-permission android:name=""android.permission.ACCESS_COARSE_LOCATION""/>
<uses-permission android:name=""android.permission.ACCESS_FINE_LOCATION"" />
<uses-permission android:name=""android.permission.ACCESS_BACKGROUND_LOCATION"" />


Para solicitar as permissões, vamos instalar a biblioteca “react-native-permissions”. Ela enviará a mensagem solicitando as permissões ao usuário e também vai nos informar do estado das permissões necessárias. 

E então podemos exportar o seguinte hook:

/* src/hooks */
//importando as dependências necessárias
import { useState, useEffect } from 'react';
import { requestMultiple, PERMISSIONS } from 'react-native-permissions';
import Geolocation from 'react-native-geolocation-service';
import { Platform } from 'react-native';

export default () => {
  const [errorMsg, setErrorMsg] = useState(null); // será utilizado para armazenar alguma mensagem de erro, caso ocorra
  const [coords, setCoords] = useState(null);   //vai armazenar a localização atual
// criando um useEffect que será executado uma vez quando o Hook for chamado (parâmetro passado ao fim da função é vazio).
  useEffect(() => {
    (async function loadPosition() {
// A função requestMultiple serve para requisitar múltiplas autorizações do usuário em sequência. As requisições são feitas na ordem passada. 

      const result = requestMultiple(
        [
          PERMISSIONS.ANDROID.ACCESS_FINE_LOCATION,
          PERMISSIONS.ANDROID.ACCESS_BACKGROUND_LOCATION
        ]).then(
          (statuses) => {
//statuses é um vetor que contém as respostas escolhidas pelo usuário em cada uma das autorizações solicitadas.
            const statusFine = statuses[PERMISSIONS.ANDROID.ACCESS_FINE_LOCATION];  //pegamos a autorização que o usuário selecionou para uso do GPS e para obter localização em primeiro plano
            const statusBack = statuses[PERMISSIONS.ANDROID.ACCESS_BACKGROUND_LOCATION]; 
//pegamos a autorização que o usuário selecionou para localização em background 
            if (Platform.Version < 29) { 
//Em APIs do Android abaixo da 29 não é necessário permissão para background location, apenas solicitar acesso ao GPS já oferece tudo que é necessário para utilizar a localização em primeiro e segundo plano. Nesse caso, apenas verificamos se a autorização do GPS é positiva
              if (statusFine == 'granted') {
                return true;
              } else {
                setErrorMsg('Usuário não aceitou solicitação de uso do GPS');
              }
            }
// Caso a API seja > 29, é necessário verificar se ambas as autorizações foram positivas. 
            if (statusFine == 'granted' && statusBack == 'granted') {
              return true;
            } else {
              setErrorMsg('Usuário não aceitou solicitação de uso do GPS');
            }
          },
        );

// caso as permissões tenham sido obtidas com sucesso, result será true e a localização do usuário poderá ser obtida.
      if (result) {
        await Geolocation.getCurrentPosition(       //se as permissões foram aceitas, obtemos a localização aqui
          ({ coords }) => {
	// O parâmetro {coords} desestrutura a resposta, obtendo apenas a parte relativa às coordenadas. Você também pode receber apenas (position) e observar outras informações que são obtidas ao se solicitar a localização. Nesse exemplo, apenas precisamos das coordenadas.
            setCoords({
              latitude: coords.latitude,
              longitude: coords.longitude,
            });
          }, (error) => {
            setErrorMsg('Não foi possível obter a localização');
          }, { enableHighAccuracy: true, timeout: 20000, maximumAge: 1000, showLocationDialog: true } 
          //showLocationDialog: essa função convida automaticamente o usuário a ativar o GPS, caso esteja desativado.
          //enableHighAccuracy: vai solicitar a ativação do GPS e coletar os dados dele
          //timeout: determina o tempo máximo para o dispositivo coletar uma posição
          //maximumAge: tempo máximo para coleta de posição armazenada em cache
        )
      }

    })()
  }, [])
//aqui retornamos as coordenadas e uma possível mensagem de erro que possa ter ocorrido.
  return { coords, errorMsg }
}


Perceba que, ao fim da execução do Hook, temos o retorno da geolocalização (coords) ou a mensagem de algum erro que ocorreu durante o funcionamento (errorMsg). A mensagem que o Android enviará para o usuário se parece com a apresentada abaixo, mas pode variar, dependendo da versão atual do sistema operacional instalado em seu smartphone: 

2 – Utilizando um mapa em sua aplicação

Após obter a geolocalização do usuário, que tal mostrarmos onde ele está localizado em um mapa? Podemos também mostrar locais próximos, ou até mesmo traçar rotas para possíveis locais desejados. Para isso, podemos utilizar diferentes API‘s disponíveis, como MapBox, Google Maps, Open Street Maps e algumas outras. Muitos destes serviços são gratuitos até um certo número de solicitações, cabendo a você selecionar o melhor para seu projeto. Para o nosso exemplo, vamos utilizar o Google Maps API, que é a mesma biblioteca utilizada em nossos projetos no laboratório.

Por ser um serviço um pouco mais restrito deveremos criar uma conta para nossa aplicação dentro da ‘Google Maps Platform’. Basta seguir este link e então cadastrar seu app na plataforma. 

Depois de cadastrado, você deverá adicionar ao seu projeto a biblioteca ‘Maps SDK for Android’. Seguindo todos os passos disponibilizados neste link, você terá disponível a API KEY necessária para inserir o mapa em seu projeto.

Para renderizar o mapa, vamos utilizar a biblioteca ‘react-native-maps’. Seguindo esse link você verá como instalar. Todas essas bibliotecas são utilizadas e testadas em nossos aplicativos no laboratório.

O código abaixo será responsável por renderizar o mapa.

/* src/screens/home */
//importando as dependências necessárias
import React, { useState } from 'react';
import MapView from 'react-native-maps';
import useLocation from '../../Hooks/useLocation';

export default function HomeScreen({ navigation }) {
  const [latitude, setLatitude] = useState(-20.398259);	
  const [longitude, setLongitude] = useState(-43.507726);	//utilizaremos estas duas variáveis (latitude e longitude) como posições padrão caso não seja possível obter a posição do usuário.

  const { coords, errorMsg } = useLocation();	    //utilizando o hook que vai nos fornecer a posição do usuário.
	
  return(
           <MapView
              showsUserLocation={true}		//destacando a localização do usuário no mapa
     	 showsMyLocationButton={false} 	//ocultando o botão que move o mapa para a localização do usuário
              toolbarEnabled={false}	//ocultando opções do google maps ao clicar em objetos do mapa
              style={{
                height: '100%',
                width: '100%',
                position: 'absolute',		
              }}	// Fazendo com que o mapa ocupe a tela inteira
              initialRegion={{
                latitude,	//posição inicial do mapa
                longitude,	//posição inicial do mapa
                latitudeDelta: 0.195,  	//determina o zoom do mapa
                longitudeDelta: 0.1921,	//determina o zoom do mapa
                ...coords	// Aqui sobrescrevemos as variáveis latitude e longitude com a posição do usuário obtida no hook que criamos para obter a localização.
              }}
            />
  ) 

}



O resultado final será parecido com o mapa na imagem abaixo. Como você pode presumir, o ponto azul ao centro mostra a localização do laboratório TerraLAB dentro do campus da UFOP, em Ouro Preto, MG:

A implementação dos botões que utilizamos no mapa pode ser verificada aqui. Adicionamos algumas funcionalidades interessantes que você pode utilizar em seu projeto.

Renderizar o mapa é bem simples, porém, é possível oferecer funcionalidades mais específicas aos usuários. As bibliotecas que instalamos nos fornecem formas de implementar marcadores, direcionar rotas, mostrar estabelecimentos ou lugares específicos no mapa.

2.1 – Implementando Marcadores nos mapas

Para esta funcionalidade utilizamos marcadores estáticos,  você pode verificar como eles foram feitos aqui. Vamos importá-los para nosso mapa e então, para renderizar, utilizaremos a propriedade Marker, que vem da mesma biblioteca do mapa que utilizamos. O código deve ficar da seguinte forma:

/* src/Components/MarkerImpl */
//importando as dependências necessárias
import React from 'react';
import { foodIcons } from '../../utils/Icons';	//utilizando ícones personalizados
import MarkerIcon from '../MarkerIcon';	//utilizando ícones personalizados
import {Marker} from 'react-native-maps';	

export default function MarkerImpl({
  onPress,	//parâmetro que nos permite passar uma função que será chamada sempre que o marcador for pressionado
  mark,		//parâmetro que recebe as informações do marcador
}) {
  return (
    <Marker
      onPress={onPress}
      tracksViewChanges={false}	//propriedade que melhora muito a performance do nosso aplicativo, mantendo os marcadores fixados no mapa e eliminando a renderização continua.
      key={mark._id}	//como temos vários marcadores, devemos adicionar um id para cada
      coordinate={{	//aqui nós inserimos a localização do marcador no mapa
        latitude: mark.latitude,
        longitude: mark.longitude
      }}
      title={`Marker_${mark.title}`}	//título do marcador
    >
      <MarkerIcon emoji={foodIcons[3].data} />	//personalizando o ícone no centro do marcador

    </Marker>
  );
}


Vale lembrar que você pode comentar a linha que utilizamos o “MarkerIcon” para mostrar o ícone padrão da biblioteca. O resultado será parecido com a foto abaixo:

Se você ficou com dúvida de como percorrer os marcadores para utilizar a função que acabamos de criar, nós exemplificamos no código do próximo tópico.

2.2 – Utilizando Rotas nos mapas

Você deve ter notado alguns ícones no canto inferior direito da imagem anterior. De cima para baixo, temos primeiro o ícone de uma bússola. Em nossa aplicação, aquele botão simboliza a criação de uma rota da posição do usuário até algum marcador no mapa. 

Para implementar essa funcionalidade, utilizamos a biblioteca ‘’react-native-maps-directions”. Instale-a seguindo as instruções no link. Essa biblioteca utiliza uma API do Google chamada “Google Maps Directions API”. Você deverá adicioná-la ao seu projeto utilizando a “Google Maps Platform” da mesma forma que adicionou as outras API‘s.

O código abaixo gera uma rota entre a localidade do usuário e um marcador selecionado no mapa:

/* src/Screens/Home/index.js */
//importando as dependências necessárias
import React, { useState } from 'react';
import MapView from 'react-native-maps';
import useLocation from '../../Hooks/useLocation';
import MapViewDirections from 'react-native-maps-directions';	//biblioteca necessária para utilização das rotas
import { markers } from '../../utils/Markers';	//objeto markers com os marcadores e suas informações
import MarkerImpl from '../../Components/MarkerImpl';	//marcadores implementados no tópico anterior

export default function HomeScreen({ navigation }) {
  	const [latitude, setLatitude] = useState(-20.398259);	//posições padrão
  	const [longitude, setLongitude] = useState(-43.507726);	//posições padrão
  	const [mapMarkers, setMapMarkers] = useState(markers);		//guardando os marcadores estáticos 

	const [localDirection, setLocalDirection] = useState(null);	//vamos utilizar este estado para guardar o local que será o ponto final da rota
 	const { coords, errorMsg } = useLocation();     //utilizando código que criamos anteriormente para localização do usuário

	  function handleRegionChanged(region) {
    		setLatitude(region.latitude);
    		setLongitude(region.longitude);
 	 }	//utilizamos esta função para guardar a posição atual do usuário


	return (
    <MapView
              showsUserLocation={true}
              showsMyLocationButton={false}
              toolbarEnabled={false}
              	  onRegionChangeComplete={handleRegionChanged}	//armazenamos a posição atual do usuário sempre que ele mudar sua localização
              style={{
                height: '100%',
                width: '100%',
                position: 'absolute',
              }}
              initialRegion={{
                latitude,
                longitude,
                latitudeDelta: 0.195,
                longitudeDelta: 0.1921,
                ...coords           
              }}
            >

{
                  mapMarkers.map((_marker) => {		//percorrendo os marcadores para mostrarmos cada um deles no mapa
                    return (
                      <MarkerImpl
                        key={_marker._id}	//enviando o id do marcador
                        mark={_marker}	//enviando informações do marcador para ser renderizado
                        onPress={() => setLocalDirection(_marker)}		//essa função será essencial para a criação da rota, já que ela vai guardar na variável localDirection a  posição final da nossa rota.
                      />
                    )
                  })
              }


                  <MapViewDirections
                    strokeWidth={3}		//tamanho da linha que vai demarcar a rota no mapa
                    strokeColor=""red""	//cor da linha
                    origin={coords}		//a posição do usuário
                    destination={localDirection}	//o local final da rota
                    apikey={SUA KEY AQUI}	//sua api key
                    mode=""DRIVING""	//determinando o tipo de transporte utilizado para calcular a rota da melhor forma
                  />
    
        </MapView>
	)
}


Observe que a rota é renderizada em nosso mapa a partir do momento em que o objeto passado dentro de “destination” é alocado. Ele será alocado sempre que o usuário pressionar algum marcador no mapa. Teremos como resultado a seguinte imagem:

3 – Considerações Finais

Como você pode observar durante o artigo, mostramos apenas algumas funcionalidades básicas existentes em uma única API para lidar com mapas em React Native, porém, há muito que se explorar nesta área, de modo que é essencial buscar e conhecer melhor as ferramentas existentes no mercado.  

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software.

4 – Referências

Qual a diferença entre GPS, A-GPS e GLONASS?

Get Started | Maps SDK for Android

Tutorial escrito por Vinícius de Paula Silva. Revisado por Prof. Tiago Carneiro e Koda Gabriel."
Automação de testes para backend NodeJS utilizando o framework Jest,http://www2.decom.ufop.br/terralab/automacao-de-testes-para-backend-nodejs-utilizando-o-framework-jest/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/11/0411-730x350.png,"Mediante o crescimento do uso da computação no dia a dia das pessoas tornou-se cada vez mais difícil criar softwares de qualidade utilizando testes manuais. Os softwares ficam cada dia maiores e mais robustos, aumentando as chances de erros passarem despercebidos para a versão de produção.

Testes automatizados são scripts que utilizam as entradas e saídas de um software para simular um usuário ou um sistema. Desta forma, podemos ter uma grande área de cobertura dos testes que pode ser automatizada sem implicar em grandes custos.

Existem vários tipos de testes, mas para esse artigo utilizaremos os termos teste unitário e teste de integração. Neste contexto, são considerados testes unitários somente os testes que verificam funções puras, isto é, independente do número de vezes que rodarem uma mesma entrada, sempre irão retornar o mesmo resultado (exemplo: uma função que multiplica dois números, ela nunca pode retornar um número que não seja 6 ao receber 3 e 2). Por outro lado, são considerados testes de integração aqueles testes de software em que módulos são combinados e testados em grupo. Por exemplo, quando algum teste faz chamadas na API de um serviço em nuvem, fazendo uso das rotas do que pertencem a API deste backend, supõe-se que essas rotas sejam funções mais complexas, que podem envolver vários módulos para realizar a tarefa demandada.

Este artigo tem por objetivo apresentar uma visão geral dos passos necessários para a automação dos testes de um backend NodeJS por meio do framework para desenvolvimento e automação de testes denominado Jest. Para isso, vamos instalar o Jest.

Instale e configurando o Jest

Primeiro, vamos criar um pasta para armazenar nossos arquivos de teste com o nome “__tests__”, depois instalamos a biblioteca jest como dependências de desenvolvimento.

 yarn add jest -D

Após instalarmos o jest, poderemos usar o comando “yarn jest –init”. Serão feitas algumas perguntas, onde iremos configurar da seguinte forma: 

“Você gostaria de usar o jest quando rodar o script “test” no arquivo package.json?” : se respondermos sim, é criado automaticamente um script no nosso package.json para rodar os testes.
“Escolha o ambiente de testes que será usado para os testes.”: Pergunta o ambiente que estamos desenvolvendo, no nosso exemplo estamos usando o NodeJS.
“Você quer que o jest adicione relatórios de cobertura?”: Se respondermos sim, o Jest exibe a porcentagem de testes verificados..
“Qual provedor deve ser usado para instrumentar o código para cobertura?”:  Qual será a ferramenta que será usada para cobrir os testes.
“Limpar automaticamente chamadas mocks e instâncias entre cada teste?”: Se respondermos sim, limpa automaticamente mocks e instâncias dos testes.
The following questions will help Jest to create a suitable configuration for your project

√ Choose the test environment that will be used for testing » node
√ Do you want Jest to add coverage reports? ... yes
√ Which provider should be used to instrument code for coverage? » v8
√ Automatically clear mock calls and instances between every test? ... yes


Após todas as perguntas serem respondidas será gerado automaticamente o arquivo “jest.config.js”. Neste arquivo podemos configurar algumas coisas para os nossos testes, nele procuraremos por “testMatch”:

// The glob patterns Jest uses to detect test files
// testMatch: [
// ""**/__tests__/**/*.[jt]s?(x)"",
// ""**/?(*.)+(spec|test).[tj]s?(x)""
// ],

Devemos alterá-lo para que possa ler todos os arquivos na pasta “__test__” que terminam com “.test.js”( Ex: “user.test.js”). Isso ajuda a evitar que haja confusão entre arquivos de teste e arquivos que carregam o conteúdo da nossa aplicação.

// The glob patterns Jest uses to detect test files
testMatch: [
""**/__tests__/**/*.test.js?(x)"",
],

Ainda no arquivo de configurações do Jest temos a variável “bail”, que por padrão vem com o valor “0” (o mesmo que “false”) como mostram as imagens abaixo. Deve-se definir o seu valor como true, ou um número diferente de zero, para que os testes sejam interrompidos caso ocorra algum erro. Dessa maneira, realizamos as correções necessárias mais rápido sem a necessidade de aguardar todos os testes acabarem de rodar.

module.exports = {
//All imported modules in your tests should be mocked automatically
//automock: false,

// Stop running tests after `n` failures
//bail: 0,

// The directory where Jest should store its cached dependency information
// cacheDirectory: ""C:\\Users\\emanu\\AppData\\Local\\Temp\\jest"",

//Automatically clear mock calls and instances between every test
clearMocks: true,
// Stop running tests after `n` failures
//bail: true,


Podemos também configurar ao projeto a biblioteca dotenv, depois criar um arquivo “.env” e “.env.test”. Neles irão ficar salvas algumas configurações. No exemplo é usado o banco de dados MongoAtlas, então dentro dos arquivos env adicionamos “MONGO_URL=url” do banco de dados. No caso do “.env.test” é adicionada a url usada apenas para testes.

yarn add dotenv

No “package.json” podemos adicionar um script para facilitar na hora de rodar os testes. Além disso, caso use “nodemon”, podemos adicionar a linha de comando “–ignore __tests__” para que quando o nosso servidor estiver rodando localmente, não seja reiniciado toda vez que fizermos alguma mudança dentro da pasta “__tests__”. Também utilizaremos o “–detectOpenHandles –forceExit”, caso ocorra algum erro que impeça que o jest saia corretamente e fique em aberto, rastreando o motivo do erro. No início de cada script tem escrito “set NODE_ENV=<ambiente de desenvolvimento> &&”. Isso é necessário para saber, em tempo de execução, qual é o ambiente que está sendo executado de forma automática.

""scripts"": {
    ""dev"": ""set NODE_ENV=dev && nodemon src/index.js --ignore tests"",
    ""start"": ""set NODE_ENV=production && node src/index.js"",
    ""test"": ""set NODE_ENV=test && jest --detectOpenHandles --forceExit""
},


No arquivo “index.js” usamos os seguintes comandos:

require(""dotenv"").config({
path: process.env.NODE_ENV === 'test’ ? '.env.test' : '.env'
});


Estes comandos fazem com que o programa leia arquivos .env diferentes de acordo com o ambiente de desenvolvimento informado no NODE_ENV, que pode ser acessado usando “process.env.NODE_ENV”.

A estrutura geral de um teste em Jest

O jest oferece duas principais funções para especificação de teste: Podemos utilizar o “describe()” e o “it()”. A “describe” é usada para descrever o que os testes irão fazer dentro dele, de uma forma geral (ex: serviços de autenticação de usuário). A função  “it” é usada para pegar os testes de forma mais individual (ex: login ou cadastro de usuário). 

describe('sum test', () => {
beforeAll(async (  => {
console.log('Before all');
});
beforeEach(async () => {
console.log('Before each');
});
afterAll(async () => {
console.log('after all');
});
afterEach(async () => {
console.log('after each');
});

});


Na figura abaixo, pode-se perceber, pela impressões no terminal das palavras “all” e “each”, quando cada um é executado.

yarn run v1.22.4
$ jest
PASS __tests__/unit/sum.test.js
sum test
√ should sum to numbers (6 ms)

console.log
Before all

at Object.<anonymous> (__tests__/unit/sum.test.js:5:17)

console.log
Before each

at Object.<anonymous> (__tests__/unit/sum.test.js:8:17)

console.log
after each

at Object.<anonymous> (__tests__/unit/sum.test.js:14:17)

console.log
after all

at Object.<anonymous> (__tests__/unit/sum.test.js:11:17)

Test Suites: 1 passed, 1 total
Tests: 1 passed, 1 total
Snapshots: 0 total
Time: 1.278 s
Ran all test suites.
Done in 2.41s.

Antes e depois dos testes

Muitas vezes, em um teste é necessário deletar o banco de dados que foi criado, para que quando os testes forem executados novamente, a execução anterior não impacte sobre as próximas, seja sobrecarregando o banco de dados ou impedindo a criação de um novo usuário cujo e-mail é usado no código do teste. 

Para estas tarefas de inicialização, limpeza  e verificação de contexto, que comumente precedem a maioria dos testes, o Jest oferece  as funções “beforeEach” e “afterEach()” que são executados antes e depois de cada teste (código dentro de um bloco “it”), e também oferece a função “beforeAll” e “afterAll” que são executados uma vez apenas, antes e depois todos os testes (código dentro de um bloco “describe”). Na figura acima, é possivel observar como estas funções funcionam. 

Se você é um iniciante, você precisa ser alertado de que gerar novos modelos dentro do banco de dados da API de uma serviço em estágio de produção está longe do ideal, nem mesmo é indicado realizar e testes direto sobre o ambiente de desenvolvimento. Isto poderia gerar conflitos nos testes ou criar usuários “fantasmas” dentro das aplicações deste serviço ou, até mesmo, apagar registros do banco de dados em produção/desenvolvimento. Então, devemos criar um banco de dado apenas para testes. Como estamos utilizando o banco de dados Mongo Atlas e seu cluster na nuvem, iremos configurar nosso banco de dados para conectar-se a um novo “database” da seguinte maneira.

describe('Users tests', () => {

const MOONGO_URL = <url banco de dados>

breforeAll(async() => {
await mongoose.connect(MOONGO_URL,
{ useNewUrlParser: true, useUnifiedTopology: true }, (err, db) => {});
});
}


Desta maneira, tudo que for criado durante os testes será salvo no banco de dados que usa a url salva na variável ‘MONGO_URL’ que pode ser um novo banco de dados apenas para os testes. Para evitar conflitos quando executar os testes outra vez e poupar espaço e armazenamento devemos deletar tudo que foi criado durante os testes. Para isso podemos usar o comando “await mongoose.connection.db.dropCollection(“<nome da coleção que será deletada>”)”.

afterAll(async () => {
	await mongoose.connection.db.dropCollection(‘User’);
});

Um primeiro cenário de teste

Para exemplificar como funcionam os testes, serão feito dois exemplos simples de testes de soma de variáveis. Temos uma função que retorna a soma correta (sum.sum()) de dois números e uma que não (sum.wrongSum());

const sum = require('../../src/util/sum');

describe('sum test', () => {
it('should sum to numbers', () => {
const a = 1, b = 2;
const c = sum.sum(a, b);
expect(c).toBe(3);
});

it('should sum to numbers',() => {
const a = 1, b = 2;
const c = sum.wrongSum(a, b);
expect(c).toBe(3);
});
});


Dentro das aspas do “it” devemos colocar o que será feito pelo teste. Seguindo a lógica em inglês, o “it” é como parte do texto entre aspa “it should sum two numbers” (isto deve somar dois números). O “expect” é como o jest verifica se os resultados que foram retornados estão corretos. Quando escrevemos “expect(c).toBe(3)”, isso quer dizer: espera-se que “c” tenha o valor igual a 3.

yarn run v1.22.4
$ jest
 FAIL  __tests__/unit/sum.test.js
  sum test
	√ should sum to numbers (4 ms)
	× should sum to numbers (4 ms)

  ● sum test › should sum to numbers

	expect(received).toBe(expected) // Object.is equality

	Expected: 3
	Received: 4

  	11 |     	const a = 1, b = 2;
  	12 |     	const c = sum.wrongSum(a, b);
	> 13 |     	expect(c).toBe(3);
     	|               	^
  	14 | 	});
  	15 | });
  	16 |

  	at Object.<anonymous> (__tests__/unit/sum.test.js:13:19)

Test Suites: 1 failed, 1 total
Tests:   	1 failed, 1 passed, 2 total
Snapshots:   0 total
Time:    	1.405 s, estimated 2 s
Ran all test suites.
error Command failed with exit code 1.
info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.


O codigo acima ilustra como o Jest informa os testes que passaram e os que falharam. As falhas são apresentadas em vermelho. Acontecem sempre que ocorrer um problema no código do teste ou quando os resultados esperados e os recebidos analisados pelo “expect” forem diferentes.

Um cenário de teste um pouco mais completo 

Para testar rotas de um backend precisamos simular uma requisição. Usamos a biblioteca Supertest para isso, instalando-a como dependência de desenvolvimento. 

yarn add supertest -D


Após a instalação da biblioteca Supertest, precisamos descrever o backend Node JS que criamos apenas com o propósito de simular a automação de seu test. Consideramos um que ele oferecer um serviço de controle de sessão de um usuário, então, criamos o arquivo “index.js” para armazenar seu código. 

A imagem abaixo mostra o index.js que tem a rota que será testada. No exemplo a seguir, a única rota definida apenas simula a criação de um usuário que, caso seja bem sucedida, retorna o usuário criado. Caso seja mal sucedida, retorna os códigos relativos às falhas.

require(""dotenv"").config({
path: process.env.NODE_ENV === 'test’ ? '.env.test' : '.env'
});

const express = require('express');
const app = express();
const cors = require('cors');

app.use(express.json());
app.use(cors());

const router = express.Router();

app.use(router.post('/create', (req, res) => {
const { email, password, name } = req.body;

if(email == null || password == null || name == null)
return res.status(400).send({ error: ""Insuficient data"" });

try {
//create user
const user = {
email: email,
name: name,
password: password
};
return res.send({ user: user });
}
catch(err) {
return res.status(500).send({ error: err });
}
}));

const server = app.listen(process.env.PORT || 3333);
module.exports = server;


Após a definição da rota “create”, também é necessário importar os listeners do nosso backend Node JS. Para isso, é preciso salvar o retorno da função app.listen(), na variável server que futuramente será utilizada pelo código de teste como destinatário das chamadas à API. Para isso, os seguintes comandos são colocados ao final do código do backend:

“const server = app.listen(process.env.PORT || 3333);” 
“module.exports = server;” 

Finalmente, a imagem a seguir apresenta o código contido no arquivo “user.test.js” que criamos, na pasta “__tests__”, para implementar os testes do backend.

const server = require('../../src/index');
const request = require('supertest');
const mongoose;

describe('Users tests', () => {

const MOONGO_URL = <url banco de dados>
breforeAll(async() => {
await mongoose.connect(MOONGO_URL,
{ useNewUrlParser: true, useUnifiedTopology: true }, (err, db) => {});
});

it('should create a user', async () => {
const response = await request(server)
.post('/create')
.send({
name: ""name test"",
email: ""email@test.com"",
password: ""123""
});

expect(response.status).toBe(200);
expect(response.body.user.name).toBe(""name test"");
});

it('should create a user', async () => {
const response = await request(server)
.post('/create')
.send({
name: ""name test2"",
email: ""email2@test.com"",
});

expect(response.status).toBe(200);
expect(response.body.user.name).toBe(""name test2"");
});
});


Além disso, é necessário importar também o código do backend que desejamos testar “const server = require(‘../src/index’)”. Na imagem acima, uma referência para o backend é armazenada na variável server. A seguir,  a variável “request” a recebe a biblioteca Supertest, que é utlizada para enviar requisições à variável server utilizando o seguinte formato: “const response = await request(<aplicação>).<método>(<rota>).<forma de envio>(<conteúdo>);”. 

“aplicação”: É colocado a variável que usamos para importar a nossa aplicação Node JS, isto é, o backend. No exemplo, trata-se da variável server.
“método”: Define que método de requisição HTTP será utilizado. Por exemplo: put, delete, post, get, ….
“rota”: É o endereço da rota que faremos a requisição.
“forma de envio”: É como será enviada a informação, com .send({}) envia um body no formato json, .set(“<nome da variável>”, “<valor da variável>”) para um header .query(<variável> : <valor>).
“conteúdo”: É o dado que será enviado na requisição.

Para verificarmos os resultados da chamada a uma rota, podemos primeiro buscar saber o que esperar do “response.status” da rota, utilizando o padrão “http status code” (https://developer.mozilla.org/pt-BR/docs/Web/HTTP/Status). Assim, saberemos se o Jest recebeu o que era esperado ou se ocorreu algum erro. A seguir, a saída do backend pode ser acessada através da variável “response”. Outra forma de acessar o que o backend retorna seria usando o “response.body”, onde podemos acessar os campos individualmente, como feito na linha 39 do exemplo, usando “response.body.token”. A próxima figura mostra o resultado do teste que desenvolvemos para nosso backend Node JS. 

yarn run v1.22.4
$ jest
 FAIL  __tests__/unit/sum.test.js
  ● sum test › should sum to numbers

	expect(received).toBe(expected) // Object.is equality

	Expected: 3
	Received: 4

  	11 |     	const a = 1, b = 2;
  	12 |     	const c = sum.wrongSum(a, b);
	> 13 |     	expect(c).toBe(3);
     	|               	^
  	14 | 	});
  	15 | });
  	16 |

  	at Object.<anonymous> (__tests__/unit/sum.test.js:13:19)

Test Suites: 1 failed, 1 of 2 total
Tests:   	1 failed, 1 passed, 2 total
Snapshots:   0 total
Time:    	1.892 s, estimated 2 s
Ran all test suites.
error Command failed with exit code 1.
info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command.

Considerações finais

Este artigo mostrou os passos iniciais para quem deseja aprender a automatizar testes para aplicações Node JS por meio do framework de desenvolvimento de testes denominado Jest. Durante este tutorial, aprendemos a instalar e configurar as tecnologias Jest  e Supertest. Depois, implementamos testes unitários muito simples que permitem entender o funcionamento do framework Jest. E aí caro leitor, esse texto foi útil para você? Tem alguma sugestão? Ficou alguma dúvida? Deixe-nos comentários.

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software.

Tutorial escrito por Emanuel Jesus Xavier e Nicolas Vasca Galindo. Revisado por Prof. Tiago Carneiro e Kleiber Luiz da Silva."
Automação de testes para iniciantes: Configurando os ambientes de desenvolvimento e teste em dispositivos Android,http://www2.decom.ufop.br/terralab/automacao-de-testes-para-iniciantes-configurando-os-ambientes-de-desenvolvimento-e-teste-em-dispositivos-android/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/10/photo-1517292987719-0369a794ec0f-628x350.jpg,"Os aplicativos desenvolvidos para celulares, denominados de aplicativos mobile, podem ser implementados por meio de diversas linguagens de programação, tais como JavaScript, Java, C#, etc. Geralmente essas aplicações são criadas para os sistemas operacionais Android e iOS, que são os mais utilizados em celulares. Uma das bibliotecas utilizadas no desenvolvimento de aplicativos mobile é a denominada de React-Native, disponibilizada pelo Facebook. Por meio dela, os programadores conseguem criar aplicativos mobile de forma prática.

 Os testes automatizados são scripts pré-programados que são capazes de instruir  um dispositivo a executar determinados comandos de forma automática, evitando a ocorrência de falhas nos aplicativos antes de serem liberados para produção. Alguns exemplos de comandos realizados nos testes automatizados são: cliques em botões, preencher campos de texto e verificar dados. Por meio dessa automatização é possível reduzir o tempo demandado em relação aos testes manuais, e consequentemente o custo final. Para realizar tais testes pode-se utilizar a ferramenta do CukeTest, que é bastante amigável, simples e prática de se utilizar para a construção dos cenários de teste, que consistem no passo a passo de comandos que o aplicativo será instruído a executar automaticamente. 

Muitas vezes a maior dificuldade que um tester encontra é como dar os seus primeiros passos para conseguir realizar os testes automatizados em aplicativos utilizando um determinado ambiente de desenvolvimento. Mesmo tendo muitos materiais disponíveis na internet, nem sempre é fácil e prático de se encontrar o conteúdo desejado. Com isso em mente, desenvolveu-se este tutorial que é focado em apresentar de forma fácil, simples e prática, a metodologia inicial (passo a passo) para a realização de testes automatizados em aplicativos desenvolvidos no React-Native, no sistema operacional do Android, utilizando-se a linguagem de programação Javascript e as ferramentas CukeTest e Appium.

OBS: As seções: “Montando o ambiente  e configurando as ferramentas do Android Studio” e “Configurando o dispositivo real (smartphone) e executando o emulador” foram feitas baseado no curso “Testes funcionais de aplicações android com appium” de Wagner Costa. Disponível em: <https://www.udemy.com/course/testes-appium/>. Acesso em: 25 de out. 2020.

1)  Configurando as ferramentas do Android Studio SDK

Passo 1: Primeiramente devemos acessar o link  (https://developer.android.com/studio) para baixar e instalar o Android Studio, que é o ambiente de desenvolvimento de aplicativos Android.

Passo 2: Com o Android Studio instalado vamos executar o programa para baixar algumas ferramentas a partir do mesmo, entrando em “Configure” e logo em seguida “SDK manager”.

Passo 3: Assim que estivermos nesta nova tela “Android SDK”, já será possível visualizarmos o local onde a SDK do Android está instalado.  Devemos salvar este local, pois ele é de suma importância para uso posterior. Logo em seguida, devemos selecionar a versão do android desejada. As versões consideradas neste tutorial são as versões 7.0, 8.0, 9.0 e a 10 como pode ser observado na imagem abaixo. Depois disso, devemos selecionar “SDK Tools “ para adicionar mais componentes.

Passo 4: Quando selecionarmos “SDK Tools”, veremos as ferramentas disponíveis para uso. Desmarcamos a opção “Hide Obsolete Packages” e então devemos habilitar as opções:  “Android SDK Tools (Obsolete)”,“Android Build -Tools ”,“Android Emulator” e “Android SDK Platform-Tools”. Depois de habilitados devemos clicar na opção “Apply” para fazer a instalação.

Passo 5: Agora devemos configurar as variáveis de ambiente. No Windows 10, busque por variáveis do sistema e selecione a opção “Variáveis de Ambiente”.

Passo 6: Devemos agora clicar em “Novo”, criar a variável de ambiente “ANDROID_HOME” e adicionar o caminho da “Android SDK Location “ obtida no passo 3. Feito isso clicamos em “OK” para salvar as alterações.

Passo 7: Devemos agora adicionar ao valor da “Path” mais três valores de caminho: 

%ANDROID_HOME%\emulator para ter acesso ao emulator
%ANDROID_HOME%\platform-tools para ter acesso ao adb
%ANDROID_HOME%\tools\bin para ter acesso ao UIAutomatorViewer

Os valores estão disponíveis abaixo para cópia sem formatação.

%ANDROID_HOME%\emulator
%ANDROID_HOME%\platform-tools
%ANDROID_HOME%\tools\bin

Passo 8: Abra o Android Studio e crie um novo dispositivo virtual clicando no “AVD Manager”.

2) Escolhendo seu ambiente de desenvolvimento: Dispositivo físico ou  Emulador 

Para desenvolver aplicativos móveis, o desenvolvedor precisa de uma máquina Android para a qual o código será compilado. Para isso o desenvolvedor pode compilar o aplicativo móvel tanto em um dispositivo físico quanto em um emulador de dispositivos Android instalado em seu laptop, isto é, um dispositivo virtual. Esta seção de texto ensina a configurar qualquer uma destas alternativas.

2.1) Configurando um dispositivo físico Android para desenvolvimento

Se preferir, você pode utilizar um dispositivo físico para realizar seus testes, essa é a melhor opção. Para desenvolver usando emuladores é preciso um laptop com  um bom processador e muita memória RAM, pelo menos de 4GB  e recomendável 8GB. Além disso, configurar um dispositivo móvel para desenvolvimento é algo bem mais simples do que preparar o emulador. 

Passo 9.1: No dispositivo físico, primeiro você deve habilitar as opções do desenvolvedor dele. Elas vêm desabilitadas por padrão. Acesse as configurações e selecione a opção “Sobre o dispositivo”, a localização deste botão pode variar de acordo com a versão android. Na tela “sobre o dispositivo”, vá em “Número da versão” e toque 7 vezes (você verá uma mensagem confirmando que as opções de desenvolvedor foram habilitadas). 

Passo 9.2: Após habilitado volte em “Sobre o dispositivo” e você verá a nova opção disponível “Opções do desenvolvedor”, clique nela. 

Passo 9.3: Vá na parte “Depuração” e ative a opção “Depuração USB”.

2.2) Configurando um emulador Android para desenvolvimento

Agora que já se sabe como configurar um dispositivo físico para desenvolvimento, ainda falta instalar e preparar as ferramentas para automação de testes, conforme discutiremos à frente. Mas antes disso,  a seguir discute-se aquilo que é necessário para  configurar um emulador Android e montar um dispositivo virtual no  laptop de um desenvolvedor.  

Passo 9.1: Depois de criado seu dispositivo virtual devemos apenas executá-lo que ele logo aparecerá na tela.

Passo 9.2: Depois do emulador ser executado pelo menos uma vez podemos executar o dispositivo virtual digitando os seguintes comandos no terminal:

emulator -list-avds  
cd %ANDROID_HOME%
cd tools
emulator @Nome_do_seu_emulador
3) Configurando o ambiente de automação de teste

Muito bem, após instalar e configurar o Android SDK e habilitar o ambiente de desenvolvimento do dispositivo Android físico ou virtual, é preciso instalar e configurar o ambiente de automação de testes.  Para isso, serão seguidos os seguintes passos: Execução do UIAutomatorViewer para inspeção de propriedades dos objetos do aplicativo em tempo de execução;. instalar e configurar o ambiente de desenvolvimento (IDE) de testes de nome CukeTest que permite a execução automatizada e a especificação  de cenários de testes segundo a técnica BDD (do inglês Behavior Driven Development); e, finalmente, instalar e configurar o framework Appium destinado ao desenvolvimento de testes de interface (caixa preta) para aplicativos móveis.  

3.1) Executando a ferramenta de inspeção de interfaces  “UIAutomatorViewer” 

O UIAutomatorViewer é uma ferramenta GUI, interface gráfica do usuário, instalada junto com o Android SDK, especificamente, junto do pacote Android SDK tools instalado no Passo 4 deste tutorial.. Esta ferramenta é capaz de exibir as propriedades dos elementos de uma página de um aplicativo Android, como por exemplo textos de botões, nomes das páginas do aplicativo, ou seja, todos os componentes presentes nas páginas de um aplicativo móvel. Consequentemente, ela é muito útil para o desenvolvimento de testes funcionais de interfaces gráficas com o usuário (do termo inglês GUI – Graphical User Interface). 

Passo 10: Para executar o UIAutomatorViewer devemos digitar “uiautomatorviewer” no terminal  do Windows (cmd).

Código para cópia sem formatação disponível abaixo.

uiautomatorviewer

Passo 11: Para identificar os componentes e suas propriedades durante os testes de um aplicativo com o UIAutomatorViewer, basta executar o aplicativo no dispositivo físico ou virtual, neste caso a figura mostra um aplicativo de calculadora sendo inspecionado. Para isso, é preciso clicar no botão  “Device screenshot“ do UIAutomatorViewer, conforme aponta a seta verde de número 1 na área superior  da imagem abaixo. Logo em seguida, clique em cima do componente que se quer inspecionar, neste caso, o botão 7 apontado pela seta verde de número 2. Finalmente, os valores das propriedades  e dependências do componente selecionado são apresentados no lado direito da tela do UIAutomatorViewer, conforme aponta a seta verde 3.

3.2) Instalando o ambiente integrado de desenvolvimento de testes “CukeTest”

Depois de executar o UIAutomatorViewer para inspeção de propriedades dos objetos do aplicativo em tempo de execução, é necessário instalar o ambiente integrado de desenvolvimento de testes do CukeTest. Trata-se de um ambiente de desenvolvimento utilizado para a criação rápida de scripts de testes Cucumber.js em JavaScript. 

Para realização de testes automatizados tanto em dispositivos web, desktop e mobile, entra em cena a ferramenta Cucumber. Ela é instalada junto com o CukeTest e é capaz de executar de forma automatizada cenários de testes especificados segundo a linguagem BDD. Para isso, ela  exige que as estruturas gerais de todos cenários de teste especificados sejam implementados em um framework de desenvolvimento de testes unitários ou funcionais de escolha do Engenheiro de Teste. A boa notícia é   que a ferramenta Cucumber é capaz de executar testes implementados em diversas linguagens e frameworks de testes  Neste tutorial, será abordado o uso do framework Appium destinado ao desenvolvimento de testes para dispositivos móveis, na linguagem Javascript. É importante pensar na portabilidade das implementações dos testes! O Cucumber é uma solução viável para essa questão. 

A ferramenta Cucumber utiliza cada implementação de teste para executar diversos cenários. Para cada cenário especificado em um teste, a ferramenta Cucumber invoca a implementação deste teste passando como parâmetro os dados específicos do cenário em questão. Assim, os Engenheiros de Teste podem e devem ser incentivados a generalizar as implementações dos testes de forma a servirem a cada vez mais cenários. É importante pensar no reuso de código dos testes também! 

Passo 12: Para instalar o, acesse o site http://cuketest.com/ ou procure pelo CukeTest no navegador e vá em: “Download now”

Passo 13: Depois disso, basta clicar no instalador e seguir o seu próprio passo a passo de instalação. Assim que finalizar a instalação, inicie o aplicativo do CukeTest e vá na opção “File” do menu e depois clique na opção “Open Project”:

Passo 14: O menu abrirá e é preciso localizar a pasta do seu repositório no seu computador. Percorra o caminho da pasta até chegar na raiz da pasta dos arquivos de teste. No nosso caso, eu vou clicar dentro da pasta geolarica-mobile:

Passo 15: Localizamos a pasta raiz dos testes que se chama “TestesGeolarica”, pois ela é a primeira pasta de testes que nós encontramos no repositório do nosso computador e vamos clicar em “Selecionar pasta” para abrir dentro do CukeTest:

Feito isso, aparecerá à seguinte tela:

Passo 16: Para finalizar basta configurarmos o arquivo “get_driver” de acordo com as configurações do seu emulador ou celular. Na propriedade “platformName”, insira o sistema operacional do seu dispositivo físico ou virtual, em “deviceName” digite o nome que o seu computador reconhece o seu celular ou emulador como device (você consegue ver pelo terminal do Windows, por exemplo, conectando o seu emulador ou celular pelo cabo USB e rodando “adb devices”. Para isso você precisa instalar o comando adb devices no terminal do CMD), a “platformVersion” é a versão do seu Android ou iOS ou do sistema operacional do  emulador. O appPackage e o appActivity são propriedades do APK do seu aplicativo, confira se o “host” e a porta “port” estão de acordo com as configuradas no servidor do Appium. Como o código do CukeTest 1.5.7.0, por padrão, vem faltando uma vírgula na linha 15 depois do termo: “true”, inserimos esta vírgula:

Passo 17: Feito isso, basta adicionar a pasta “node_modules” clicando com o botão direito na barra que está escrito o nome do seu projeto (“Testes Geolarica”, no meu caso) e clicando na opção “show in CMD Window”, que está em cima da pasta “features” para abrir o terminal do CMD dentro da pasta do seu repositório. Digite o comando: “yarn add install” e aperte enter para finalizar a instalação da pasta node_modules do seu projeto. 

3.3) Instalando o framework de testes funcionais de interfaces “Appium”

Appium é um framework gratuito para automação de teste de software, que funciona  em aplicativos da web e mobile. Por meio do protocolo denominado de WebDriver ele é capaz de guiar aplicativos desenvolvidos para os sistemas operacionais iOS, Android e Windows, fazendo a comunicação da ferramenta do Cucumber, usada para executar testes automatizados em um formato BDD cujas implementações se comunicam com dispositivos físicos ou emuladores. 

Passo 18: Para baixar o Appium, pesquise por “appium” no seu navegador busque pela página apresentada na figura a seguir. Ao encontrar esta página, clique em “download appium”. Para obter os mesmos resultados obtidos neste tutorial utilizamos a versão 1.7.2, pois ela é mais estável e é a utilizada no TerraLab:

Passo 19: Depois de instalar o Appium, o executamos e clicamos em “Start Server v1.7.2”.

E aparecerá a seguinte tela mostrando o Appium aberto e pronto para realizar a comunicação entre o Cucumber e o seu emulador ou dispositivo físico. Feito isso, você já pode realizar o seu primeiro teste mobile.

3.4) Resolvendo problemas 

Se o modo depuração não estiver habilitado no seu celular, vai aparecer uma mensagem nele pedindo para você habilitar o modo depuração. Caso você esteja utilizando o emulador não se preocupe, pois o modo depuração já vem ativado por padrão. No caso do celular, se não aparecer, remova o cabo USB, conecte de novo e dê a permissão para o modo depuração. No emulador, não é comum o device não aparecer, mas se não aparecer, tente fechá-lo e abri-lo novamente.

Para conferir se o seu dispositivo físico ou emulador foi devidamente reconhecido pelo seu computador, basta instalar  e executar a ferramenta denominada de ADB devices.

Depois de instalar a ferramenta do “ADB Devices”, entre no terminal de comando do “CMD” do Windows, digite o comando: “adb devices” e aperte a tecla “Enter”. Você deverá visualizar o nome do seu dispositivo e na frente dele o termo “device”.  Para demonstrar o funcionamento do dispositivo ou emulador, utilizou-se um dispositivo físico. Após entrar no terminal do CMD do Windows, digitou-se o comando “adb devices” e apertou-se a tecla “Enter”, fazendo com que o terminal exibisse o nome do dispositivo físico utilizado. Na frente do nome do dispositivo físico apareceu a mensagem “unauthorized” por que o modo de depuração não havia sido autorizado no dispositivo físico: 

Para ativar o modo de depuração do seu dispositivo físico basta utilizar novamente o comando “adb devices” e apertar a tecla “Enter”, que aparecerá uma mensagem no seu dispositivo físico pedindo a depuração. Basta dar  um “OK” nesta mensagem e aparecerá a seguinte tela:

Assim que aparecer escrito “device” na frente do seu dispositivo físico ou emulador, ele estará adequadamente configurado para os testes. Depois disso, basta prosseguir com a realização do primeiro teste mobile.

4) Realizando o primeiro teste mobile

Agora que já está com o ambiente configurado é hora de ver na prática como as ferramentas funcionam. Para realizar o nosso primeiro teste, clique aqui para baixar um modelo de teste simples, em que ocorrem uma série de cliques e o texto “Meu primeiro teste”  é escrito.

Feito isso,  adicione a pasta node_modules na raiz desse projeto: entre na raiz da pasta do seu computador em que você baixou o projeto modelo acima, entre no terminal do CMD nesse caminho de pasta, digite “yarn add install” e aperte a tecla “Enter”. Terminado esse processo, a pasta node_modules terá sido adicionada adequadamente à raiz do projeto mobile.  


É preciso instalar também o aplicativo que vamos utilizar para os testes. Baixe e instale-o no seu dispositivo ou emulador que utilizará para os testes, por meio deste link:



Seguindo a semântica do BDD que é um conjunto de práticas que visa auxiliar na construção de um produto auxiliando os envolvidos na compreensão do que está sendo construído, temos um cenário estruturado para esse teste. 

4.1) Cenário de teste mobile desenvolvido

Este é o nosso cenário: Usuário digita e visualiza um texto.

Dado: Usuário está na tela que possibilita digitação

Quando: Digito o texto “Meu primeiro teste” 

Então: O texto digitado aparecerá na tela

Passo 20: Antes de começar, certifique-se de que seu servidor do appium foi iniciado. Ao abrir o projeto pelo CukeTest teremos esse cenário implementado. Não se esqueça de modificar  o arquivo “get_driver” de acordo com as instruções dadas anteriormente, pois é preciso alterar o “deviceName” de acordo com seu emulador ou dispositivo móvel.

Passo 21: Clique em “run this scenario” e o teste começará (Lembre-se de que se estiver utilizando dispositivo físico, de desbloquear o aparelho e o conectá-lo no computador via porta USB, antes de iniciar o teste). Após iniciar o teste aguarde um pouco e logo seu emulador irá abrir. Quando o teste começa, automaticamente o aplicativo será aberto em seu emulador/celular. Você pode acompanhar o teste pelo tela do aplicativo e pelo CukeTest.

O CukeTest oferece uma barra de saída, denominada de “output” em que podemos acompanhar o progresso do teste através dessas bolinhas verdes, que indicam cada etapa que passar com sucesso no teste. No final nos é informado o número total de etapas e de cenários compilados e quantos obtiveram sucesso quanto ao resultado esperado.

Para acompanhar seu teste pelo aplicativo rodando basta observar seu emulador/celular. Você verá a funcionalidade sendo executada de forma automatizada. Após conseguir rodar o primeiro teste você poderá criar seu próprio cenário e comandos, executando testes para outras funcionalidades, utilizando o mesmo aplicativo.

Passo 22: Antes de criar novas funções, você deve abrir a pasta “step_definitions” e em seguida abra o arquivo “definitions.js”, onde você vai encontrar alguns comandos já criados. A ideia é você ver se a função que você deseja já foi criada, uma vez que a proposta do CukeTest é promover o reuso dos códigos. 

Passo 23: Depois de verificar os comandos disponíveis é hora de criar seu novo cenário.
Para isso, nesse mesmo projeto, clique com o botão direito na pasta “features”. Em seguida clique em “New File” e escolha o nome do seu arquivo, utilizando a extensão “.feature”

Passo 24: Após a criação do novo arquivo você poderá alterar o nome da funcionalidade em <Feature Name > e em seguida adicionar um novo cenário em <add new scenario> :

Passo 25: Você pode alterar seu cenário e criar os comandos que desejar. Para isso dê um clique duplo no passo que for alterar. Essa é uma maneira mais fácil de visualizar seus cenários, porém, você pode optar por utilizar outro meio de visualização em que o cenário aparecerá em forma de texto, o que pode tornar a criação dos testes mais rápida.

Passo 26: Para criar um passo você pode utilizar a frase que desejar, ficando atento às propriedades  dos componentes, que são colocadas entre aspas. Ao preencher o campo com o comando desejado o CukeTest irá identificar se a função já está implementada ou não. Se já existir o código a cor da seta na frente muda para verde e se clicar nela será redirecionado para o trecho do “definitions.js” em que a mesma foi criada. Se for um comando novo, a setinha ficará cinza e ao clicar nela o escopo da função com os parâmetros já serão criados no lugar certo com o comando ”return ‘pending’;” dentro do arquivo “definitions.js”, tornando a seta alaranjada. Quando a função for realmente implementada a seta ficará verde e você já poderá executar este passo.

Neste link você encontra a documentação do WebdriverIO, que foi a biblioteca utilizada para encontrar as funções para localizar e interagir com os elementos desejados (botões, páginas, imagens, etc), ou seja, realizar ações de clique, verificar textos e demais funções para você utilizar no seu “definitions.js”: http://v4.webdriver.io/api.html

Para localizar os atributos dos componentes do aplicativo, basta verificar suas propriedades através da ferramenta do UIAutomatorViewer, como mostrado nos passos 10 e 11 deste tutorial. 

Considerações Finais 

Neste artigo, explicamos como implantar um ambiente de desenvolvimento de testes automatizados para aplicativos móveis desenvolvidos para dispositivos e emuladores Android.  Para isso, apresentamos um conjuntos de passos agrupados nas seguintes fases: Instalação do Android Studio SDK e  do UiAutomatorViewer; Instalação do CukeTest e do Cucumber;  Instalação do Appium e, finalmente, implementação e execução de um cenário de teste. Com isso, esperamos servir como uma porta de entrada para aqueles que buscam se iniciar no mundo da automação de testes para aplicativos móveis. Por enquanto, abrangemos somente a plataforma Android. No futuro, esperamos tratar da plataforma iOS. 

No projeto que disponibilizamos, pode-se encontrar algumas funções que te ajudarão nos testes como funções para clicar, escrever e verificar, as quais você usará de acordo com a demanda da funcionalidade. À medida que for praticando e fazendo vários testes você vai se familiarizar com a ferramenta e consequentemente criará testes cada vez mais robustos.
Estude novas funções e crie diferentes exemplos de testes. Assim que possível nos dê um feedback sobre a utilidade deste tutorial. Gostaríamos de obter sugestões que ajudem a construir tutoriais cada vez melhores.

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software.

Tutorial escrito por Bruno Cota Silva, Ray da Silva Basilio, Vinícius Almeida de Mattos. Revisado por Prof. Tiago Carneiro.

Referências

ANDROID STUDIO. Android Studio provides the fastest tools for building apps on every type of Android device. Disponível em: https://developer.android.com/studio. Acesso em: 28 out. 2020.

APPIUM. Automation for Apps. Disponível em: http://appium.io/. Acesso em: 28 out. 2020.

BUI, Trong. Testes de automação com Cucumber BDD em times ágeis. 2018. Disponível em: https://www.infoq.com/br/articles/cucumber-bdd-automation-testing/. Acesso em: 28 out. 2020.

BUSINESS WORLD – Innovative Technologies. Best Mobile Automation Testing Tools. Best Mobile Automation Testing Tools. 2019. Disponível em: https://www.businessworldit.com/automation/best-mobile-automation-testing-tools/ . Acesso em: 28 out. 2020.

COCOA, Kazu. GITHUB: ApiDemos-debug.apk. 2020. Disponível em: https://github.com/appium/appium/blob/master/sample-code/apps/ApiDemos-debug.apk. Acesso em: 28 out. 2020.

COSTA, Wagner. Curso da plataforma UDEMY: Testes funcionais de aplicações Android com Appium. 2020. Disponível em: https://www.udemy.com/course/testes-appium/. Acesso em: 25 de out. 2020.

CUCUMBER. Cucumber Concepts. Disponível em: http://cuketest.com/en/cucumber/concepts#cucumber. Acesso em: 28 out. 2020.

CUKETEST. Quickly create test scripts with CukeTest. 2015. Disponível em: http://cuketest.com/. Acesso em: 28 out. 2020.

DAUER, Stella. ADB: drivers, ferramentas, comandos e mensagens de erro. 2019. Disponível em: https://www.nextpit.com.br/adb-drivers-android-windows. Acesso em: 28 out. 2020.

FEUERWEHR. Verwaltungssoftware Fireplan ausgezeichnet. 2017. Disponível em: https://www.feuerwehr-ub.de/aktuelle-meldungen/verwaltungssoftware-fireplan-ausgezeichnet/ (adaptado). Acesso em: 28 out. 2020.

MAMEDE, Elisabeth. BDD e Testes de Software. 2018. Disponível em: https://medium.com/@elisabethmamede/bdd-e-teste-de-software-a708df3502e. Acesso em: 28 out. 2020.

PRIMEIROTESTZIP.zip. Disponível em: https://drive.google.com/file/d/1xQIfQFkEXmpVyV2bVt_vta0-XzjWcgFQ/view. Acesso em: 28 out. 2020.

SILVA, Rafael da.  Engenheiro de testes. 2012. Disponível em: http://www.dsc.ufcg.edu.br/~pet/jornal/setembro2012/materias/profissoes.html. Acesso em: 28 out. 2020.

SVLABS. Teste manual ou teste automatizado? 2018. Disponível em: https://blog.svlabs.com.br/teste-manual-ou-teste-automatizado/.  Acesso em: 28 out. 2020.

WEBDRIVERIO. Webdriverio API Docs. Disponível em: http://v4.webdriver.io/api.html. Acesso em: 28 out. 2020."
Entendendo o funcionamento do CICD dentro do Git flow,http://www2.decom.ufop.br/terralab/entendendo-o-funcionamento-do-cicd-dentro-do-git-flow/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/10/PostBG-730x350.jpg,"Se você utiliza o GitLab provavelmente já viu na interface informações sobre o CICD e sobre o Git flow. Caso ainda não utilize nenhuma dessas ferramentas, saiba que é possível organizar o seu projeto de uma forma padronizada, inteligente e automatizar grande parte das atividades que garantem uma maior qualidade para seu produto.

Utilizar o Git flow, além de garantir uma organização no desenvolvimento, permite ao próprio GitLab obter métricas a partir das atividades da equipe que podem ser usadas para guiar de forma mais precisa o desenvolvimento realizando acompanhamento do número de bugs injetados, quantas funcionalidades são feitas em uma sprint e qual o tempo médio que um desenvolvedor trabalha em determinadas funcionalidades. Já o CICD garante que seu produto esteja sempre passando por testes automatizados e seja entregue de forma rápida aos clientes, sem interferência da equipe. 

Nesse artigo iremos explicar primeiro um pouco do Git flow já que entendendo o Git flow se torna mais fácil perceber quando executar o CICD.

ENTENDENDO O GIT FLOW

O Git flow é um modelo de utilização que equipes de desenvolvimento podem seguir para organizar os branches e permitir uma utilização mais plena do Git. 

Considere os dois branches explicadas abaixo como trunks, isto é, branches fixos que irão existir durante todo o desenvolvimento de um produto.

Master: Contém o código de produção deste produto. Todo o código desenvolvido nos outros trunks são eventualmente integrados ao Master para que possam ser lançados junto à nova versão do produto. 

Develop: Contém o código em desenvolvimento deste mesmo produto e, comumente, permanece constantemente mudando e sendo incrementado. Esse é o trunk mais movimentado pois é aqui que cada nova funcionalidade é acrescentada ao longo de todo processo de desenvolvimento.

Todo repositório no GitLab é criado com um Master. O projeto é inicializado no Master e então imediatamente o trunk Develop é criado. A partir desse ponto, os desenvolvedores irão trabalhar somente na Develop.

Além dos trunks fixos, trabalhamos constantemente com os branches abaixo:

Feature: São branches criados a partir do trunk Develop para o desenvolvimento de uma funcionalidade específica. Cada funcionalidade nova deve ser desenvolvida em seu próprio branch que tem por convenção o nome iniciado por feature/nome-da-funcionalidade.

Nesse exemplo um novo branch Feature/* foi criado a partir do Develop. O desenvolvedor realizou três commits nesse branch e então fez um merge para o Develop. Esse merge produziu uma nova versão do Develop, dessa vez contendo a nova funcionalidade. 

A princípio, isso pode funcionar sem grandes problemas se existir somente um desenvolvedor no projeto, mas o que ocorre quando existem dois desenvolvedores ou mais trabalhando simultaneamente? Veja o exemplo abaixo:

Dois desenvolvedores criaram um branch a partir da mesma versão do trunk Develop. O primeiro desenvolvedor pediu um merge e o trunk Develop recebeu uma nova funcionalidade. O segundo desenvolvedor então acaba a funcionalidade em que está trabalhando e agora vai pedir um merge para o trunk Develop. Esse é o momento onde podem começar a existir conflitos de código. O desenvolvedor da primeira funcionalidade alterou algo que poderia impactar no que o segundo desenvolvedor fez? Ou até mesmo o contrário, o segundo poderia ter alterado algo que o primeiro teria usado do trunk Develop original? É por conta desses conflitos que é importante não sair simplesmente aceitando qualquer merge. Assim, a integração contínua (CI – Continuous Integration) surge como uma ferramenta valiosa para verificar os impactos de cada merge! 

Então, é chegado o momento de conhecer um novo branch utilizado no processo de desenvolvimento. 

Staging: Esse branch contém uma versão do produto em processo de homologação. Versões candidatas (release candidates) do código que estão sendo testadas para que possam, então, ser integradas ao master. Podem ocorrer casos onde, enquanto uma equipe continua trabalhando no trunk Develop já visando um próximo release, outra parte da equipe está corrigindo problemas que possam ter aparecido nas builds em staging. Qualquer alteração feita neste branch deve também ser incorporada no trunk Develop para propagar as correções realizadas. Por convenção, esse branch é chamado staging/*.*.* onde os asteriscos representam a versão do produto sendo preparada para lançamento seguindo as lógicas de versionamento utilizadas pelo time. 

Continuando o exemplo anterior, imagine que seu Product Owner (PO) e o Cliente concluíram que as duas funcionalidades implementadas antes eram suficientes para liberar uma versão do software. Um novo branch é criado a partir do trunk Develop e vai ser chamado de Staging/*.*.*

A partir desse momento, Staging caminha de forma independente do Develop. Ao criar o branch Staging, uma release experimental do software é liberado e aqui ocorre a entrega contínua (CD – Continuous Delivery) para o seu ambiente de testes. 

Após um tempo com o teste rodando, você recebeu alguns feedbacks e seu PO, junto ao Cliente, considerou que algumas alterações eram necessárias nas funcionalidades entregues pela versão em staging. Um dos desenvolvedores que haviam trabalhado nas funcionalidades anteriores agora foi alocado para trabalhar corrigindo esse problema encontrado. 

Um novo branch de Feature foi criada a partir do branch Staging. A funcionalidade foi corrigida e posteriormente o merge foi feito no Staging. Repare que, nesse momento, além do merge ser feito no Staging, ele também é feito no Develop.

Temos outro detalhe importante na figura acima. Reparem que, enquanto um dos desenvolvedores trabalhava estabilizando o trunk Staging, outro desenvolvedor continuou a implementação de novas funcionalidades no trunk Develop, que vinham sendo implementadas para um novo release futuro do produto. 

Agora que o problema encontrado na versão de Staging foi corrigido, após um período de testes essa versão do software foi considerada estável o suficiente para ser lançada. Um merge é requisitado do Staging para o Master dando origem a uma nova versão de produção, momento onde o CD novamente é executado, desta vez realizando o deploy no ambiente de produção. Mas o que acontece se uma falha de software (bug) é descoberta após uma versão do produto ser lançada? 

Hotfix: Um branch de hotfix é criado quando algum bug é encontrado na Master e precisa ser imediatamente corrigido, ou seja, não é possível esperar uma nova versão de desenvolvimento passar por homologação. Esse é um dos poucos momentos em que o Master é alterada diretamente. Por convenção, especificamos o nome do branch como hotfix/nome-da-funcionalidade.

Quando um usuário reportar uma falha crítica na versão de produção, um branch de hotfix será criado à partir do Master, o problema será corrigido e então esse branch será integrado tanto ao Master quando ao Develop para que a correção da falha se propague por todos os trunks.A figura abaixo ilustra esta situação.

Agora que você já conhece como funciona o Git flow, deve ter percebido em quais momentos o CI e o CD são executados. 

QUANDO EXECUTAR A INTEGRAÇÃO CONTÍNUA?

A integração contínua (CI – continuous integration) é executada sempre que um merge é requisitado para os trunks Develop, Master ou Staging. Isso vai garantir a integridade das principais branches do projeto. 

QUANDO EXECUTAR A ENTREGA CONTÍNUA?

A entrega contínua (CD – continuous delivery) deve acontecer sempre que um commit for feito para os trunks Staging ou Master. Ou seja, quando queremos fazer deploy no ambiente de testes ou de produção. 

Você já conhece a teoria, vamos ver então como criar um pipeline CICD? 

ENTENDENDO E CRIANDO O ARQUIVO DE EXECUÇÃO DO CICD

O CICD no GitLab é controlado pelo arquivo .GitLab-cy.yml, criado na raiz do projeto. Cada projeto vai ter seu proprio arquivo yml que atende suas necessidades específicas. A base de todo arquivo yml, não importando qual a natureza do projeto, seja ele um backend, frontend web ou frontend mobile, é a seguinte:

# Na linha abaixo selecionamos qual imagem docker deve ser utilizada para o CICD. No TerraLab trabalhamos utilizando React e o Node. Quase todos os nossos CICD’s são feitos a partir da imagem node:latest. É possível especificar uma versão do Node simplesmente trocando o latest pela versão por exemplo, node:10.

image: node:latest

# Às vezes, uma imagem não possui todos os serviços que desejamos. Esses serviços podem ser manualmente instalados, o que frequentemente demora mais do que o desejado e consome tempo do CICD, ou então podemos especificar o serviço desejado para ser integrado diretamente à imagem escolhida anteriormente. No caso abaixo, estamos informando que gostaríamos de possuir a última versão do Mongo como um serviço no nosso docker. 

services:
 - mongo:latest
 
# Os stages representam os passos na execução do pipeline do CICD. Temos os seguintes passos abaixo possíveis no nosso projeto, feitos com base no Git flow. É importante notar que nem todos os passos serão sempre executados. Por exemplo, é possivel executar somente o build e test, para um merge request, ou o stagingDeploy para realizar o deploy no ambiente de testes. 

stages:
 - build
 - test
 - stagingDeploy
 - productionDeploy

# Como nosso projeto é feito em node, sempre que precisamos executar algo pela primeira vez é necessário realizar o build. Artefatos gerados por um estágio do pipeline não são carregados para o próximo, a menos que isso seja especificado. No nosso caso, o build vai criar a pasta node_modules que deve ser guardada em cache para ser utilizada nos outros estágios. 

cache:
 paths:
  - node_modules/

# É aqui que começa a divisão dos estágios dentro do Git flow. Estamos especificando em quais situações o pipeline vai ser executado a menos que alguma alteração seja feita dentro do próprio estágio. 

.only-default: &only-default
 only:
  - master
  - develop
  - staging
  - merge_requests


Tendo definido os parâmetros acima no yml, agora começa a parte de código específica de cada projeto. O exemplo abaixo é para o CICD de um Backend.

# Abaixo estamos definindo cada um dos estágios especificados no script anteriormente. 

#Incluímos only-default dessa forma no estágio para especificar que ele deve obedecer a execução default que tínhamos descrito antes no arquivo. Ou seja, o build vai ser sempre executado não importa a situação. 

build:
 <<: *only-default
 stage: build
 script:
  - npm install

# O estágio de testes é executado somente quando ocorrem requisições de merge. Não queremos que ele fique executando sempre que alguém faz um commit. Por isso, trocamos o estado default para only: -merge_requests

test:
 only:
  - merge_requests
 stage: test
 artifacts:
  paths:
   - bin/
 script:
  - npm install
  - npm test

# De forma semelhante ao teste, queremos que o nosso deploy para Staging seja feito somente quando um commit for realizado na Staging

deployStaging:
 only:
  - staging
 stage: deployStaging
 script:
  - 'which ssh-agent || ( apt-get update -y && apt-get install openssh-client -y )'
  - chmod a+x disableHostKeyChecking.sh
  - bash ./deployStaging.sh
 
# Como fizemos para o Staging, queremos que o deploy para produção seja executado somente caso exista um commit na Master 

deployProduction:
 only:
  - master
 stage: deployProduction
 script:
  - 'which ssh-agent || ( apt-get update -y && apt-get install openssh-client -y )'
  - chmod a+x disableHostKeyChecking.sh
  - bash ./deployProduction.sh

CRIANDO O SEU PRÓPRIO RUNNER

Até então, o CICD apresentado acima é executado nos Runners providos gratuitamente pelo GitLab. Um Runner é uma máquina onde o CICD vai ser rodado. Ele pode estar localizado em alguma nuvem, como AWS, algum servidor ou até mesmo localmente no seu computador. Esses Runners são bons para projetos pequenos, porém possuem um tempo limite de execução por mês. Caso seu projeto comece a crescer demais e você acabe consumindo muitas horas de CICD, você tem a opção de contratar mais minutos ou então criar o seu próprio Runner. 

A figura acima é um exemplo dos Runners oferecidos pelo GitLab (Podem ser encontrados nas configurações do seu projeto,na camada escrita CI/CD e, posteriormente, em Runners).

Não importando onde você for rodar o Runner, nesse mesmo ambiente é necessário ter o docker instalado.

Instalação do Docker

No link abaixo é possível baixar e instalar o docker para o seu sistema operacional:

https://www.docker.com/products/docker-desktop

A instalação é bem simples e não vai ser abordada nesse post. Não é necessário criar nenhum docker manualmente, o próprio Runner se encarrega disso. 

Um tutorial completo de instalação pode ser encontrado no site oficial:

https://docs.docker.com/engine/install/

É importante notar que, enquanto é possível utilizar um docker Linux em cima de um sistema operacional Windows, algumas funcionalidades são limitadas, por exemplo, a virtualização que é necessária para rodar emuladores como o de Android dentro do docker. Caso o seu CI/CD precise de um emulador em algum momento, seu sistema operacional base deve ser Linux. 

Agora que você já possui o docker instalado, vamos criar e registrar o Runner.

Para configurarmos o Runner precisamos realizar dois passos, o primeiro é baixar o GitLab Runner e instalá-lo em seu computador e o segundo é pegar o token disponível no site para atrelar o Runner ao seu projeto.

Instalação do Runner

Podemos conferir o passo a passo para instalar o Runner de acordo com seu sistema operacional nesse link: https://docs.GitLab.com/Runner/install/. Para rodar o Runner, você deve ter o Git instalado e configurado no seu computador.

Após a instalação do Runner, é necessário que seja feito o registro do mesmo. Alguns dados sobre o seu projeto serão requisitados durante o processo. Eles podem ser encontrados na aba CICD do repositório, em “Set up a specific Runner manually”. 

Caso utilize um sistema operacional diferente do Ubuntu para o qual peguei de exemplo, siga o passo a passo da documentação do GitLab (https://docs.GitLab.com/Runner/register/) que não enfrentará problemas. O registro é extremamente semelhante em todos os sistemas operacionais.

Começaremos dando o comando para registrar o Runner: 

sudo GitLab-Runner register 

Logo após executarmos esse comando, é pedido uma URL do GitLab e entraremos com a padrão:

https://GitLab.com

Assim que colocarmos a URL ele irá pedir o token do seu projeto. É usando o token que ele vai vincular o Runner criado junto ao projeto de origem. O token pode ser encontrado nas configurações do seu projeto como indicado na imagem.

Após isso, é pedido uma descrição para o seu Runner, e fica de forma arbitrária o padrão de descrição a se seguir. É uma boa prática colocar uma descrição que detalhe um pouco sobre o funcionamento deste Runner. Logo depois uma Label é pedida. É importante colocar uma Label pois na hora de criar seu arquivo YML é utilizando as tags que você vai escolher em qual Runner o pipeline vai ser executado. 

Exemplos de labels: “Runner-produção”, ”Runner-homologação”, ”Runner-teste”.

Após a label, nos é perguntado o executor do Runner, e colocaremos “Docker”, seguindo o modelo abaixo:

Please enter the executor: ssh, docker+machine, docker-ssh+machine, kubernetes, docker, parallels, virtualbox, docker-ssh, shell:
docker

Como escolhemos o Docker como executor, seremos perguntados sobre uma imagem padrão para usarmos em nossos projetos. Esta parte varia muito de acordo com o seu projeto e das ferramentas dele, dependendo da linguagem, banco de dados e suas dependências. Como faremos para uma aplicação em Node.Js utilizaremos a imagem “node:latest”. Essa imagem garante que possamos utilizar no docker funções atribuídas a última versão do Node.Js. 

Acabando o registro, basta inicializar o Runner com o comando:

.\GitLab-Runner.exe start

Pronto , nosso Runner está atrelado a nosso projeto e vai aparecer como ativo na página de Runners.

Ficaremos com uma tela semelhante a imagem acima no site do GitLab (nesse caso existem dois Runners rodando). Não se preocupe se a “circunferência verde” demorar um pouco para aparecer, demora um certo tempo até o GitLab reconhecer que o Runner está online.

Agora que instalamos tudo que precisávamos para nosso CI/CD acontecer, vamos a parte mais importante que é adicionar uma configuração no nosso arquivo yml criado anteriormente.Pegando como exemplo um dos estágios que tínhamos criado, é simples especificar que você quer que esse estágio rode em cima de um Runner específico e não em um Runner aleatório.

deployProduction:
 only:
  - master
 stage: deployProduction

 # Repare que agora temos aqui as tags. Como um parâmetro de tags passamos “Runner-deploy-aws” que é o nome do Runner que criamos para fazer deploy no nosso ambiente de produção na aws. Agora que especificamos um Runner, esse estágio do pipeline vai sempre utilizar esse Runner específico para rodar.

 tags:
  - Runner-deploy-aws 
 script:
  - 'which ssh-agent || ( apt-get update -y && apt-get install openssh-client -y )'
  - chmod a+x disableHostKeyChecking.sh
  - bash ./deployProduction.sh


Usar um Runner particular pode ser vantajoso em várias situações. Temos a questão do tempo limite mensal provido pelo GitLab e temos também a capacidade de preparar o ambiente do Runner de antemão. Já deixar instalado os serviços utilizados, versões diferentes de bancos de dados para testes e diversas outras personalização características de cada projeto. 

YML COMPLETO

Abaixo segue o arquivo yml completo utilizado no CICD de um dos nossos backends:

image: node:latest
services:
 - mongo:latest
 
stages:
 - build
 - test
 - stagingDeploy
 - productionDeploy

cache:
 paths:
  - node_modules/

.only-default: &only-default
 only:
  - master
  - develop
  - staging
  - merge_requests

build:
 <<: *only-default
 stage: build
tags:
  - Runner-deploy-aws 
 script:
  - npm install

test:
 only:
  - merge_requests
 stage: test
tags:
  - Runner-deploy-aws 
 artifacts:
  paths:
   - bin/
 script:
  - npm install
  - npm test

deployStaging:
 only:
  - staging
 stage: deployStaging
tags:
  - Runner-deploy-aws 
 script:
  - 'which ssh-agent || ( apt-get update -y && apt-get install openssh-client -y )'
  - chmod a+x disableHostKeyChecking.sh
  - bash ./deployStaging.sh
 
deployProduction:
 only:
  - master
 stage: deployProduction
tags:
  - Runner-deploy-aws 
 script:
  - 'which ssh-agent || ( apt-get update -y && apt-get install openssh-client -y )'
  - chmod a+x disableHostKeyChecking.sh
  - bash ./deployProduction.sh

CONSIDERAÇÕES

A grande questão do CICD é entender quando ele vai ser executado. Como vimos, entender o Git flow ajuda bastante nisso e esperamos que, com a explicação acima, isso tenha ficado mais claro para os leitores.

Não entramos muito no código específico de cada estágio porque isso é extremamente particular de cada projeto. Você usa Angular? React? Faz deploy no Heroku ou na AWS? Isso pode mudar totalmente a parte script de cada estágio. Porém, no final das contas o código contido ali é, na grande maioria das vezes, exatamente o código que você utiliza para poder fazer as coisas manualmente no seu terminal. Tudo que o CICD faz é executar esses códigos em sequências dividindo os estágios pelo Git flow.

Tem dúvida em algo? Quer conversar sobre o artigo ou trocar uma ideia sobre tecnologia? Deixa uma mensagem ai que logo a gente responde!

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software.

Escrito por Arilton Junior. Revisado por Ramon Barros, Koda Gabriel e Prof. Tiago Carneiro."
Como fazer uso do GitLab segundo a cultura organizacional do TerraLAB,http://www2.decom.ufop.br/terralab/como-fazer-uso-do-gitlab-segundo-a-cultura-organizacional-do-terralab/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/10/Métricas-de-Desenvolvimento-2-730x350.png,"Avaliar o progresso de um trabalho é vital no gerenciamento de qualquer negócio, seja ele a prestação de algum serviço, desenvolvimento de um projeto ou produto. Fazer essa estimativa, entretanto, está longe de ser algo trivial, dependendo da atividade realizada. Este é o caso, por exemplo, quando se trabalha com o desenvolvimento de softwares. Para realizar esse monitoramento, é comum o uso de métricas. São elas que fornecem, como o próprio nome sugere, as medidas de progresso de cada equipe, bem como a contribuição de cada colaborador nesse avanço. A seleção das métricas e a forma como elas são adquiridas são um grande desafio para os gerentes de projeto. Elas devem ser facilmente coletadas e representativas, para que o seu uso seja justificável. Contudo, isso pode variar de empresa para empresa. Por mais que existam metodologias de trabalho padronizadas, cada organização possui suas particularidades no que diz respeito à forma de trabalho. Com isso, uma métrica que é interessante para uma empresa x, que trabalha segundo uma cultura organizacional específica, pode não ser tão interessante para uma empresa y, que possui uma forma de trabalho distinta, mesmo que elas atuem no mesmo segmento de mercado.

No TerraLAB, adotamos as metodologias ágeis como principal influência, a partir do qual desenvolvemos nosso próprio processo de desenvolvimento de software (BOPE). Para tanto, desenvolvemos um backlog, de acordo com as necessidades do cliente, que é cumprido, gradativamente, através de ciclos de desenvolvimento (Sprints). Todos os nossos projetos são alocados em repositórios no Gitlab, conforme já mencionado em textos anteriores aqui do blog. Além de um interessante gerenciador de repositórios, o Gitlab é capaz de fornecer métricas muito úteis para a gestão de projetos de software, de maneira fácil e rápida. Todavia, para que a obtenção dessas métricas seja possível, o uso do Gitlab deve ser feito segundo um padrão específico.

Diante do que foi dito acima, esse texto apresentará as principais métricas obtidas pelo Gitlab, como devem ser organizadas as branchs principais e como realizar os commits e merge requests, de acordo com o que é sugerido pelo GitLab flow e a cultura organizacional do TerraLAB.

Métricas fornecidas pelo Gitlab

As métricas descritas nesse texto serão aquelas adquiridas em licenças gratuitas de repositórios privados e que sejam interessantes para nosso contexto organizacional. Dentro desse contexto, vamos falar a respeito das métricas contidas no campo Value Stream Analytics, que focam em medidas da “velocidade” de desenvolvimento de projetos. Essas métricas são muito úteis para identificar gargalos no processo de desenvolvimento, possibilitando à gerência revelar e triar causas raízes de lentidão no ciclo de desenvolvimento de softwares. As métricas obtidas com essa ferramenta são:

Figura 1 – Dashboard métricas Value Stream Analytics Gitlab

Issue: essa métrica reporta o tempo gasto para criar determinada issue e tomar a decisão de “resolvê-la”, seja atribuindo uma label a ela ou adicionando-a a uma milestone (o que vier antes).  A label será monitorada apenas se já houver uma issue board list criada para isso.

Uma milestone seria um marco no processo de desenvolvimento da aplicação, onde um determinado volume de trabalho deve estar pronto. Em outras palavras, se estabelece um período de tempo onde determinada parcela do backlog deve ser entregue. Em alguns casos, pode-se estabelecer uma milestone para cada sprint ou para cada release a ser apresentada ao cliente. No GitLab, através do uso de milestones, tem-se a possibilidade de obter-se uma métrica interessante que seria o burndown chart. Trata-se de um gráfico que reporta o quanto a equipe caminhou no desenvolvimento das atividades estipuladas, para a milestone, ao longo do tempo. Contudo, essa métrica não está disponível para a versão atual que usamos do GitLab (versão grátis). Em virtude disso, nós, aqui do TerraLAB, não fazemos uso, ainda, das milestones. Faz parte dos nossos planos, entretanto, usá-las para definirmos releases a serem apresentadas aos clientes em futuros projetos. E a forma como elas serão feitas pode ser assunto para um novo texto aqui no blog.

Plan: mede o tempo entre a ação tomada no estágio anterior e enviar o primeiro commit para a branch. O primeiro commit para a branch é o gatilho que diferencia a métrica Plan da Code, e ao menos um dos commits na branch precisa conter o número da issue relacionada. O GitLab rastreia isso usando o issue closing pattern, como closes #xxx, onde xxx é o número da issue relacionada ao commit (Figura 2). Se nenhum dos commits na branch mencionar o número da issue a qual ela se refere, ela não é considerada na medida do tempo dessa etapa.

Figura 2 – Relacionando um commit a uma issue.


Code: mede o tempo entre enviar o primeiro commit (estágio anterior) e criar uma merge request (MR) relacionada a esse commit. A chave para manter o processo rastreado é incluir o issue closing pattern na descrição da MR (Figura 3). Se o issue closing pattern não estiver presente na descrição da MR, o GitLab não retornará o valor da métrica.

Figura 3 – Issue closing pattern em uma descrição de MR.


Test: registra o tempo para executar todo o pipeline desse projeto. Está relacionado ao tempo que o CI/CD leve para rodar todos os commits enviados naquela MR definida no estágio anterior.

Review: retorna o tempo gasto para revisar a MR, desde o momento que ela é criada até o momento onde é concluída.

Staging: registra o tempo entre realizar o merging até o deploy para produção de uma MR. Para que isso ocorra, é necessário que exista um ambiente de produção (trunk de produção – Figura 4).

Figura 4 – MR deployed no ambiente de produção.


Total: essa métrica registra o tempo total do processo. Ou seja, desde a criação de uma issue até o deploy do código no ambiente de produção.

No dashboard apresentado na figura 1, além de reportar as métricas aqui mencionadas para cada issue contida no projeto, à direita, no lado esquerdo teremos a mediana de cada uma dessas métricas. Somado a isso, na parte superior do dashboard teremos informações referentes a:

novas issues criadas;
commits  realizados;
deploys realizados e;
frequência de realização de deploys. 

Esses valores podem ser coletados, paro o caso de usuário com versão grátis do GitLab, dentro de intervalos padronizados de tempo (7, 30 e 90 dias).

Issue closing pattern.

Para a obtenção das métricas citadas acima junto ao GitLab, é importante falarmos do chamado issue closing pattern. Quando um commit ou MR resolve uma ou mais issues, é possível ter essas issues fechadas automaticamente quando o commit ou MR alcança a branch padrão do projeto.

Se uma mensagem de commit ou descrição de MR contiver texto correspondendo a um padrão definido, todas as issues referenciadas no texto correspondido serão fechadas. Isso acontece quando o commit é enviada para a branch padrão de um projeto ou quando um commit ou MR é aprovada nele.

Por exemplo, se Closes # 4, # 6, Related to # 5 for incluído em uma descrição de MR, as issues # 4 e # 6 serão fechados automaticamente quando o MR for aprovada, mas não # 5. Usar Related to indica a issue  # 5 como uma issue relacionada, mas não a fechará automaticamente.

Como isso funciona, “nos bastidores”, a coleta dessas métricas:

As issues e as merge requests (MRs) são agrupados em pares, de forma que para cada par de <issue, merge request>, a MR tem uma issue closing pattern para a issue correspondente. Todas as outras issues e MRs não são consideradas.

Em seguida, os pares <issue, merge request> são filtrados pelos últimos XX dias (especificado pela UI – para o caso de usuários com licenças gratuitas, 7, 30 ou 90 dias ). Portanto, proíbe que esses pares sejam considerados.

Para os pares <issue, merge request> restantes, verificamos as informações de que precisamos para os estágios, como data de criação da issue, tempo para uma MR ocorrer, etc.

Resumindo, qualquer coisa que não siga o fluxo do GitLab flow não será rastreado e o dashboard do Value Stream Analytics não apresentará nenhum dado para:

MRs que não fecham uma issue;
Issues não rotuladas (sem labels atribuídas a ela) com uma label presente no Issue Board ou para issues as quais não foram atribuídas uma milestone;
Staging, se o projeto não tiver ambiente de produção.
Exemplo de fluxo de trabalho e das métricas coletadas

Nessa seção, vamos descrever um simples fluxo de desenvolvimento dado como exemplo no GitLab Docs. Nele fica mais claro o que cada métrica representa dentro de fluxo de desenvolvimento. Perceba que se um estágio (métrica) não tiver um registro de início e de parada, ele não é medido e é desconsiderado no cálculo da mediana. Considerando que a issue board list foi criada, que o CI para teste e um ambiente de produção foram devidamente configurados:

Uma issue é criada às 9:00 (início do registro da métrica Issue);
A issue em questão é adicionada à sprint backlog (issue board list) às 11:00 (fim do registro da métrica Issue/ início do registro da métrica Plan);
Começa-se a trabalhar na issue criando uma branch localmente e realizando um commit às 12:00;
Um segundo commit para a branch é realizado mencionando o número da issue às 12:30 (fim do registro da métrica Plan/ início do registro da métrica Code);
Sobe-se a branch e cria-se uma MR que contém o issue closing pattern, referente a essa issue, na sua descrição às 14:00 (fim do registro da métrica Code/ início do registro das métricas Test e Review);
O CI começa a rodar o seus scripts definidos em .gitlab-ci.yml e leva 5 minutos (fim do registro da métrica Test);
Revisa a MR, garantindo que tudo está ok e finaliza a merge dessa MR às 19:00 (fim do registro da métrica Review/ início do registro da métrica Staging);
Agora que a MR foi realizada, um deploy para o ambiente de produção começa e termina às 19:30 (fim do registro da métrica Staging).

Do exemplo acima, os tempos correspondentes a cada métrica seriam:

Issue: 2 horas (9:00 – 11:00);
Plan: 1 hora (11:00 – 12:00);
Code: 2 horas (12:00 – 14:00);
Test: 5 minutos;
Review: 5 horas (14:00 – 19:00);
Staging: 30 minutos (19:00 – 19:30);
Total: 10:30 horas (9:00 – 19:30).

Algumas considerações adicionais podem ser acrescido para um melhor entendimento do assunto. Como pode ser notado no exemplo acima, não importa se o primeiro commit não menciona o número da issue, você pode fazer isso depois em qualquer outro commit da branch na qual você está trabalhando. Vocês devem ter percebido também que a métrica Test não é calculada para o tempo total do ciclo, uma vez que ela está incluída no processo de revisão (Métrica Review).

E você, caro leitor! Gostou das informações compartilhadas no texto de hoje do blog do TerraLAB? Você também faz uso do GitLab para versionamento e gestão da produtividade de sua equipe? Deixe nos comentários suas experiências, dicas e sugestões para obtenção de métricas junto ao GitLab!

Escrito por Ramon Barros. Revisado por Arilton Junior e Prof. Tiago Carneiro."
Documentando sua API Rest com Swagger,http://www2.decom.ufop.br/terralab/documentando-sua-api-rest-com-swagger/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/10/swagger-730x350.png,"As APIs são responsáveis por desempenhar papel fundamental na comunicação entre as mais diferentes aplicações e, apesar de estarem tão presentes nos softwares, elas são invisíveis aos usuários. A sigla API vem da abreviação do termo “Application Programming Interface”, ou, em portugues interface de programação de aplicações. 

Em uma definição formal, o conceito de API está relacionado a um conjunto de rotinas e padrões estabelecidos por um software para a utilização das suas funcionalidades oferecidas por outros aplicativos. 

Dentro do universo das APIs existe um padrão conhecido como REST (Representational State Transfer). O conceito de REST se aplica à construção de aplicações que se comuniquem com um serviço ou servidor por meio da WEB, uma API REST utiliza o protocolo HTTP (Hypertext Transfer Protocol) para extrair, inserir, alterar e deletar dados nestes serviços/servidores.  

Uma boa documentação de uma API REST é crucial para desenvolvedores, testadores, mantenedores entenderem claramente o comportamento fornecido por ela. Então, uma questão relevante é: Como fazer essa documentação da melhor forma possível? 

Existem várias ferramentas que ajudam a criar uma boa documentação, as principais são API Blueprint, RAML e Swagger. Este artigo apresentará o Swagger, por ter a maior e mais ativa comunidade de desenvolvedores, tornando mais fácil achar material para aprender a usar ou tirar eventuais dúvidas.

O que é o Swagger?

O Swagger é um framework composto por diversas ferramentas que, independente da linguagem, auxilia a descrição, consumo e visualização de serviços de uma API REST. 

No framework Swagger, existem ferramentas para os seguintes tipos de tarefas a serem realizadas para o completo desenvolvimento da API de um serviço WEB: 

1) A especificação da API consiste em determinar os modelos de dados que serão entendidos pela API e as  funcionalidades presentes na mesma. Para cada funcionalidade, é preciso especificar o seu nome, os parâmetros que devem ser passados no momento de sua invocação e os valores que irão ser retornados aos usuários da API. Entre esta ferramentas, podemos citar o OpenAPI Specification.

2) Após especificar a API, o framework facilita sua implementação, com a ferramenta Swagger Codegen é possível montar o código inicial automaticamente nas principais linguagem de programação.

3) Os testes de API são extremamente importantes, pois ajudam a garantir o funcionamento, o desempenho e a confiabilidade da sua aplicação. O Swagger oferece ferramentas para teste manuais, automatizados e de desempenho 

4) Para auxiliar na utilização da API, o Swagger dispõe de ferramenta para deixar a visualização mais intuitiva, permitindo também que interajam com a API.

O Swagger permite criar a documentação da API de 3 formas:

1- Automaticamente: Simultaneamente ao desenvolvimento da API é gerada a documentação.

2- Manualmente: Permite ao desenvolvedor escrever livremente as especificações da API e as publicar posteriormente em seu próprio servidor.

3- Codegen: Converte todas as anotações contidas no código fonte das APIs REST em documentação.

Conheça o OpenAPI Specification

Apesar de ser open source desde sua criação, o Swagger Specification sempre teve uma empresa vinculada ao seu desenvolvimento. Entretanto em 2016, a SmartBear doou o projeto para a OpenAPI Initiative, uma organização criada pela mesma e que tem como integrantes alguns gigantes da indústria como Google, IBM, Microsoft. O projeto passou a ser chamado de OpenAPI Specification.

O OpenAPI Specification (OAS) é uma especificação para descrever, produzir, consumir e visualizar serviços de uma API REST. Com o OAS você poderá descrever recursos, URIs, modelo de dados, métodos HTTP aceitos e códigos de resposta. Infelizmente, nem toda ferramenta suporta a versão mais recente da OAS, logo, você terá que usar versões mais antigas caso queira utilizar as demais ferramentas que o Swagger disponibiliza.

Figura 1- Exemplo de especificação.

A Figura 1 é um exemplo de especificação da API do nosso aplicativo da Orquestra de Ouro Preto. O arquivo está em YAML(YAML Ain’t Markup Language) uma linguagem em formato de serialização de dados legível por humanos, que é bastante utilizada em arquivos de configuração.

Outras ferramentas.

Como dito acima, o Swagger possui diversas ferramentas e provê todo um ecossistema para você trabalhar tanto com o  OAS. Veremos agora um pouco sobre as principais delas.

Swagger Editor

O Swagger Editor é uma ferramenta online que permite criar manualmente a documentação da API. Ao utilizar YAML, faz com que o desenvolvedor não tenha dificuldades em descrever os seus serviços. Outro benefício do Editor é possuir um conjunto de templates de documentos que servem como base para quem não deseja iniciar a documentação do “zero”.

Figura 2 – Template da API da Orquestra de Ouro Preto 

Ao lado esquerdo da Figura 2, podemos ver o “código” da documentação. No lado direito, é possível ver o resultado gerado a partir das informações contidas neste código. Ao editar o lado esquerdo  podemos conferir de forma simultânea o resultado no lado direito.

Swagger UI

Com o Swagger UI, a partir da especificação da API, podemos criar documentações elegantes e acessíveis ao usuário, permitindo assim uma compreensão maior da API, pois além de poder ver os endpoints e modelos das entidades com seus atributos e respectivos tipos, o módulo de UI possibilita  que os usuários da API  interajam intuitivamente com a API usando uma sandbox. A sandbox é uma plataforma de testes onde as aplicações podem ser alteradas sem interferir no meio de produção. Nela, os desenvolvedores podem executar todas as operações de mudanças experimentais que vão garantir o bom funcionamento da solução, evitando danos que possam prejudicar o sistema.

Figura 3 – Swagger UI do projeto Orquestra de Ouro Preto

Cada barra colorida na Figura 3 é uma rota da API da Orquestra de Ouro Preto. Nas barras podemos ver o caminho da rota, o método utilizado e alguns dos parâmetros necessários.  Ao clicar em uma das barras ela se expande mostrando um exemplo de como cada parâmetro deve ser preenchido e a resposta esperada, como podemos ver nas figuras abaixo, 4 e 5.

Figura 4 – Exemplo de parâmetro para a rota “news’’.
Figura 5 – Exemplo da respostas para a rota “news’’.

 É possível testar a requisição, para isso basta apenas clicar no botão “Try it out”, que está no canto superior na Figura 4, preenchendo os parâmetros e clicar no botão execute. Dessa forma,  um desenvolvedor que utiliza esta documentação da API sabe exatamente o que esperar nas respostas e o que necessário enviar nas requisições. Há duas maneiras simples de tornar o módulo UI da API público. A primeira é utilizar a plataforma SwaggerHub, a plataforma online do Swagger, mas a plataforma é paga e o plano free permite publicar apenas 3 projetos. A outra maneira é, se a API for em NodeJS, você pode colocá-la no seu servidor e criar um endpoint na API dando acesso ao módulo.

Swagger Codegen

O Swagger Codegen é um projeto muito interessante, a partir  da especificação  em YAML gera automaticamente o “esqueleto” da API em diferentes linguagens, como Java, Python,Kotlin, Lua, Haskell, C++, entre outras . Isso mesmo, com algumas linhas de comando você cria todo o código inicial da sua API na linguagem que desejar. Se você deseja utilizar o Codegen é recomendável que primeiro verifique quais versões do OpenAPI Specification ele suporta. Você pode conferir no link: https://github.com/swagger-api/swagger-codegen#compatibility.

Swagger Inspector e ReadyAPI

Assim como o Postman e o Insomnia, o Swagger Inspector é uma GUI que auxilia nos testes  manuais da sua API, não só Rest como SOAP(Simple Object Access Protocol ) ou GraphQL. Diferente dos outros dois citados, o Swagger Inspector não precisa ser instalado, todos seus testes são feitos na nuvem, basta criar uma conta no SwaggerHub, os teste são salvos e você pode acessar a qualquer momento.  Se você utiliza o Chrome ou Firefox, você será solicitado a baixar a extensão, ela permite o compartilhamento de recursos de origem cruzada (CORS), que normalmente não é permitido em navegadores. Isso permite que seu navegador acesse informações de servidores fora do site em que você está e faça uma chamada à API.

ReadyAPI é a nova ferramenta paga que juntou LoadUI Pro e SoapUI Pro, com a ReadyAPI você pode fazer testes automatizados(SoapUI Pro), de segurança e carga (LoadUI Pro) em um só ambiente.

Considerações finais

Este artigo introduziu brevemente algumas das ferramentas mais comumente utilizadas junto com o projeto Swagger. Contudo, a comunidade do Swagger é grande e com isso acabam surgindo diversas ferramentas ao longo do tempo. Você pode conferir muitas destas ferramentas em Tools and Integrations. O ambiente em torno do Swagger é rico, facilitando todo o processo de documentação e teste. Vale a pena considerar escrever suas próximas especificações de APIs com esse padrão. Em um próximo post sobre Swagger, vamos fazer uma abordagem mais prática, mostrando como utilizar algumas dessas ferramentas.

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software.

Se você gostou do Post, deixe um comentário e não esqueça de compartilhar em suas redes sociais para que outros possam ter acesso a essas informações! Seu feedback é muito importante para a qualidade das postagens. Sinta-se livre para comentar, discutir, perguntar sobre os conceitos do Swagger com a nossa comunidade. E não deixe de ficar de olho no restante do nosso conteúdo!

Escrito por Wesley Dias e revisado por Prof. Tiago Carneiro"
Uma Breve Introdução à Arquitetura Limpa com Node.js,http://www2.decom.ufop.br/terralab/uma-breve-introducao-a-arquitetura-limpa-com-node-js/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/09/WhatsApp-Image-2020-09-30-at-17.28.10-2-626x350.jpeg,"“Se você acha que uma boa arquitetura é cara, tente usar uma arquitetura ruim.”
     – Brian Foote e Joseph Yoder

Suponha que você acabe de ser contratado por uma empresa para trabalhar em um componente de um grande sistema. Você abre o repositório onde o código está hospedado e se assusta com o que vê: arquivos com milhares de linhas de código, dependências por todos os lados, falta de testes, falta de deploy automático e diversos outros problemas. Cada desenvolvedor passa meses para apenas entender a aplicação (provavelmente não existe documentação) e, quando começa a desenvolver, sabe que cada nova mudança pode encadear diversos problemas e falhas na aplicação. Talvez tenha vivenciado algo parecido e, se vivenciou, certamente não tem boas lembranças do ocorrido.

Todos esses problemas citados acima são características de um software sem arquitetura bem definida ou, em alguns casos, até sem arquitetura nenhuma (link). Arquitetura de software é uma área de extrema importância na engenharia de software e não deve ser subestimada em nenhum projeto da área. Nenhum sistema consegue sobreviver ao tempo caso não existam decisões arquiteturais bem articuladas pela equipe de desenvolvimento.


Nesse artigo não vamos entrar em muitos detalhes sobre o conceito de arquitetura de software, uma vez que é muito amplo e difícil de conceituar, mas deixarei aqui a definição simplista de Ralph Johnson, um dos membros do GOF (link), que diz: “Arquitetura de software é sobre as coisas importantes… Seja lá o que for.” Por outro lado, podemos entender Arquitetura de Software como a maneira na qual os componentes de um sistema se estruturam e interligam por relações de dependência, por meio das quais solicitam funcionalidades a outros componentes para que possam executar sua própria missão.

Arquitetura Limpa

Arquitetura Limpa, do termo inglês Clean Architecture,  é um padrão arquitetural usado na engenharia de software. Esse padrão foi conceituado por Robert Martin, mais conhecido como Uncle Bob, um engenheiro de software famoso e um dos autores do Manifesto Ágil (link), além de diversos livros na área. O conceito foi apresentado no artigo em seu blog em 2012 (link) e foi baseado em uma arquitetura já conhecida, a Arquitetura Hexagonal (link), conceituada em 2005. Mais tarde, em 2017, ele aprofundou-se no tema em seu livro: Arquitetura Limpa: O guia do artesão para estrutura e design de software (link).

A ideia dessa arquitetura é criar componentes fracamente acoplados que possam ser conectados através de portas e adaptadores, o que permite o isolamento das regras de negócio da aplicação (ex: entidades, casos de uso) de preocupações externas (ex: frameworks, banco de dados). A lógica de negócios não deve depender de GraphQL ou REST por exemplo, ou se os dados estão sendo obtidos por um banco de dados ou um arquivo CSV simples. Essa separação tem, dentre outros benefícios, auxiliar na mudança de componentes e facilitar testes automatizados.

Na imagem acima, que é a principal representação dessa arquitetura, pode-se notar uma separação por camadas em círculos, com algumas setas apontando do círculo mais externo ao mais interno. Essas setas demonstram o fluxo de dependências, em que os círculos externos podem depender dos círculos internos sem que o contrário se aplique. Ou seja, mudanças na entidade por exemplo podem ter efeito em todas as outras camadas da aplicação, mas mudanças nos drivers e frameworks são isoladas do resto da aplicação, já que não devem existir dependências entre elas.

A Arquitetura Limpa casa muito bem com o DDD (Domain-Driven Design) (link). Irei utilizar a notação usada pelo DDD para definir as camadas da aplicação, que são: Domínio, Aplicação, Interfaces e Infraestrutura. Cada uma dessas camadas são definidas da seguinte maneira:

Domínio: A camada de domínio é o coração do software, reunindo as regras cruciais de negócios da empresa inteira. É o local onde as entidades e objetos de valor viverão. De acordo com Uncle Bob: “Uma entidade pode ser um objeto com métodos ou um conjunto de estruturas de dados e funções. Isso não importa, contanto que as entidades possam ser usadas por muitas aplicações diferentes na empresa.”.
Aplicação: Essa camada contém as regras de negócio específicas da aplicação. Na Arquitetura Limpa, Uncle Bob descreve os casos de uso como sendo as principais funcionalidades da aplicação. Esses casos de uso utilizam as entidades a fim de atingir seus objetivos, criando assim uma dependência nelas.
Interfaces: Aqui estão os adaptadores. Adaptadores são criados para isolar as entidades e casos de uso das ferramentas utilizadas na aplicação (frameworks e drivers), convertendo os dados para o formato que mais convém às camadas mais externas. Esses adaptadores, de acordo com Uncle Bob, devem funcionar pelo conceito de Interfaces de programação orientada a objetos (link) nos casos de uso, como apresentado no diagrama do canto inferior da imagem acima. Como em Javascript não existem interfaces, isso pode ser feito através de injeção de dependência (link), que será explicado melhor nas seções seguintes.
Infraestrutura: A infraestrutura consiste em tudo o que existe independentemente da aplicação: bibliotecas externas, mecanismo de banco de dados, servidor de aplicação, filas de mensagens e assim por diante.

Nas seções a seguir, mostraremos como uma aplicação pode ser arquitetada usando esses conceitos na prática, com códigos escritos em Node.js. Para isso irei usar de exemplo uma aplicação simples de um blog de notícias. Na prática, é difícil seguir 100% o que Uncle Bob teorizou, porém tentaremos ser o mais fiel possível, baseado nos fundamentos apresentados em seu  livro.

Domínio

No nosso exemplo, uma entidade para posts no blog seria modelada da seguinte forma:

/* domain/post/post.js */
const buildMakePost = ({ Id, validator }) => ({
 id = Id.makeId(),
 title,
 description,
 image,
 tags,
} = {}) => {
 const { error } = validator({
   id, title, description, image, tags,
 });
 if (error) throw new Error(error);
 
 return Object.freeze({
   getId: () => id,
   getTitle: () => title,
   getDescription: () => description,
   getImage: () => image,
   getTags: () => tags,
 });
};
 
module.exports = buildMakePost;


A primeira coisa que dá para perceber é que não existe nenhuma declaração “require” ou “import” no começo do código. Isso é importante já que as entidades fazem parte da camada de domínio, onde não deve existir dependências sobre outras camadas. 

Cada post possui um ID único, que é gerado através de uma função “makeID”. Cada post possui também uma função “validator”, que valida a corretude dos campos da entidade. Essas funcionalidades podem fazer parte de qualquer biblioteca, ou podem até ser implementadas pelo próprio desenvolvedor do código; isso não importa para a entidade. A validação será realizada pela função (ou a classe/objeto que possui a função) passada por parâmetro no momento da construção do post. Isso faz com que seja muito fácil alterar as funções de validação e que cria IDs nesse caso, criando um fraco acoplamento da entidade sobre esses componentes. Outro benefício é uma melhora na testabilidade dessas entidades, já que fica fácil criar stubs (link) para cada uma das funções, sendo assim possível testar esse código independentemente de qualquer implementação, possibilitando então uma análise comparativa entre diversas implementações dessas funções. Esse padrão é basicamente a injeção de dependência (link), muito utilizado nesse tipo de arquitetura. Ao final do código acima, é retornado um objeto “congelado”, com diversos getters dos atributos da entidade. Esse Object.freeze() faz com que todos objetos dentro dele tornam-se imutáveis, impedindo que novas propriedades sejam adicionadas a eles. Isso permite que, caso o código falhe, ele irá falhar na criação da entidade e não na chamada dessas funções.

A entidade posts é exposta ao resto da aplicação através de um arquivo index.js, apresentado a seguir:

/* domain/post/index.js */
const buildMakePost = require('./post');
const Id = require('../../infrastructure/post/Id');
const validator = require('../../infrastructure/post/validator');
 
const makePost = buildMakePost({ Id, validator });
 
module.exports = makePost;


Esse é o lugar onde todas as dependências internas da entidade são chamadas e onde a entidade é construída e exportada para o restante da aplicação, com todas as dependências embutidas. Caso o código passe a utilizar outra classe para gerar os IDs dos posts por exemplo, a única alteração que deve ser feita nessa parte do código é na importação da classe (ou nem isso caso ela mantenha o mesmo caminho da implementação anterior), sem qualquer interferência na entidade.


Aplicação

Na camada de aplicação estão os casos de uso do sistema. A funcionalidade de criação de um post ficaria da seguinte forma: 

/* application/post/add-post.js */
const makePost = require('../../domain/post');
 
const makeAddPost = ({ postsDb }) => (postInfo) => {
 const post = makePost(postInfo);
 
 return postsDb.insert({
   id: post.getId(),
   title: post.getTitle(),
   description: post.getDescription(),
   image: post.getImage(),
   tags: post.getTags(),
 });
};
 
module.exports = makeAddPost;

Nesse arquivo, a primeira coisa que dá para perceber é o “require” no começo do código. Esse “require” serve para importar a nossa entidade “post”, a única dependência direta desse caso de uso, que deve fazer parte da camada de domínio, claro. Depois temos nossa fábrica (link) de posts “makeAddPost”, que recebe as dependências externas por injeção de dependência (que nesse caso é apenas o banco de dados da aplicação). Como será visto mais a frente, esse banco de dados é um adaptador, que vem da camada de interfaces da aplicação. A fábrica retorna uma outra função (obs: estamos usando “arrow functions” (link) nesses exemplos, para deixar a sintaxe do código mais curta), que recebe os dados do post, que estão embutidos na variável postInfo. O post então é criado com esses dados pela função “makePost”. Ao final de tudo o post é inserido no banco de dados pela função “insert”, que recebe um objeto com as propriedades do post que acabou de ser construído. Essa estrutura faz com que o postsDb (que é um adaptador para o banco de dados) não precise saber nada sobre a entidade de posts; isso é importante já que na Arquitetura Limpa, a aplicação deve passar dados entre camadas no formato ou estrutura que é mais conveniente a camada que está recebendo os dados.

Como na camada de domínio, nossos casos de uso são expostos ao resto da aplicação pelo index.js.

/* application/post/index.js */
const makeAddPost = require('./add-post');
const postsDb = require('../../interfaces/post/data-access');
 
const addPost = makeAddPost({ postsDb });
 
const postService = Object.freeze({
 addPost,
 // editPost,
 // listPosts,
 // etc...
});
 
module.exports = postService;

Esse arquivo importa todas as fábricas dos casos de uso desse serviço (que nesse caso é apenas do caso de uso de criação de posts, mas poderiam ter vários outros ali), e cria os casos de uso, passando as dependências externas por injeção de dependência. No final será retornado um objeto congelado, que é o serviço em si, com todos os casos de uso implementados.

Interfaces

Nessa camada estão os adaptadores para os frameworks e drivers da aplicação. Um adaptador para o banco de dados dos posts da aplicação ficaria da seguinte forma:

/* interfaces/post/data-access/posts-db.js */
const Id = require('../../../infrastructure/post/Id');
 
const makePostsDb = ({ makeDb }) => {
 async function insert({ id: _id = Id.makeId(), ...commentInfo }) {
   const db = await makeDb();
 
   const result = await db
     .collection('posts')
     .insertOne({ _id, ...commentInfo });
   const { _id: id, ...insertedInfo } = result.ops[0];
   return { id, ...insertedInfo };
 }
 
 return Object.freeze({
   insert,
   // remove,
   // update,
   // etc...
 });
};
 
module.exports = makePostsDb;

Esse adaptador protege as camadas interiores (descritas anteriormente) de uma dependência direta no banco de dados, já que essas camadas não necessitam saber como fazer as operações no banco de dados (como um “insert” por exemplo), todas essas operações são manipuladas aqui. Como há injeção na dependência do banco de dados, fica fácil trocar o banco de dados caso a aplicação passe a utilizar outro (nesse caso estamos usando o MongoDB). Caso você altere o banco de dados da aplicação para um que tenha uma API diferente da que o MongoDB oferece, essa parte do código também seria afetada para adaptar-se a nova API; porém isso não é um problema já que esse código é completamente isolado das regras de negócio (camadas de domínio e de aplicação). Esse adaptador retorna um objeto congelado que expõe todas as interações da aplicação com o banco de dados.

Como já é de praxe, a camada é exposta em um arquivo index.js.

/* interfaces/post/data-access/index.js */
const makePostsDb = require('./posts-db');
const makeDb = require('../../../infrastructure/post/db');
 
const postsDb = makePostsDb({ makeDb });
 
module.exports = postsDb;

Aqui a conexão do banco de dados é importada da camada de infraestrutura e injetada no adaptador “makePostsDb”, que é exportado logo a seguir.

Como nossa aplicação irá funcionar na Web, precisamos também de um adaptador para lidar com as conexões HTTP, visto que existem vários frameworks diferentes que lidam com isso e nossa aplicação não pode ficar dependente de apenas um deles. Esses adaptadores, para quem conhece o padrão MVC (link), são basicamente controllers. No nosso exemplo, um adaptador para uma requisição POST na API ficaria da seguinte forma (obs: o nome do arquivo é basicamente uma convenção que estamos usando nesse artigo, a primeira parte é o método HTTP e a segunda o serviço. Apesar de ter ficado estranho aqui, acreditamos que é melhor seguir a convenção em todos os casos do que alterar apenas para esse caso específico. No caso de uma requisição GET ficaria “get-post.js”):

/* interfaces/post/controllers/post-post.js */
const makePostPost = ({ addPost }) => async (httpRequest) => {
 try {
   const { source = {}, ...commentInfo } = httpRequest.body;
   source.ip = httpRequest.ip;
   source.browser = httpRequest.headers['User-Agent'];
   if (httpRequest.headers.Referer) {
     source.referrer = httpRequest.headers.Referer;
   }
   const posted = await addPost({
     ...commentInfo,
   });
   return {
     headers: {
       'Content-Type': 'application/json',
       'Last-Modified': new Date(posted.modifiedOn).toUTCString(),
     },
     statusCode: 201,
     body: { posted },
   };
 } catch (e) {
   return {
     headers: {
       'Content-Type': 'application/json',
     },
     statusCode: 400,
     body: {
       error: e.message,
     },
   };
 }
};
 
module.exports = makePostPost;


Aqui o caso de uso “addPost” é injetado no adaptador que irá retornar uma função que recebe uma requisição HTTP como parâmetro. Essa função extrai os dados da requisição HTTP e os passa para o caso de uso “addPost”, e retorna uma resposta HTTP de sucesso caso tudo ocorra corretamente ou uma resposta de falha caso algum erro ocorrer no processo. Como no adaptador do banco de dados, aqui fica fácil alterar o tipo de comunicação que a aplicação está usando; poderia-se usar GraphQL, gRPC ou até alterar toda a aplicação para rodar em desktop ao invés de rodar na Web.

O index.js fica assim:

/* interfaces/post/controllers/index.js */
const { addPost } = require('../../../application/post');
const makePostPost = require('./post-post');
 
const postPost = makePostPost({ addPost });
 
const postController = Object.freeze({
 postPost,
 // getPost,
 // putPost,
 // etc...
});
 
module.exports = postController;

Os casos de uso são importados e o controller é retornado como um objeto congelado com as requisições HTTP.

Infraestrutura

A última camada da arquitetura é a camada de infraestrutura e é aqui que todas as bibliotecas, frameworks e drivers estão. Aqui iremos mostrar como ficaria a declaração da API e a configuração das rotas, mas no nosso exemplo teríamos nessa camada também a inicialização do banco de dados, o framework para validação dos dados do post e o framework para a geração do ID do post (e possivelmente muitos outras coisas também).

/* infrastructure/post/webserver/routes.js */
const express = require('express');
 
const { postPost } = require('../../../interfaces/post/controllers');
const makeCallback = require('../../../interfaces/post/express-callback');
 
const router = express.Router();
 
router
 .post('/posts', makeCallback(postPost));
// .get('/posts/:id', makeCallback(getPost))
// .delete('/posts/:id', makeCallback(deletePost));
 
module.exports = router;

Acima está a configuração das rotas da API. Pela primeira vez na aplicação, o framework para lidar com comunicação HTTP é importado e utilizado; no caso foi escolhido o ExpressJS (link). Depois, são importados também os controllers e um adaptador “express-callback”. Esse adaptador irá receber os controllers e irá convertê-los ao formato que o express espera no “Router”. O formato que o express espera é uma função callback que recebe uma requisição e uma resposta como parâmetros e, é para esse formato que o adaptador irá convertê-los. Ao final o router é exportado para o servidor poder usá-lo.

Por último temos a declaração da API com o Express:

/* infrastructure/post/webserver/index.js */
const express = require('express');
const bodyParser = require('body-parser');
 
const routes = require('./routes');
 
const app = express();
app.use(bodyParser.json());
app.use(routes);
 
module.exports = app;

Aqui é criado uma aplicação usando o Express e dois middlewares são utilizados: o body-parser para facilitar o manuseio dos dados que chegam nas requisições HTTP e as próprias rotas da aplicação, que foram configuradas anteriormente. Ao final a API é exportada. Não estamos iniciando o servidor nesse arquivo, essa parte juntamente com toda configuração relacionada à rede (porta, protocolo, etc) será realizada em um local na raiz da aplicação que é “bin/www”. Esse arquivo “www” é um shell script (link) que vai lidar com todo o processo descrito anteriormente. Isso é uma boa prática do express, já que isso permite testar a API sem realizar chamadas de rede, com vários benefícios: execução rápida de testes e obtenção de métricas de cobertura do código. Isso também permite implantar a mesma API em condições de rede flexíveis e diferentes, além de deixar o código mais limpo.

Organização dos componentes

Agora que a aplicação foi separada por camadas, analisar como organizar os componentes é mais uma decisão arquitetural importante a se fazer. Componentes oferecem um mecanismo para agrupar artefatos. Nada exige que um arquiteto use componentes, acontece que geralmente é útil ter um nível de modularidade mais alto do que o nível mais baixo oferecido pela linguagem. Nesse artigo iremos citar dois tipos de organização por componentes que mais se encaixam nessa arquitetura: particionamento técnico e particionamento por domínio.

No particionamento técnico, todos os componentes são agrupados de acordo com a camada em que faz parte no DDD. Já no particionamento por domínio, os componentes são agrupados bem… por domínio!

Cada um desses particionamentos possuem vantagens e desvantagens sobre o outro (Capítulo 8 do livro: link). O particionamento técnico tende a funcionar muito bem quando se tem uma arquitetura monolítica, ou seja, quando todos os serviços são em um único processo. Já o particionamento por domínio casa muito bem com as arquiteturas de microsserviços, onde cada serviço é executado em um processo diferente.

Testes

Não teria como terminar esse artigo sem falar pelo menos um pouco sobre testes. Uma coisa que muitos desenvolvedores fazem é querer diferenciar código de testes de códigos da aplicação; sim, até tem alguma diferença, mas no final tudo continua sendo código e assim devem ser tratados da mesma maneira. As mesmas ideias apresentadas em todo esse artigo valem também para os códigos de testes: organização em camadas, isolamento das regras de negócio dos frameworks e drivers, etc. Seus testes não devem estar totalmente acoplados a um frameworks (Jest, mocha, etc), mas sim adaptável a mudanças futuras. Sabemos, tudo isso tem cara de “overengineering” (link), mas acredite; para uma aplicação grande, com dezenas de milhares de testes, isso pode fazer toda a diferença quando um framework ficar ultrapassado, deixar de ser útil ou apresentar algum tipo de falha. 

Considerações finais e recomendações

“Entendi… então devo usar essa arquitetura em toda aplicação que eu desenvolver?”, e, como em 50% das respostas a perguntas relacionadas a arquitetura de software: talvez!. Não existe bala de prata em Engenharia de Software, já dizia Fred Brooks (link), tudo deve ser analisado por contrapartidas (nossa tradução livre de tradeoffs). A Arquitetura Limpa é extremamente poderosa em aplicações complexas que, ou já são muito grandes ou tendem a crescer no futuro, mas para aplicações simples e pequenas não há necessidade e tudo isso se torna apenas “overengineering” como citamos na seção anterior. Uma analogia que gostamos de fazer nesse último caso é: Contratar o Oscar Niemeyer para desenvolver a arquitetura do boteco da esquina. Um exemplo de aplicação que não beneficiaria em nada e apenas ganharia complexidade desnecessária é a própria aplicação usada como exemplo nesse artigo, um blog simples, que possui apenas funcionalidades CRUD em seus serviços.

“Quão grande deve ser minha aplicação para fazer sentido aplicar a Arquitetura Limpa?”, e, como nos outros 50% das respostas a perguntas relacionadas a arquitetura de software: Depende!. Não tem como selecionar um limite arbitrário para quando se deve usar Arquitetura Limpa e quando não se deve usar. Cada aplicação se comporta de forma diferente, e talvez não faça sentido pro arquiteto do projeto aplicar esses conceitos mesmo em uma aplicação imensa com inúmeros serviços e funcionalidades. Mas esperamos que esse artigo de alguma forma auxilie esse arquiteto ou o próprio time de desenvolvimento a tomar essas decisões no futuro.

Para finalizar, gostaríamos de deixar aqui algumas recomendações para quem se interessou e gostaria de se aprofundar mais no assunto. Obviamente, o livro do Uncle Bob Arquitetura Limpa: O guia do artesão para estrutura e design de software (link) é a principal referência no assunto mas também gostaríamos de recomendar todos os livros mais recentes do mesmo autor (da série “Clean”). Mesmo que alguns discordem de muita coisa dos livros dele acreditamos que todos possuem ensinamentos valiosos para qualquer profissional da área. Outros livros que gostaríamos de recomendar é o DDD (link) e o Fundamentals of Software Architecture (link), esse último lançado esse ano inclusive (2020). Por fim gostaríamos de recomendar também esse vídeo (link), que é sem dúvidas a melhor implementação dessa arquitetura que conseguimos encontrar e que foi o que utilizamos como base para grande parte do código desenvolvido neste artigo.

Código no Github: link

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software.

Escrito por Ailton Sávio Sacramento Júnior e revisado por Igor Muzetti e Prof. Tiago Carneiro"
A vida do Product Owner não é Fazer Piada!,http://www2.decom.ufop.br/terralab/a-vida-do-product-owner-nao-e-fazer-piada/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/09/1_2t_QPjFq2xiL1ioQKy7vvg-730x350.jpeg,"1. Manifesto Ágil e o papel de um Dono do Produto

O manifesto ágil ocorreu em 2001 onde 17 Engenheiros de Software se reuniram para avaliar a forma que aplicações estavam sendo desenvolvidas. Chegaram aos 4 princípios:

Indivíduos e interações entre pessoas mais que processos e ferramentas.
Software em funcionando mais que documentação abrangente.
Colaboração com o cliente mais que negociação de contratos.
Flexibilidade e adaptabilidade mais que seguir um plano.

A partir de então, alguns processos, metodologias e frameworks se mostraram bem condizentes com essas ideias. O Scrum, Kanban e Extreme Programming (XP) rebatiza um ator importante, uma espécie de analista de negócio que pré-existia no ecossistema tradicional de software. No Scrum ele recebe o nome de Product Owner. Nos demais, seria um representante do cliente diante ao time de desenvolvimento, que interpreta um papel semelhante [2].

Como a ideia é abordarmos o assunto de uma forma prática, iremos apresentar aqui os 4 amigos, isso mesmo, 4 pessoas que se curtem e vivem em pró de um objetivo comum: levar humor ao povo brasileiro. É possível que você já tenha visto algum vídeo desse grupo. Este grupo, nada mais é, que uma equipe que conta piadas de um determinado tema em uma apresentação, onde eles se revezam. Um vídeo bem legal que vale a pena conferir é: Fila de Piadas – Tema: Belo Horizonte.

As questões que podem ter surgido são:

Quem é o Dono do Produto nesse exemplo?
Algum dos 4 integrantes, pode ser o P.O.?
Será o integrante que fez a chamada inicial?

Na verdade, tudo é possível, mas provavelmente eles têm algum diretor que analisa o mercado de humor brasileiro e desempenha o papel de Dono do Produto. Ele provavelmente não apareceu no vídeo. Porém esse suposto integrante pode ter sido de extrema importância. Por exemplo, caso ele tenha trazido a sugestão de realizar um evento na capital mineira. Estudado que os mineiros são receptivos, gostam de casos divertidos, trazido algumas ideias, como por exemplo, abordar a forma do mineiro falar, alguns problemas que eles enfrentam, situações do cotidiano, sotaque e etc.

Voltando para a abordagem teórica, não se esqueça que estamos tratando de uma visão de Engenharia de Software, nesse meio o foco é desenvolver algoritmos, montar plataformas para que o usuário possa interagir, garantir a qualidade do software, permitir a manutenção do mesmo e deixá-lo apto a receber novos comportamentos acordados com o Dono do Produto. Mas nada impede que seja algo divertido também.

Se você partir para uma teoria mais enraizada, com certeza, irá se deparar com o SCRUM Guide [1], dentro desse documento ou em qualquer abordagem mais teórica, será encontrado um Backlog do Produto, onde o principal objetivo do Dono do Produto é gerenciá-lo, fazendo com que o time de desenvolvimento busque nesse “pacote” o que será trabalhado. Ainda em termos literários retirados do guia, podemos citar as seguintes atividades do PO em relação ao Backlog.

(I) Expressar claramente os itens do Backlog do Produto; (II) Ordenar os itens do Backlog do Produto para alcançar melhor as metas e missões; (III) Otimizar o valor do trabalho que o Time de Desenvolvimento realiza; (IV) Garantir que o Backlog do Produto seja visível, transparente, claro para todos, e mostrar o que o Time Scrum vai trabalhar a seguir; (V) Garantir que o Time de Desenvolvimento entenda os itens do Backlog do Produto no nível necessário; (Scrum Guide, 2017, pág 2)

Para continuar, em termos práticos considere o seguinte esquadrão (squad) como sendo um diretor de projetos que representa o cliente diante da equipe, um mestre de cerimônia para garantir que nada fique travado e 4 comediantes. Essa equipe tem a missão de levar humor pelo planeta através de contos engraçados. No Backlog desse esquadrão estarão os seguintes itens.

Fila de Piadas: Espirito Santo  
Fila de Piadas: São Paulo  
Fila de Piadas: Rio de Janeiro   
Fila de Piadas: Goias 
Fila de Piadas: Brasília    
Fila de Piadas: São Luís      
Fila de Piadas: Ceará          

Considere que, ao criar a lista do backlog, a atividade (I) do guia tenha sido realizada, uma vez que o time já tem conhecimento do que é uma fila de piadas, as próximas podem ser bem simples. 

No item (II), considere que o P.O. escolheu a região do sudeste por ser a mais próxima, ou seja, já olhou para uma matriz de esforço e impacto, para alinhar os itens do backlog. 

Em (III), ele já iniciou a tarefa de otimização, uma vez deixando menos distante a próxima parada, o que pode economizar tempo e permitir que seja realizado um maior número de eventos. Porém, não precisa se restringir a isso e já enviar aos desenvolvedores alguns escopos de piadas baseados nos clientes que assistirão o espetáculo, para que o time não precise fazer esse trabalho.

Agora, em (IV) é uma atividade mais operacional, ele pode simplesmente enviar por e-mail as informações do backlog, dentre outras, também pode utilizar alguma ferramenta de gestão que permita uma visão colaborativa de todos os integrantes.

Por fim, em (V) uma boa prática seria utilizar o primeiro princípio do manifesto ágil, interação entre indivíduos. Ele poderia reunir o time e apresentar brevemente os itens do backlog ainda em confecção. Cuidado, lá vem um spoiler: Esse é um início de um refinamento do backlog, mas esse processo objetiva ter uma ideia do como fazer também, portanto aguarde um pouco.

2. Cerimoniais, Papéis e atuação do Dono do Produto

A ideia desta seção de texto é relatar a relação do P.O. com os principais cerimoniais existentes nos métodos ágeis e também sua relação com os demais papéis no processo de Engenharia de Software. Para referenciar os cerimoniais em questão e não ficarmos presos ao framework SCRUM, passaremos por uma espécie de ciclo de desenvolvimento conhecido como PDCA. Plan, Do, Check, Adapt (Planejar, Fazer, Checar, Adaptar).

2.1 Plano de Trabalho

O planejamento do trabalho geralmente é realizado diante de todo o Esquadrão (Squad). O Product Owner apresenta os principais itens a serem desenvolvidos, podendo estes estarem refinados ou não. O refinamento é a especificação do que precisa ser realizado para cumprir determinada demanda. Como boa prática, deve-se levar os itens já refinados. Caso não esteja, pode-se utilizar parte do encontro para refiná-lo ou priorizar algum outro que já esteja pronto.

Esse refinamento é um trabalho em conjunto entre o Dono do Produto e Time de Desenvolvimento, onde o primeiro papel relata as ideias de “O quê” e “Por que” de se construir o item, junto com os requisitos de negócio e funcionalidade. Um ponto importante, na visão do P.O., é que ele é o responsável, porém pode pedir ajuda a todo o time caso sinta necessidade. Por fim, cabe ao Time de Desenvolvimento a tarefa do “Como Fazer”.

Para o exemplo prático, cabe ao nosso esquadrão decidir qual item irá ser o próximo. O time agora poderá se basear na fila de piadas de BH, para tomar essa decisão. Voltando ao nosso backlog, uma vez estando na capital mineira, por motivos de deslocamento os itens do sudeste foram colocados no topo pelo P.O., ou seja, são os prioritários. Hipoteticamente, suponha que o item FP-SP não estava completamente refinado, portanto ficou entre Espírito Santo e Rio de Janeiro. Depois de discutir, por algum motivo comprovado, o time entende que FP-RJ é mais vantajoso, uma vez que os cariocas gastam mais com eventos humorísticos.
Voltando um pouco agora para a teoria, para finalizar o planejamento o Dono do Produto precisa estar ciente que o trabalho é viável de ser realizado no tempo combinado. Ele ficará disponível para tirar dúvidas em relação aos requisitos enquanto o time decide como será feito. Por fim, com os itens acordados a equipe pode iniciar as atividades.

2.2 Acompanhamento do Processo

Em metodologias ágeis é muito comum um encontro rápido e diário entre o time de desenvolvimento. No ciclo PDCA isso representa a fase do “DO”, ou simplesmente fazer. A ideia é o time diariamente se comunicar e entender se o trabalho está adiantado ou se terão que pensar em um plano B, para cumprir o compromisso. Bom, e onde está o P.O.? Nesse momento ele sempre está disponível para tirar dúvidas de negócio ou funcionalidade do time de desenvolvimento, estudar as próximas demandas, interagir com clientes e partes interessadas. Ele também acompanha o processo, mas baseado em métricas de performance e resultado dos times. Um bom exemplo é através da porcentagem de trabalho realizado em relação ao planejado.

2.3 Revisão do Trabalho

Nessa etapa o Dono do Produto é primordial. Pode-se dizer que ele é o principal responsável desse momento, pois convida as partes interessadas da organização para ver o trabalho desenvolvido, testado e pronto para ir para produção. Um ponto importante desse momento é que ele incorpora a visão do cliente, analisa se as regras de negócio e funcionalidades combinadas com o time de desenvolvimento foram cumpridas. Esse “Check” ou checar, também é uma grande oportunidade para levantar débitos técnicos e coletar feedbacks para possivelmente adicionar no Backlog.

No nosso exemplo prático, seria um momento de um ensaio do evento a ser apresentado, onde as piadas são apresentadas e o esquadrão avalia se realmente está tudo certo para a hora do espetáculo.

2.4 Adaptação do Processo

Por fim, mas não menos importante, chega o momento de “Adapt” ou adaptar. Isso mesmo, depois de refinar, planejar, fazer, checar. Todos os membros do time, com a exceção do representante do processo, que atua como um facilitador, levantam os principais pontos fortes e de melhoria. Durante esse encontro, o Dono do Produto atua como um membro qualquer do time, analisando o processo, comportamentos, dificuldades e facilidades durante toda a caminhada. O objetivo dessa fase é um plano de ação para melhorias a serem feitas em paralelo com as demandas para a evolução do esquadrão como um todo.

Em um contexto geral, podemos afirmar que o Dono do Produto é um representante do cliente junto ao time de desenvolvimento. Tem papel fundamental na otimização e geração de valor do trabalho executado e garante a transparência e acompanhamento das atividades diante a Organização. É ele quem garante que as atividades realizados produzirão resultados aderentes às necessidades do cliente. 

Neste artigo apresentamos as principais atribuições de um Product Owner, cuja atuação se assemelha à função do Analista de Negócios nos processos tradicionais de desenvolvimento de software. Buscamos oferecer um exemplo de como o Dono do Produto em um contexto alheio à computação para facilitar o entendimento do texto por leitores com pouca ou nenhuma experiência em desenvolvimento de software. Este artigo foi útil para você? Você vem atuando em função similar à do Dono do Produto? Compartilhe sua experiência conosco, envie-nos uma mensagem! O convidamos a seguir nossas redes sociais.

3. Referências

[1] Guia do Scrum, disponível em https://www.scrum.org/resources/scrum-guide SCRUM Guide. Acessado em 16/08/2020.

[2] Princípios e Práticas para Desenvolvimento de Software com Produtividade, disponível em https://engsoftmoderna.info/, acessado 16/08/2020, Versão atual: 2020.1.4 – ISBN: 978-65-00-01950-6 (impresso) e 978-65-00-00077-1 (e-book).[3] Ciclo PDCA, disponível em https://pt.wikipedia.org/wiki/Ciclo_PDCA

Escrito por Pedro Saint Clair Garcia e revisado por Igor Muzzeti, Ramon Barros e Prof. Tiago Carneiro."
Tutorial: Como realizar um scraping de dados em um website,http://www2.decom.ufop.br/terralab/tutorial-como-realizar-um-scraping-de-dados-em-um-website/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/09/WhatsApp-Image-2020-09-16-at-6.38.18-PM-730x350.jpeg,"Você já precisou acessar diversas vezes as mesmas páginas para verificar informações? Já pensou na possibilidade de alimentar um aplicativo com informações de uma página web?  Diversos trabalhos requerem esse tipo de prática repetitiva e é aqui que entra o Web Scraping. 

Ao criar um scrapper, temos uma forma de automatizar essa atividade para que as consultas nesses sites sejam muito mais rápidas e eficientes. O trabalho não é fácil e requer conhecimentos na área de desenvolvimento web, pois é necessário analisar o código do site alvo do seu scraper para desenvolver o código. 

Cada site possui um código diferente do outro e, dificilmente, um scraper feito para um vai funcionar em outro. No tutorial abaixo, vamos demonstrar as tecnologias que usamos para fazer Web Scraping no TerraLAB e a construção de um código de scraping que vai coletar notícias do nosso blog! Interessado? É só continuar lendo!

Instalação 

O primeiro passo é instalar o Node e o Yarn no seu computador. Para tal, você pode seguir os tutoriais oficiais de ambos. 

Como instalar o node
Como instalar o Yarn (note que, caso você faça a instalação do yarn utilizando o chocolatey, ele já instala o node junto)

Após instalado, você deve criar uma pasta para o seu projeto e abrir um terminal apontado para essa pasta. Inicialize a pasta com um package.json utilizando o seguinte comando:

yarn init

Pressione enter para aceitar as configurações padrões para cada pergunta e espere até que a execução finalize. 

A dependência que iremos utilizar para realizar o scraping é chamada de Puppeteer. Você pode ler mais sobre o Puppeteer e outras funcionalidades, além das que serão utilizadas nesse tutorial, na documentação oficial.

Para instalar o Puppeteer, digite no terminal aberto. 

yarn add puppeteer

Além de baixar as ferramentas  do Puppeteer necessárias, ele também fará download de uma versão recente do Chromium, que é uma versão em código aberto do Chrome. Esse navegador garantidamente funciona com o Puppeteer, por isso o download é feito em conjunto. Para utilizar outros navegadores, confira a documentação oficial. 

Um Scraper simples

Neste tutorial, faremos um scraping na página de posts no blog do TerraLAB (esse mesmo que você está lendo agora!). No fim, iremos salvar os dados em um arquivo de formato csv que pode ser aberto e visualizado.

Inicialmente, vamos começar criando um arquivo chamado index.js dentro da pasta e abrindo esse arquivo no editor de sua preferência. 
O código abaixo representa um scraper extremamente simples, como um Hello World. Cada parte está comentada com uma explicação da sua funcionalidade.

const puppeteer = require(""puppeteer""); // importe o pacote puppeteer
 
 
let scrape = async () => { // crie uma função assíncrona que irá realizar o scraping
  const browser = await puppeteer.launch({
    headless: false,
  }); // cria um browser. A propriedade headless define se o browser irá abrir com interface gráfica ou se apenas irá executar em segundo plano, sem GUI. false = irá abrir interface gráfica; true = não irá abrir interface gráfica
 
 
  const page = await browser.newPage(); // cria uma nova aba no navegador acima
 
 
  await page.goto(""http://www2.decom.ufop.br/terralab/posts/?category=all""); // define a página que queremos acessar e a função goto navega até essa página
  
  await page.screenshot({path: 'example.png'}); // tira um screenshot e salva na pasta onde esse arquivo está
 
  browser.close(); // fecha o browser, indicando que finalizamos o scraping
 
  return []; // no momento, não desejamos retornar nada. Por isso, return []
};
 
 
//chamada da função scrape. O then/catch é para esperar que a função assíncrona termine e para que possamos tratar eventuais erros. 
scrape()
  .then((value) => {
    
  })
  .catch((error) => console.log(error));

Ao observar a sequência de execução do código, é possível perceber que estamos dando para o navegador uma série de instruções, semelhante a forma como iríamos interagir manualmente (abrir o navegador, acessar o site, clicar em um link…). 

Para executar o código acima, abra um terminal na pasta onde está salvo o arquivo e digite o seguinte código:

node <nome_do_arquivo>.js 

No nosso caso, 

node index.js 

Exatamente como a execução de qualquer arquivo .js normal.

A execução da função não imprimirá nada no terminal, mas criará a captura de tela example.png na pasta do projeto. A captura, no momento desse post, ficou assim: 

Como você pode ver, a proporção de tela é diferente da comumente utilizada em computadores atuais. Isso acontece porque o padrão do Puppeteer é abrir um navegador que não está em tela cheia. Em vários sites, incluindo o nosso, isso não é um problema, mas pode acontecer do site alvo do seu scraping ter uma aparência diferente quando exibido nessas dimensões, ocultando informações que você deseja pegar. 

Para garantir que o navegador irá abrir em tela cheia, você pode adicionar uma configuração à página antes de navegar até o site desejado. Dessa forma, você garante que ele já irá carregar corretamente. Adicione o código abaixo na linha após a declaração da constante page. (const page = ….). 

const page = await browser.newPage(); // cria uma nova aba no navegador acima
// ADICIONE AS LINHAS ABAIXO vvv 
await page.setViewport({
    width: 1366,
    height: 768,
    deviceScaleFactor: 1,
  });

Aqui estamos indicando para a página ser aberta na resolução HD.

Leia mais sobre redimensionamento da página e os parâmetros que podem ser passados aqui. 

Com a nova configuração, o print fica dessa forma: 

Definindo o que será coletado e descobrindo como coletar 

Feito isso, temos a página aberta! Nosso objetivo agora é, de fato, coletar as informações que desejamos. Nesse tutorial iremos coletar todos os links das notícias publicadas para que, depois, possamos obter o texto delas individualmente. 

Inicialmente, faremos alguns testes manuais utilizando o navegador. Abra seu navegador de preferência e navegue até a página que estamos analisando. Clique aqui para navegar até ela. 

Para realizar os testes manuais, usaremos o console do navegador para testar a busca por elementos dentro do site. Aprender a utilizar o console é essencial para alguém que gostaria de trabalhar nessa área. Quase todos os navegadores modernos possuem um console de algum modo, porém recomendamos que você utilize o Chrome, ou algum outro baseado em Chromium. Nesse exemplo, utilizamos o navegador para abrir o console no Chrome. Você pode clicar na página em qualquer lugar com o botão direito do mouse e na opção Inspecionar. Ou utilizar o teclado pressionando as teclas Ctrl + Shift + i. Caso utilize outro navegador, utilize os comandos apropriados. 

Um painel se abrirá, conforme imagem abaixo. 

O próximo passo é clicar com o botão direito em um dos elementos que desejamos coletar com o scraping. Nesse caso, um dos “quadrados” que contém notícias. A opção a ser selecionada é a Inspecionar. 

Ao clicar, observe que o conteúdo da barra lateral se altera e passa a indicar onde, no código HTML do site, se encontra o elemento selecionado no passo anterior. 

Você pode notar que o nosso objetivo, a URL do post indicado no quadrado, já está aparecendo ali. O próximo passo é identificar como selecionar essa URL, e, depois, como selecionar todas as URLs da página sob o mesmo padrão. Para tal, clique com o botão direito na tag <a> que contém o link que desejamos coletar com o scraping, na opção Copy, e, após, na opção Copy JS Path. 

Ele copiará para a área de transferência do seu computador o caminho daquele elemento. Para realizar testes com esse código, altere para a aba Console, conforme mostrado abaixo.

Utilizando as teclas ctrl + v ou clicando com o botão direito e em colar, você colocará no console o caminho selecionado no passo anterior. Aperte a tecla Enter e note que o código do elemento foi impresso, indicando que nosso seletor funcionou! 

Nosso objetivo, entretanto, é deixar essa seleção o mais genérica possível, sem perder de vista que apenas queremos selecionar os links. Para tal, é interessante, por exemplo, remover o id do seletor. Identificadores demarcados com # no HTML representam apenas um elemento, e acabam restringindo a busca. Vamos testar remover a parte “#post-1305 >” do código e pesquisar novamente. Nosso seletor ficaria assim: 

document.querySelector(""div:nth-child(1) > a"")

Perceba que, conforme indicado abaixo, o resultado não foi o esperado! O resultado da busca agora é outro. Entretanto, ainda é interessante que façamos a remoção do ID. Dessa forma, voltemos a analisar o HTML de forma a buscar um identificador mais genérico, mas que ainda garanta o resultado conforme esperado.

Voltando para a aba Elements, observe que, acima da tag <a> que desejamos selecionar, temos uma <div>, e, acima dela, a tag <article>! Por se tratar de um identificador mais genérico, a tag article é uma boa opção de substituição. E perceba que o ID que removemos, #post-1305, pertence a essa tag. 

Vamos criar um novo seletor com a tag article. Como o elemento <article> é pai do elemento <div>, que é pai do elemento <a>, nosso novo seletor ficaria assim: 

document.querySelector(""article > div:nth-child(1) > a"")

E funciona! 

Aqui você provavelmente já percebeu que ter uma boa noção de programação Web faz muita diferença nessa área de scraping. É possível tentar avançar e fazer o seu próprio scraping sem esse conhecimento, porém seria interessante aprender o básico antes de tudo. 

O próximo passo é remover o seletor :nth-child(1).  Esse seletor indica que queremos apenas o primeiro elemento identificado por um seletor anexo (div). Entretanto, observe que o querySelector apenas retorna um elemento, sendo esse o primeiro elemento que corresponder com a seleção. Dessa forma, podemos retirar o  :nth-child(1). O novo seletor ficaria assim:

document.querySelector(""article > div > a"")

Agora temos um seletor simples, que se vale apenas de tags semânticas do HTML, e que retorna o esperado! O próximo passo é testar se uma pesquisa por todos os elementos que satisfazem essa busca apenas retorna os links de notícias dessa página. Para isso, utilizamos outro tipo de busca: a função querySelectorAll. O seletor seria o seguinte:

document.querySelectorAll(""article > div > a"") 

O resultado é um vetor com 12 elementos <a>. Essa página contém exatamente 12 posts, o que é um forte indício de que nosso seletor está correto! Mas, para ter certeza, vamos abrir um desses elementos e observar o seu conteúdo. Dentre inúmeros elementos que não são importantes para o que estamos desejando, podemos encontrar a tag <a> e o link do segundo post listado na página no momento (posição 1 do vetor). 

Aplicando seletores no Puppeteer

Agora sabemos que nosso seletor está correto e funciona! O próximo passo é utilizar esse seletor dentro do Puppeteer. A constante page possui duas funções que serão importantes nesse momento: $eval e $$eval. Ambas recebem como primeiro parâmetro um seletor (string) e uma função a ser aplicada no resultado desse seletor, como segundo parâmetro. A principal diferença é que $eval apenas retorna o primeiro elemento encontrado e $$eval retorna todos os elementos encontrados. 

O código para pegar essas urls no Puppeteer ficaria assim: 

const urls = await page.$$eval(""article > div > a"", (el) => {
    return el.map((a) => a.getAttribute(""href""));
  });
O await é importante para garantir que o código vai esperar pelo carregamento da página e pela busca dos elementos! 
A função $$eval será chamada na constante page (page.$$eval)
Como dito, o primeiro parâmetro da função é o seletor a ser utilizado. O seletor que definimos nos passos anteriores é o “article > div > a”. 
O segundo parâmetro é uma função que faz o tratamento dos dados e prepara o retorno para que fique no formato que precisamos. Explicaremos a função com mais detalhes abaixo.
O resultado disso será um vetor de strings que conterá as urls dos posts a serem acessados.

A função de retorno que utilizamos acima é essa:

(el) => {
    return el.map((a) => a.getAttribute(""href""));
});

Utilizamos arrow functions para fazer essa função. Para entender mais sobre arrow functions você pode ler sobre elas aqui.

(el) = é o conjunto de elementos retornados após se aplicar o seletor do primeiro parâmetro na página. Esse valor é um conjunto porque usamos a função $$eval, que seleciona todos os elementos que se adequam ao seletor. O vetor contém todas as tags <a> que selecionamos.
Temos então a função da arrow function que é composta pelo código 
return el.map((a) => a.getAttribute(“href”));
Como el é um vetor, vamos utilizar a função map para aplicar uma função passada entre parênteses a cada posição desse vetor. Cada posição é uma das tags <a> selecionadas. A função que o map aplicará será a seguinte
(a) => a.getAttribute(“href”)
Novamente, utilizamos a notação de arrow function. A constante a contém o item de uma das posições do vetor, e, para esse item, utilizaremos a função getAttribute para pegar um atributo desse item. O atributo a ser pego é o href, pois queremos a URL da tag <a> que selecionamos.
Após aplicado o map, ele combina o resultado de cada posição e forma um novo vetor, que será retornado e salvo na variável urls. 

Para utilizar essa função no seu código, modifique-o nos locais indicados abaixo:

 await page.goto(""http://www2.decom.ufop.br/terralab/posts/?category=all""); 
  // ADICIONE A FUNÇÃO ABAIXO v v v 
  const urls = await page.$$eval(""article > div > a"", (el) => {
    return el.map((a) => a.getAttribute(""href""));
  });
// ADICIONE A FUNÇÃO ACIMA ^ ^ ^ 
 
  browser.close(); 
 
// ALTERE O RETORNO, DE FORMA A RETORNAR A CONSTANTE urls 
  return urls; 
 
 
scrape()
  .then((value) => {
	// ADICIONE A LINHA ABAIXO PARA IMPRIMIR O RESULTADO OBTIDO ATÉ AGORA v 
    console.log(value)
  })
  .catch((error) => console.log(error));

Após realizar as alterações acima e salvar o arquivo, você pode executá-lo novamente pelo terminal. O resultado deve ser semelhante ao seguinte: 

Caminhando entre páginas 

Agora que sabemos como pegar todas as urls de uma página, precisamos caminhar entre todas as páginas de postagens disponíveis. Vamos voltar para o navegador e observar o elemento de caminhar entre as páginas. Em especial a seta >>, que caminha para a próxima página disponível. Clique na seta >> com o botão direito, em inspecionar, e observe o elemento encontrado na barra lateral.

Novamente, clique com o botão direito no elemento na barra lateral. Em seguida, clique em Copy e em Copy Js Path. 

Troque para a aba Console e teste o código! Verificamos que ele funciona, mas, novamente, precisamos fazer uma “limpeza” no seletor, deixando-o o mais simples e conciso possível.

Novamente, o primeiro passo é retirar os ids demarcados por # e buscar um substituto que mantenha nosso seletor funcionando. Observando o HTML da página, podemos observar que a tag <ul> possui um marcador de class chamado “page-numbers”. Enquanto identificadores # são ruins por demarcarem apenas um elemento, identificadores de classe são utilizados em vários elementos. Então, podem ajudar a selecionar o que queremos, mas sem especificar excessivamente e acabar removendo acidentalmente algo da lista. 

Nós podemos utilizar seletores de classe da seguinte forma: elemento.seletor-de-classe. Nosso seletor então ficaria assim: 

document.querySelector(""ul.page-numbers > li:nth-child(5) > a"")

Vimos também que é importante remover seletores do tipo :nth-child. Novamente observando o HTML da página, notamos que o elemento que queremos (>>) contém também uma classe que podemos utilizar para selecioná-lo.

Atualizando nosso seletor para utilizar essas classes, ele ficaria assim: 

document.querySelector(""ul.page-numbers > li > a.next.page-numbers"")

O próximo passo é de fato, manualmente, navegar para as próximas páginas e testar se nosso seletor continua funcionando nelas! 

Ao selecionar >> uma vez, a página é atualizada e nosso seletor continua funcionando.

Ao selecionar >> uma segunda vez, a página é atualizada e nosso seletor ainda funciona! 

Ao selecionar >> uma terceira vez, o retorno é null! Isso acontece porque estamos na última página de postagens, e não podemos mais avançar. Isso não é um problema! Vamos utilizar isso como verificador de que já caminhamos em todas as páginas e podemos parar de buscar. 

Aplicando no Puppeteer o caminhamento de páginas

Para essa parte, é importante conhecer mais duas funções da constante page. A função $ e a função $$. Elas têm uma lógica similar à $eval e $$eval, com a diferença de que não recebem uma função para aplicar no elemento encontrado, retornando o elemento em si. Isso permite verificar, por exemplo, se um elemento pode ou não ser encontrado na página. Caso encontre, o elemento estará disponível, senão, o resultado será null. 

Altere seu código, adicionando e modificando as partes indicadas.

await page.goto(""http://www2.decom.ufop.br/terralab/posts/?category=all""); // define a página que queremos acessar e a função goto navega até essa página
  
 
 // ACRESCENTE A PARTE ABAIXO v v v v v 
 
  let haveNext = false; // flag para decidir se existe uma próxima página ou não
  let links = []; // vetor onde armazenaremos  todos os links coletados
 
  do {
    haveNext = false; // a flag vai para falso sempre ao entrar no loop
    const urls = await page.$$eval(""article > div > a"", (el) => {
      return el.map((a) => a.getAttribute(""href""));
    }); // fazemos a chechagem pelas urls dessa página normalmente 
 
    links = links.concat(urls); //concatenamos o resultado dessa página com o das páginas anteriores 
 
    // a linha abaixo utiliza o seletor da seta >> para o elemento com a função $
    const button_next_page = await page.$(""ul.page-numbers > li > a.next.page-numbers""); 
 
    //se o elemento existir (for !== null)
    if (button_next_page) {
      //aguarda pelo término da execução das duas coisas abaixo antes de prosseguir
      await Promise.all(
        [
          page.waitForNavigation(),  //espera que a navegação entre as páginas tenha terminado
          page.$eval(""ul.page-numbers > li > a.next.page-numbers"", e => e.click()) //encontra a seta >> com com $eval e clica no elemento
        ]
      );
      haveNext = true; // caso tenha encontrado a seta >>, a flag vira true e o código do loop é executado novamente
    }
  } while (haveNext);
 
 // ACRESCENTE A PARTE ACIMA ^ ^ ^ 
 
  browser.close(); // fecha o browser, indicando que finalizamos o scraping
// MODIFIQUE O RETORNO ABAIXO, PARA RETORNAR O VETOR COM TODOS OS LINKS v v 
  return links;

Ao executar o arquivo dessa vez, obtemos como resposta todos os links encontrados. 

Coletando as informações das postagens

O último passo é, de fato, abrir cada link encontrado e obter as informações de cada postagem! Vamos utilizar essa página como teste para nossos seletores. 

Inicialmente, vamos pegar o título da postagem. Clique com o botão direito no título da postagem e em inspecionar. Identifique o elemento na lateral e, conforme ensinado anteriormente, clique com botão direito em Copy e em Copy JS path. 

O resultado é o seletor abaixo: 

document.querySelector(""#content > div.header-callout > section > div > div > div > h3"")

Fazendo os mesmos exercícios propostos anteriormente de limpeza do seletor, podemos remover o id #content e observar que o seletor continua funcionando normalmente! 

Nosso seletor final, então, é:

document.querySelector(""div.header-callout > section > div > div > div > h3"")

Podemos também pegar a URL da imagem principal do post! Encontrando o seletor inicial com os passos já mencionados, temos como primeira proposta:

document.querySelector(""#post-1678 > header > a > img"") 

Nesse caso, também podemos remover o id #post-1678 e obtemos o mesmo resultado, ao executar novamente o seletor! 

O seletor final da imagem é:

document.querySelector(""header > a > img"")

O último passo é selecionar o texto de fato! Ao clicarmos com botão direito bem no início do texto e clicarmos em inspecionar, podemos notar que uma div encapsula toda a postagem! 

E essa vai ser a div cujo JS Path usaremos. O seletor, já sem o id, fica assim:

document.querySelector(""div.entry-content > div"")

Perceba que esse seletor trás o texto sem imagens e, caso seja necessário, você pode tratar esse resultado conforme sua necessidade. Você pode ler mais sobre na documentação do Puppeteer.

Com todos os seletores em mãos, podemos completar nosso código!

Coletando as informações no Puppeteer

Uma última função apresentada neste tutorial! A função waitForSelector espera até que um seletor específico esteja disponível. Ela é muito utilizada de forma a garantir que algo que você deseja utilizar já tenha finalizado o carregamento antes de que você, de fato, interaja com esse elemento. Nesse caso, iremos utilizar o seletor do texto e esperar até que ele esteja disponível antes de prosseguir com o código.

Modifique o seu código de acordo com as indicações abaixo!

. . . // parte do passo anterior
} while (haveNext);
// ADICIONE A PARTE ABAIXO  v v v v v v
const posts = []; // vetor que conterá as postagens que são a resposta
  //for para caminhar em cada uma das URLS
  for (const url of links) {
    await page.goto(url); // caminha para a URL 
    await page.waitForSelector(""div.entry-content > div""); //espera até que o texto esteja disponível para ser selecionado
 
    const title = await page.$eval(""div.header-callout > section > div > div > div > h3"", (title) => title.innerText); //seleciona o elemento que contém o título e aplica nele a função innerText, que retorna o texto do elemento 
    const image = await page.$eval(""header > a > img"", (image) =>
      image.getAttribute(""src"")
    ); // seleciona o elemento da imagem e busca o atributo src da imagem, que contém a url da mesma.
 
    const content = await page.$eval(""div.entry-content > div"", el => el.innerText); // seleciona o conteúdo da postagem e pega o texto desse elemento
 
    const post = {
      title, 
      image, 
      content
    }; // cria um objeto com as informações acima
 
    posts.push(post); // adiciona o objeto no vetor 
 
  }
// ADICIONE A PARTE ACIMA ^ ^ ^ 
  browser.close(); // fecha o browser, indicando que finalizamos o scraping
// MODIFIQUE O RETURN ABAIXO PARA RETORNAR O VETOR COM OS POSTS ENCONTRADOS v 
  return posts; 

O resultado ao executar no cmd é um vetor contendo todos os posts encontrados! Tenha paciência, a execução desse código pode levar alguns minutos, pois é necessário caminhar entre todas as postagens do blog. 

Salvando o resultado em um csv

Para salvar o resultado em um csv, vamos utilizar a biblioteca csv-writer. Você pode ler mais sobre essa biblioteca aqui. Adicione-a executando o seguinte código em um terminal aberto na pasta do projeto.

yarn add csv-writer

Após finalizado, importe-o no topo do arquivo.

const puppeteer = require(""puppeteer""); // importe o pacote puppeteer
// ADICONE A LINHA ABAIXO v v v v 
const createCsvWriter = require(""csv-writer"").createObjectCsvWriter; //importe o pacote csv-writer

Depois, altere a chamada da sua função scrape conforme indicado abaixo: 

scrape()
  .then((value) => {
 
// ADICIONE A PARTE ABAIXO v v v v v
    // cria o arquivo e adiciona um HEADER com os titulos das colunas e atribui a constante csvWriter
    const csvWriter = createCsvWriter({
      path: ""file.csv"",
      header: [
        { id: ""title"", title: ""Titulo"" },
        { id: ""image"", title: ""Imagem"" },
        { id: ""content"", title: ""Conteudo"" },
      ],
    });
    // salva no arquivo acima os valores recebidos do scraper
    csvWriter
      .writeRecords(value) // retorna uma promise
      .then(() => {
        console.log(""...Feito"");
      });
 
// ADICIONE A PARTE ACIMA ^  ^ ^ 
  })

O resultado final, ao se executar novamente o código, é um arquivo chamado file.csv que é gerado na pasta do projeto. Esse arquivo pode ser aberto em qualquer leitor de tabelas. 

Caso seja do seu interesse, ao invés de salvar em uma tabela, você pode também enviar os dados para o back-end da sua aplicação, de forma a alimentar um aplicativo ou até mesmo outra aplicação web! 

Dependendo da codificação de fonte do terminal, ao salvar em csv, acentos e pontuações podem ser salvas incorretamente. Entretanto, em chamadas de backend o texto é enviado corretamente. 

Confira o código completo desse tutorial aqui. 

Conclusão

Gostou do tutorial? Ele foi útil para você? Conte para gente, nos comentários, se você teve alguma dúvida ou se já utilizou o Puppeteer em algum projeto seu! 

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software."
Saiba como implementar o serviço de notificações no seu App React Native utilizando a Firebase,http://www2.decom.ufop.br/terralab/saiba-como-implementar-o-servico-de-notificacoes-no-seu-app-react-native-utilizando-a-firebase/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/09/imagem-blog-e-twitter-1-730x350.jpeg,"No tutorial de hoje, vamos ensinar você a implementar o serviço de notificações em um aplicativo React Native. Para tal, vamos utilizar a funcionalidade Cloud messaging da Firebase, e o módulo React Native Firebase que, na data de conclusão desse tutorial, se encontra na versão 6! Esse guia pode não funcionar em versões futuras. Vamos lá?

Antes de começarmos, esse tutorial parte do princípio de que você tem um projeto React Native e um projeto Firebase ativo. Se não tiver algum desses requisitos, você pode conferir os seguintes tutoriais: 

Iniciando com React Native
Criando um novo projeto Firebase
Instalação 

O módulo app do Firebase deve ser instalado antes de se utilizar qualquer outro serviço disponível pela Firebase.

# Usando npm
npm install --save @react-native-firebase/app

# Usando Yarn
yarn add @react-native-firebase/app
Configurando no Android 

Para permitir que o Android faça uma conexão segura com seu projeto Firebase, um arquivo de configuração deve ser baixado e adicionado ao seu projeto.

No console Firebase, adicione uma nova aplicação Android e entre com os detalhes do seu projeto. O “package name do android” (Android Package Name) deve ser exatamente o que está no package name do seu projeto local. Esse nome pode ser encontrado dentro da tag manifest no arquivo /android/app/src/main/AndroidManifest.xml. O caminho se inicia na pasta raiz do seu projeto. 

Baixe o arquivo google-services.json e coloque dentro do seu projeto na seguinte localização: /android/app/google-services.json.

Configurando o Firebase com as credenciais Android

Para permitir que o Firebase use suas credenciais no Android, o plugin google-services precisa estar ativo nesse projeto. Isso requer que dois arquivos sejam modificados na pasta Android.
Primeiramente, adicione o plugin google-services como uma dependência dentro do arquivo /android/build.gradle:

buildscript {
  dependencies {
    // ... outras dependências
    classpath 'com.google.gms:google-services:4.3.3'
    // Adicione a linha acima --- /\
  }
}

Depois, execute o plugin adicionando o código a seguir ao seu arquivo /android/app/build.gradle

apply plugin: 'com.android.application'
apply plugin: 'com.google.gms.google-services' // <- Adicione essa linha
Configurando no iOS 

No console da Firebase, adicione uma nova aplicação iOS e entre com os detalhes do projeto. O “identificador bundle iOS” (iOS bundle ID) deve ser exatamente o bundle ID do seu projeto local. O bundle ID pode ser encontrado dentro da aba “General” quando seu projeto está aberto no Xcode. 

Baixe o arquivo GoogleService-Info-plist. 

Utilizando o Xcode, abra o arquivo /ios/{projectName}.xcodeproj (ou o arquivo /ios/{projectName.xcworkspace se você estiver utilizando Pods. Essa segunda opção é geralmente a utilizada quando se está desenvolvendo um app em React Native). 
Clique com o botão direito no nome do projeto e em “Adicionar arquivos” (Add files), como demonstrado abaixo:

Selecione o arquivo GoogleService-Info.plist que você salvou no seu computador e certifique-se que a opção “Copiar itens se necessário” (Copy items if needed) está marcada.

Configurando o Firebase com as credenciais iOS

Para permitir que o Firebase use suas credenciais no iOS, o SDK do Firebase deve ser configurado durante a fase bootstrap do seu aplicativo. 

Para fazer isso, abra o seu arquivo /ios/{projectName}/AppDelegate.m e siga os passos abaixo.

No início do arquivo, importe o Firebase SDK:

#import <Firebase.h>

Dentro da função já existente didFinishLauchingWithOptions, adicione as linhas indicadas no começo do método: 

- (BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions {
  // Adicione essa parte --- \/
  if ([FIRApp defaultApp] == nil) {
    [FIRApp configure];
  }
  // Adicione essa parte --- /\
  // ...
}
AutoLinking & fazendo build novamente

Assim que os passos acima tiverem sido concluídos, a biblioteca React Native Firebase deve ser ligada (linked) ao seu projeto e sua aplicação precisará de um novo build.

Usuários do React Native 0.60+ tem acesso ao “link automático” (autolinking), de forma que nenhum passo a mais é necessário. Para automaticamente realizar o link, faça build novamente do seu projeto: 

# Android apps
npx react-native run-android

# iOS apps
cd ios/
pod install --repo-update
cd ..
npx react-native run-ios

Assim que o build tiver terminado, sua aplicação vai estar conectada no firebase utilizando o módulo app. Esse módulo por si só não tem muitas funcionalidades! Dessa forma, para utilizar outros serviços da Firebase, cada um dos módulos individuais deve ser instalado separadamente. Nesse tutorial, falaremos apenas do módulo de notificações. 

Link manual

Se você estiver utilizando uma versão antiga do React Native que não suporta autolinking, ou deseja integrar o Firebase a um projeto já existente, você pode seguir os passos de instalação manual para o iOS ou para o Android.

Passo adicional, pode ou não ser necessário!
Android: Habilitando Multidex

Conforme sua aplicação começa a crescer com mais e mais dependências, suas builds podem começar a falhar com um erro bem comum: Execution failed for task ‘:app:mergeDexDebug’. Esse erro ocorre quando o Android chega no limite de 64K  métodos. 

Uma solução comum é adicionar suporte multidex para o Android. Essa solução é comumente utilizada e resolve o problema, mas é recomendado que você leia a documentação do Android e compreenda como ela pode afetar sua aplicação.

Você pode encontrar outros erros comuns e suas devidas soluções na documentação oficial do React Native Firebase, na seção Miscellaneous.

Utilizando o Cloud Messaging para o envio de notificações
Instalação

Esse módulo requer que o módulo app do Firebase já esteja devidamente instalado e configurado. Nos passos acima, já garantimos isso! Dessa forma, o próximo passo é instalar o módulo de messaging:

# Instalando o módulo messaging com o yarn
yarn add @react-native-firebase/messaging

# Instalando o módulo messaging com o npm
npm install @react-native-firebase/messaging


# No iOS, execute o seguinte comando após o final da instalação
cd ios/ && pod install
Próximos passos, iOS: 

O iOS precisa de configurações adicionais antes que você possa receber notificações em seus aparelhos! Existem também uma série de pré-requisitos que são necessários para que o messaging possa ser ativado e devidamente utilizado: 

Você deve ter uma conta de desenvolvedor Apple ativa. 
Você deve ter um dispositivo físico com iOS para receber mensagens
Firebase Cloud Messaging (o serviço de envio de notificações) funciona integrado ao Serviço de Notificações Push da Apple (APNs), entretanto, APNs só funcionam em dispositivos reais. Dessa forma, não é possível testar essa funcionalidade em um emulador.
Configurando seu App iOS

Antes de começar a receber notificações, você deve explicitamente habilitar “Notificações Push” (Push Notifications) e “Modos em plano de fundo” (Background Modes) no Xcode.

Abra o workspace do seu projeto (encontrado dentro da pasta /ios). O nome do arquivo é iniciado com o nome do seu projeto, por exemplo: /ios/{myappname}.xcworkspace. Após aberto, siga os passos abaixo: 

Selecione o seu projeto
Selecione o target do projeto
Selecione a aba “Signing & Capabilities”.
Habilitando as notificações Push

O próximo passo é adicionar o recurso (capability) de notificações Push (Push notifications). Isso pode ser feito pela opção Capability dentro da aba “Signing & Capabilities” selecionada no passo anterior.

Clique no botão “+ Capabilities”
Procure por “Push notifications”

Após selecionado, o recurso será listado abaixo de outros recursos habilitados no projeto. Se nenhuma opção aparecer, o recurso pode já estar ativo no projeto. 

Habilitando o modo plano de fundo (Background Modes)

Depois disso, o recurso Background Modes precisa ser ativado, ativando também os submódulos Background fetch e Remote notifications. Isso pode ser feito pela opção Capability dentro da aba “Signing & Capabilities” (mesmo local do passo anterior). 

Clique no botão “+ Capabilities”
Procure por “Background Modes”

Após selecionado, o recurso será listado abaixo dos outros recursos já ativos. Se nenhum resultado aparecer na busca, o recurso pode já estar ativo no projeto. 
Agora, certifique-se de que os submódulos “Background fetch” e “Remote notifications” estão ativos, conforme indicado abaixo:

Ligando o APNs com o Firebase Cloud Messaging (FCM) no iOS

Alguns passos são necessários: 

Registrar uma key
Registrar um App Identifier
Gerar um provisioning profile

Todos esses passos requerem que você tenha acesso à sua Conta de desenvolvedor Apple. Assim que estiver devidamente logado na conta, navegue até a aba Certificates, Identifiers & Profiles na barra lateral: 

Registrando uma chave

Uma chave pode ser adicionada, dando ao FMC acesso completo ao serviço de Notificações da Apple (APN).  Na opção “Keys”, registre uma nova chave. O nome da chave não importa, mas você deve garantir que o serviço de APN está ativo, conforme imagem abaixo: 

Clique em “Continue” e depois em “Salvar”. Após salvar, você verá uma tela que te mostrará o Key ID privado e te permitirá baixar a chave. Copie o ID privado e faça download da chave para sua máquina. 

O arquivo e o Key ID agora podem ser adicionados ao seu projeto na Firebase. No Firebase Console, navegue até configurações de projeto (Project Settings) e selecione a aba Cloud Messaging. Selecione sua aplicação iOS dentro do menu iOS app configuration. 


Faça upload do arquivo baixado e entre com a key ID: 

Registrando um identificador do App (App Identifier)

Para que as notificações funcionem quando seu aplicativo for para produção, você deve criar um App Identifier que será linkado à aplicação que você está desenvolvendo.

No Menu “Identifiers”, registre um App Identifier. Selecione a opção “App IDs” e clique “Continue”. 

A tela abaixo mostra como colocar o App Identifier na sua aplicação utilizando o Bundle ID. Essa é uma string única gerada quando você inicia seu projeto React Native. Seu Bundle ID pode ser obtido no Xcode, dentro da opção “Target” e dentro da aba “General”. Veja imagem abaixo: 

Próximos passos: 

Entre com uma descrição para o identificador
Entre com o Bundle ID copiado do Xcode
Desça a página e ative a opção Push Notifications (e também qualquer outro serviço que seu app utilizar)

Salve o identificador, ele vai ser utilizado para criar um provisioning profile no próximo passo.

Gerando um provisioning profile

Um provisioning profile permite uma comunicação assinada entre a Apple e sua aplicação. Como as notificações apenas podem ser testadas em dispositivos reais, um certificado assinado garante que o aplicativo a ser instalado no seu dispositivo é um app genuíno e tem as permissões corretas ativas para funcionar. 

No menu “Profiles”, registre um novo perfil. Selecione a opção iOS App Development e clique em Continue. Se você seguiu o passo dois corretamente, seu App Identifier vai estar disponível no menu para ser escolhido:

Clique em Continue. Na próxima tela você vai poder ver os certificados na sua conta Apple. Selecione o certificado de usuário para o qual você deseja designar esse provisioning profile. Se você ainda não tiver criado um certificado de usuário, siga a documentação oficial da apple. 

O provisioning profile criado pode agora ser utilizado quando você for fazer build da sua aplicação para um dispositivo real utilizando o Xcode (tanto no modo debug  como no modo release). Dentro do Xcode, selecione seu projeto na opção target e selecione a aba “Signing & Capabilities”. Se nas preferências do Xcode ele já estiver ligado à sua conta Apple, ele pode automaticamente sincronizar o perfil criado acima. Senão, você pode adicioná-lo manualmente seguindo a imagem abaixo: 

Utilizando as notificações
Solicitando permissões – iOS 

O iOS não permite que notificações sejam exibidas a não ser que você tenha solicitado permissão explícita do usuário. O trecho de código abaixo provém um método que chama uma função nativa de permissão de notificação:

import messaging from '@react-native-firebase/messaging';

async function requestUserPermission() {
  const authStatus = await messaging().requestPermission();
  const enabled =
    authStatus === messaging.AuthorizationStatus.AUTHORIZED ||
    authStatus === messaging.AuthorizationStatus.PROVISIONAL;

  return enabled;
}

Recomendamos que você chame esse método o mais cedo possível na sua aplicação, de forma a garantir que as notificações funcionem corretamente quando você precisar delas! 

Tokens de dispositivo

Para enviar uma notificação para um aparelho, você precisa de acesso ao seu token único. Um token é automaticamente gerado e pode ser acessado utilizando o módulo Cloud Messaging. Ele deve ser salvo no banco de dados do seu sistema e deve ser facilmente acessível quando necessário. 

Salvando tokens

Assim que sua aplicação estiver iniciada, você pode chamar o método getToken do módulo Cloud messaging para receber o token único do dispositivo. Faça conforme indica o exemplo abaixo:

import React, { useEffect } from 'react';
import messaging from '@react-native-firebase/messaging';
import { Platform } from 'react-native';

async function saveTokenToDatabase(token) {
  //Salve o token no banco de dados do seu sistema, como salva qualquer outro conteúdo que você utiliza
}

function App() {
  useEffect(() => {
    // Pega o token do dispositivo
    messaging()
      .getToken()
      .then(token => {
        return saveTokenToDatabase(token);
      });
      
   
    // escuta mudanças no token
    return messaging().onTokenRefresh(token => {
      saveTokenToDatabase(token);
    });
  }, []);
}

Você pode ler mais sobre como essas funções funcionam na documentação oficial. 

Futuramente, traremos aqui um tutorial de como enviar notificações a partir do backend em nodejs utilizando os tokens salvos e gerados pelo front-end. Se já quiser implementar, você pode seguir o exemplo da documentação oficial, que utiliza o Cloud Firestore para armazenamento, e adaptar conforme for necessário para seu projeto. 

Notificações em background ou com o app fechado

Para que as notificações em background funcionem, chame a função setBackgroundMessageHandler o mais cedo possível na lógica da sua aplicação. O exemplo abaixo faz a chamada no index.js: 

// index.js
import { AppRegistry } from 'react-native';
import messaging from '@react-native-firebase/messaging';
import App from './App';

// Register background handler
messaging().setBackgroundMessageHandler(async remoteMessage => {
  console.log('Message handled in the background!', remoteMessage);
});

AppRegistry.registerComponent('app', () => App);
Lidando com notificações em background – iOS

No iOS, é necessário adicionar algumas configurações para que as notificações em background funcionem corretamente. Você pode ler sobre o porquê isso é necessário aqui. No seu arquivo index.js, faça as seguintes modificações: 

// index.js
import { AppRegistry } from 'react-native';
import messaging from '@react-native-firebase/messaging';

messaging().setBackgroundMessageHandler(async remoteMessage => {
  console.log('Message handled in the background!', remoteMessage);
});

function HeadlessCheck({ isHeadless }) {
  if (isHeadless) {
    // O app foi aberto pelo iOS em plano de fundo, ignore
    return null;
  }

  return <App />;
}

function App() {
  // Sua aplicação vem aqui
}

AppRegistry.registerComponent('app', () => HeadlessCheck);

Para que a propriedade isHeadless funcione corretamente, atualize seu arquivo AppDelegate.m como indicado abaixo:

// adicione esse import no topo do seu arquivo `AppDelegate.m` 
#import ""RNFBMessagingModule.h""

// no método ""(BOOL)application:(UIApplication *)application didFinishLaunchingWithOptions:(NSDictionary *)launchOptions"" 
// Use `addCustomPropsToUserProps` para passar propriedades para a inicialização do seu app 
// Ou passe ‘nil’ se você não tiver nenhuma, conforme exemplo abaixo.
// Para o `withLaunchOptions` por favor, passe o objeto `launchOptions` 

NSDictionary *appProperties = [RNFBMessagingModule addCustomPropsToUserProps:nil withLaunchOptions:launchOptions];

// Encontre a instância do `RCTRootView` e atualize o `initialProperties` com a sua  instância de  `appProperties`
RCTRootView *rootView = [[RCTRootView alloc] initWithBridge:bridge
                                             moduleName:@""nameOfYourApp""
                                             initialProperties:appProperties];

Lidando com interações

Quando um usuário interage com a sua notificação clicando nela, o comportamento padrão é abrir a aplicação. Em vários casos, é útil saber se o app foi aberto por uma notificação (dessa forma, você pode fazer algo com os dados enviados nela, como ir para uma tela em específico, por exemplo). 

Para isso, a API provê duas funções: getIniticialNotification (quando a aplicação é aberta estando previamente fechada) e onNotificationOpenedApp (quando a aplicação está aberta, mas está em plano de fundo).

Você pode ver um exemplo de uso dessas duas funções abaixo.

import React, { useState, useEffect } from 'react';
import messaging from '@react-native-firebase/messaging';
import { NavigationContainer, useNavigation } from '@react-navigation/native';
import { createStackNavigator } from '@react-navigation/stack';

const Stack = createStackNavigator();

function App() {
  const navigation = useNavigation();
  const [loading, setLoading] = useState(true);
  const [initialRoute, setInitialRoute] = useState('Home');

  useEffect(() => {
    // Assume que uma notificação do tipo mensagem contém uma propriedade “tipo” no payload que indica a tela a ser aberta

    messaging().onNotificationOpenedApp(remoteMessage => {
      console.log(
        'Notification caused app to open from background state:',
        remoteMessage.notification,
      );
      navigation.navigate(remoteMessage.data.type);
    });

    // Checando se uma notificação inicial está disponível
    messaging()
      .getInitialNotification()
      .then(remoteMessage => {
        if (remoteMessage) {
          console.log(
            'Notification caused app to open from quit state:',
            remoteMessage.notification,
          );
          setInitialRoute(remoteMessage.data.type); // exemplo: ""Configuracoes""
        }
        setLoading(false);
      });
  }, []);

  if (loading) {
    return null;
  }

  return (
    <NavigationContainer>
      <Stack.Navigator initialRouteName={initialRoute}>
        <Stack.Screen name=""Home"" component={HomeScreen} />
        <Stack.Screen name=""Settings"" component={SettingsScreen} />
      </Stack.Navigator>
    </NavigationContainer>
  );
}

A chamada de getInitialNotification deve sempre ocorrer dentro de um método do lifecycle do React que ocorre após a montagem (como o componentDidMount ou o useEffect). 

No Android, você pode testar as notificações em um emulador. No iOS, apenas em um dispositivo real. 

Notificações em primeiro plano!

Para escutar por notificações em primeiro plano (ou seja, quando seu app está aberto), chame o método onMessage dentro da sua aplicação. O exemplo abaixo gera um Alert quando uma nova notificação chega e o app está aberto:

import React, { useEffect } from 'react';
import { Alert } from 'react-native';
import messaging from '@react-native-firebase/messaging';

function App() {
  useEffect(() => {
    const unsubscribe = messaging().onMessage(async remoteMessage => {
      Alert.alert('A new FCM message arrived!', JSON.stringify(remoteMessage));
    });

    return unsubscribe;
  }, []);
}

Você pode ler mais sobre notificações que contém apenas o campo data e outras especificidades e variações na documentação oficial. Sempre consulte primeiro a documentação oficial quando tiver alguma dúvida! 

Conclusão

Nos diga se gostou deste texto e se ele lhe foi útil? Fique de olho em nossas redes sociais, e nos demais conteúdos do nosso blog! Não deixe de compartilhar com a sua comunidade e contar a sua experiência para a gente aqui nos comentários!

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software.

Post escrito por Koda Gabriel e revisado por Arilton Aguilar, Ramon Barros e Camilla Silva."
Dicas para facilitar a portabilidade de aplicativos React Native entre iOS e Android,http://www2.decom.ufop.br/terralab/dicas-para-facilitar-a-portabilidade-de-aplicativos-react-native-entre-ios-e-android/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/09/KodaPost2-730x350.png,"OReact Native surgiu como uma ótima ferramenta de desenvolvimento híbrido, visando facilitar o dia a dia dos desenvolvedores e diminuir significativamente a carga de trabalho de portar aplicativos para iOS e/ou Android. 

Entretanto, apesar de possuir boa parte de seu framework compatível com ambos os sistemas, ainda existem particularidades que só funcionam em um deles. Nessas horas, é importante prestar atenção, de forma a tornar o desenvolvimento híbrido de fato, e diminuir a quantidade de bugs e dificuldades ao testar em dispositivos diferentes.

Hoje trazemos algumas dicas que podem facilitar o seu desenvolvimento híbrido com React Native, diminuindo os problemas causados por compatibilidade de módulos!

Se possível, desenvolva primeiro testando em dispositivos iOS

Nem sempre você tem uma máquina iOS disponível, mas, caso consiga, dê preferência em iniciar o desenvolvimento testando em dispositivos iOS. Aqui, no blog do TerraLAB, já fizemos um tutorial de como instalar uma Máquina Virtual MacOS Catalina no seu computador Windows, então você pode testar essa alternativa, caso não tenha um computador Mac disponível.

Pela nossa experiência, observamos que garantir que o App está responsivo e harmônio no iOS, na maioria das vezes garante que ele estará responsivo e harmônico também no Android. 

Cuidado ao gerenciar toques na tela!

Para lidar com toques na tela em geral (sejam em botões ou qualquer outro componente), temos componentes Touchable para gerenciar essas ações. Entretanto, algumas delas tem uma “reação” ao toque mais de acordo com o esperado no iOS, e outra, mais de acordo com o Android. 

Os quatro componentes mais comuns são: 

TouchableHighlight 
Funciona no iOS e no Android, mas tem uma reação ao toque que é própria do iOS
TouchableNativeFeedback
Funciona apenas no Android e tem uma reação ao toque que é própria para ele
TouchableOpacity
Funciona no iOS e no Android, e reage reduzindo a opacidade do botão ao ser tocado
TouchableWithoutFeedback
Funciona no iOS e no Android, não possui reação ao toque

Portanto, nota-se que existem botões com feedback mais apropriado para cada sistema operacional. Nossa dica, nesses casos, é criar um componente que retorne o Touchable apropriado para cada OS (TouchableHighlight ou TouchableNativeFeedback), e utilizar os outros dois apenas em casos específicos. 

Um exemplo de componente é o seguinte:

import React, {useRef} from 'react';
import {
  Platform,
  TouchableNativeFeedback,
  TouchableOpacity,
} from 'react-native';
 
const Touchable = ({innerElement, onPressFunc, style, disabled}) => {
  const touchableOpacityRef = useRef(null);
  const onPressIOS = () => {
    onPressFunc();
    touchableOpacityRef.current.setOpacityTo(50);
  };
  if (Platform.OS === 'ios') { // aqui garantimos que o componente certo será retornado
    return (
      <TouchableOpacity
        disabled={disabled}
        ref={touchableOpacityRef}
        onPressIn={() => onPressIOS()}
        style={style ? style : []}>
        {innerElement}
      </TouchableOpacity>
    );
  } else {
    return (
      <TouchableNativeFeedback
        disabled={disabled}
        onPress={onPressFunc}
        style={style ? style : []}>
        {innerElement}
      </TouchableNativeFeedback>
    );
  }
};
 
export default Touchable;

Utilizando o componente Platform, do React Native, conseguimos testar qual sistema operacional está sendo utilizado naquele momento no dispositivo e retornar o componente correto para uso. 

Recebemos alguns parâmetros extras para garantir o funcionamento correto e possibilitar o melhor uso do componente. 

innerElement (obrigatório)
Os componentes que ficarão dentro do Touchable e deverão ser tocáveis
onPressFunc (obrigatório)
Função que será chamava ao tocar no Touchable
style (opcional)
Variável de estilo que, se enviada, aplicará o seu estilo ao componente
disabled
Variável booleana que, se enviada, desativará ou ativará o Touchable

Após criar esse componente, basta utilizá-lo sempre que precisar de um TouchableHighlight ou TouchableNativeFeedback. Ele retornará o que for adequado ao sistema operacional em utilização no momento, sempre garantindo o funcionamento do Touchable.

Recentemente, o React Native liberou em sua nova versão o componente Pressable, que será, ao que tudo indica, um novo componente de manutenção longa que permitirá lidar com toques na tela. Ele funciona tanto em iOS como em Android e possui por padrão o estilo de feedback do iOS. Entretanto, é possível ativar o estilo de feedback do Android passando uma propriedade extra. Você pode ler sobre esse novo componente aqui. 

Caso esteja começando um projeto agora, ele é a melhor solução! Para manutenção em projetos que já estão em andamento, recomendamos a criação do componente listado acima, que facilitará na hora do uso.

Preste atenção nos notchs

Você sabe o que é um notch? “Também chamado de entalhe, o notch é uma área na parte frontal de um smartphone que fica sobre a tela, normalmente para acomodar a câmera frontal ou sensores, como o de luminosidade, para ajustar o brilho de forma automática.” (TechnoBlog, 2018). Ele se popularizou há alguns anos, e agora é comum de ser encontrado em vários dispositivos Android e é encontrado nos iPhone X e 11. 

Por ele ficar parcialmente para “dentro” da tela, ele ocupa um espaço no centro que, caso não se tome cuidado, pode acabar tampando parte da sua aplicação. Ela precisa levar em conta esses aparelhos, de forma a não causar sobreposição de informações. 

É importante ressaltar o comportamento padrão de cada SO ao lidar com notchs na tela. No Android, o notch faz parte da StatusBar, que assume uma cor sólida. No iOS, a StatusBar assume uma cor transparente, deixando em evidência o que está atrás dele — no caso, seu aplicativo. Isso também deve ser levado em conta durante as decisões de design. 

Para resolver esses problemas, recomendamos o uso de uma pequena biblioteca, disponível no npm e yarn, que retorna a altura da StatusBar do dispositivo, área onde fica o notch, possibilitando que você tome decisões a partir disso. 

Você pode ler sobre essa biblioteca aqui.

Como descrito na documentação, ela retorna o valor conforme não somente o SO (iOS ou Android), mas também considerando o modelo do aparelho iOS ao retornar, enviando o valor correto se o aparelho for a versão X ou 11. Para o Android, o valor é calculado em tempo real e retornado ao usuário.

No Android, também é possível utilizar o componente StatusBar, disponível já no React Native de forma nativa. Apenas nesse sistema, a propriedade currentHeight retorna a altura atual da StatusBar. Você pode ver mais sobre esse componente aqui.

Sempre teste na maior quantidade de dispositivos possível!

Isso vale tanto para o desenvolvimento híbrido, quanto para qualquer outro tipo de desenvolvimento! Sempre teste bastante, em vários dispositivos, de forma a garantir que nada está fora do lugar, não está cabendo na tela ou tendo um comportamento diferente do esperado.

Em máquinas MacOS, físicas ou virtuais, é possível testar em todos os dispositivos iOS atualmente suportados pela Apple. É importante usufruir desse recurso! 

No Android, é possível instalar diversas versões, simulando vários aparelhos com tamanhos e proporções diferentes de tela usando o Android Studio. O recurso é pesado, mas vale a pena ser utilizado conforme possível para garantir que seu App não terá comportamento inadequado em nenhum aparelho!

Conclusão

Essas foram nossas dicas de hoje! Esperamos que elas te ajudem ao desenvolver em React Native e que minimizem o trabalho em garantir compatibilidade com os dois sistemas operacionais. Gostou das nossas dicas? Tem alguma que poderia nos ajudar, ou ajudar outras pessoas? Comenta aqui embaixo sua opinião!"
Criando uma Máquina Virtual para desenvolvimento iOS,http://www2.decom.ufop.br/terralab/criando-uma-maquina-virtual-para-desenvolvimento-ios/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/08/MaquinaVIrtualRetangulo-730x350.png,"Desenvolver para iOS é uma barreira que diversos desenvolvedores encontram, seja pela necessidade do pagamento anual da conta de desenvolvedor para o deploy do aplicativo ou a obrigatoriedade de utilizar um sistema MacOS para a compilação dele. Visando derrubar uma parte dessa barreira, o TerraLab apresenta neste tutorial como instalar o MacOS Catalina em uma máquina virtual e os programas básicos para o desenvolvimento.

Para o sistema funcionar com fluidez, é necessário um computador com recursos consideráveis. O hardware em que usamos a máquina virtual está listado abaixo:

16GB de Ram DDR4 3000MHz
Processador com 12/6, 19MB Cache e 3.2Ghz

A placa de vídeo é dispensável no caso, visto que o Mac, na máquina virtual, não tem suporte para aceleração de vídeo (então você infelizmente não vai conseguir ativar os efeitos legais de transparência e blur). 

Além disso, o seu computador também deve ser capaz de virtualização e essa opção deve ser ativada na BIOS. A maioria dos computadores hoje em dia suportam. Então, se você tiver o hardware potente o suficiente para rodar uma máquina virtual, provavelmente é atual o suficiente para não ter problema com isso.

Caso esteja em um Notebook, um teclado e mouse USB são necessários, visto que a máquina virtual provavelmente não vai suportar o teclado interno e o touchpad. 

Instalando o VMWare

Vamos utilizar uma versão específica do VMWare Player. A versão utilizada não é a mais atual e não deve ser atualizada. As versões mais novas possuem alguns bugs relacionados a instalação do Mac, que dificultam ou impedem a execução normal do sistema. 

Você pode baixar essa versão AQUI.

Ao longo da instalação pode ser necessário reiniciar o computador uma ou mais vezes.

Quando você finalmente chegar na tela abaixo, quer dizer que o VMWare Player está corretamente instalado. Feche totalmente o programa antes de seguir para a próxima etapa. 

Desbloqueando o VmWare

Normalmente, o VmWare vem bloqueado para instalação do Mac. É necessário desbloquear esse suporte e para isso vamos utilizar o aplicativo Unlocker que pode ser baixado aqui.

Ao extrair o Unlocker em algum lugar, você deve rodar o arquivo win-install.cmd como administrador. A execução do script leva alguns segundos e nesse meio tempo você vai ver a tela abaixo:

Se você não viu essa tela, provavelmente é porque não executou o arquivo como administrador ou não fechou o VMWare Player e o VMWare não foi desbloqueado com sucesso.

Baixando o MacOS e o VMWare ToolsTOOLS

Existem diversas formas de conseguir a imagem de instalação do Mac. Você pode criar sua própria imagem utilizando um dispositivo Mac real ou pode baixar a imagem de algum lugar. Estamos disponibilizando o link abaixo com download do sistema. Foi essa imagem que usamos para desenvolver o aplicativo da Orquestra Ouro Preto.

LINK DA ISO DO MACOS CATALINA

Além da imagem do sistema, também vamos precisar da imagem do VMWare Tools que pode ser encontrada no link abaixo. 

LINK DO VMWARE TOOLS

Vamos usar o VMWare Tools para instalar alguns pacotes no Mac que melhoram a velocidade do sistema, resolução e suporte avançado para dispositivos USB, dentre outras coisas.

Configuração Inicial da Máquina Virtual

Abra o VMWare Player. Você deve seguir os passos abaixo para configurar sua máquina virtual.

Clique em “Create a New Virtual Machine”. 

Nessa tela, selecione a opção para instalar um sistema operacional depois. Clique em Next.

Nesta tela, escolha MacOS e a versão 10.14. Clique em Next. 

Agora, escolha o nome da sua máquina virtual e sua localização. O nome normalmente é uma referência para a função da máquina ou seu sistema operacional. Aqui iremos chamar de Catalina.

É necessário escolher um local com espaço suficiente em disco para caber o HD da máquina virtual, o mínimo recomendado é 40GB mas estamos utilizando 100GB no tutorial. 40GB é muito pouco considerando o tamanho das ferramentas de desenvolvimento e demais subprodutos que serão gerados.

Escolha o tamanho conforme explicamos acima. Qualquer coisa acima de 100GB é válida. Além disso, marque para armazenar o disco em um único arquivo, armazenar em múltiplos arquivos compromete muito a velocidade de transferência dentro da máquina virtual.

Clique em Next.

Agora vamos personalizar o Hardware da máquina virtual. Clique em Customize Hardware quando chegar na tela abaixo.

Para memória, você deve selecionar a metade da memória disponível no seu computador. O mínimo recomendado para o Mac Catalina é 4GB de Ram. Não testamos com um computador contendo essa quantidade de memória, então o desempenho pode eventualmente não ser tão bom.

Em processador, escolha metade do número total de threads que o seu tiver. Marque também a opção “Virtualize Intel or AMD” como marcada na imagem.

Em CD/DVD driver, escolha a opção da imagem e selecione a ISO do MacOS Catalina que você baixou anteriormente.

Em USB Controller, marque a opção apresentada abaixo.

Após realizar essa configuração, clique em Close e em Finish. Sua máquina virtual vai ser criada e vai ficar na tela principal do VMWare Player do seguinte modo:

Nesse momento, feche totalmente o VMWare. Precisamos configurar um arquivo na pasta onde criamos a nossa máquina virtual e para isso é necessário que o VMWare não esteja rodando.

No diretório escolhido para a máquina virtual, você vai ter a seguinte estrutura de arquivos:

Abra o arquivo com extensão VMX em um editor de textos. Na imagem acima seria o arquivo macOS 10.vmx de tamanho 2KB.

Vamos realizar duas modificações. A primeira delas é a seguinte:

A linha

virtualHW.version = ""16""

deve ser alterada para

virtualHW.version = ""10""

A próxima modificação vai depender do seu processador. Vamos adicionar alguns dados ao final desse mesmo arquivo. 

Se seu processador for INTEL, vá até o final do arquivo, crie uma nova linha e digite nela isso:

smc.version = ""0""

Se seu processador for AMD, vá até o final do arquivo, crie uma nova linha e digite nela isso:

smc.version = ""0""
cpuid.0.eax = ""0000:0000:0000:0000:0000:0000:0000:1011""
cpuid.0.ebx = ""0111:0101:0110:1110:0110:0101:0100:0111""
cpuid.0.ecx = ""0110:1100:0110:0101:0111:0100:0110:1110""
cpuid.0.edx = ""0100:1001:0110:0101:0110:1110:0110:1001""
cpuid.1.eax = ""0000:0000:0000:0001:0000:0110:0111:0001""
cpuid.1.ebx = ""0000:0010:0000:0001:0000:1000:0000:0000""
cpuid.1.ecx = ""1000:0010:1001:1000:0010:0010:0000:0011""
cpuid.1.edx = ""0000:1111:1010:1011:1111:1011:1111:1111""
featureCompat.enable = ""FALSE""

Agora você pode salvar o arquivo e abrir novamente o VMWare. Clique com o botão direito na sua máquina virtual e então clique em “Settings”.

Você vai ter uma nova aba aqui, a “Options”. Clique nela para chegar na seguinte tela:

Aqui, você deve alterar o sistema operacional guest para Microsoft Windows, versão Windows 10 x64, ficando desta forma:

Basta clicar em OK. Nenhuma outra configuração é necessária. 

Instalando o MacOS Catalina

Agora que você configurou o VMWare Player, basta dar Play na maquina virtual.

Se tudo tiver dado certo, você vai ver essa tela:

Caso você veja o aviso abaixo, clique em “Remind Me Later”

Se o seu mouse ficar preso na maquina virtual, basta apertar “Ctrl + Alt” no teclado para libertar ele. 

Se seu Mouse ou Teclado não funcionam durante a instalação, você pode usar o menu mostrado abaixo para tentar conectar os seus dispositivos.

O problema ao utilizar a conexão desta forma é que eles ficam totalmente presos no VMWare e não podem mais ser utilizados no Windows. Se você conectar o teclado na máquina virtual dessa forma, possivelmente vai precisar reiniciar para usar o Windows novamente, visto que não vai conseguir desprender os periféricos com Ctrl + Alt.

Quando acabar de carregar (pode levar algum tempo), selecione o idioma e avance para a próxima tela.

Sua máquina virtual pode ficar meio lenta agora, mas isso pode mudar quando instalar o pacote VMWare Tools no fim, então não desista no meio do caminho!

Na tela abaixo, clique em “Utilitário de Disco” e em “Continuar”

Você precisa selecionar o HD demonstrado na figura e então clicar na opção “Apagar” da Barra de Ferramentas Superior. 

Altere somente o nome, como desejar, e clique em “Apagar”

Ao final, clique em “Ok” para confirmar e feche o “utilitário de disco”. Você vai retornar para o menu de instalação. Clique em “Instalar MacOS” e então em “Continuar”. Concorde com os termos, selecione a unidade de disco que você criou e confirme a instalação. É normal demorar bastante na tela abaixo

Eventualmente, depois de MUITO tempo de instalação, você vai acabar chegando nessa tela:

Não se desespere, está tudo certo. Nesse momento você deve fechar o VMWare e abri-lo novamente, mas não inicie a máquina virtual!

Clique com o botão direito na máquina virtual e clique em “Settings”. Em CD/DVD você vai selecionar agora o arquivo Darwin.iso, o VMWare Tools que você baixou junto da imagem do Catalina.

Em options, retorne o sistema operacional para Mac OS e escolha a versão 10.14. Várias janelas de aviso podem aparecer e você deve clicar “Ok” em todas. 

Clique em OK para confirmar as novas configurações e inicie a máquina virtual novamente. Agora a instalação vai continuar normalmente! Na tela abaixo escolha o local do mundo onde você está:

Quando ver essa tela, escolha a opção “Personalizar os Ajustes”. Se não fizer isso, o teclado vai vir configurado errado e algumas teclas estarão fora de lugar. 

Mantenha seu idioma preferido como quiser, porém na tela de teclado você deve clicar no botão de adicionar

Escolha o seu padrão de teclado e adicione. ABNT2 é o mais usado atualmente. De todo modo, você vai poder ver um preview do teclado antes de confirmar.

Clique no teclado antigo e depois  em subtrair para removê-lo.

Continue a instalação deixando o idioma de ditado como o padrão. Na tela abaixo marque para não transferir nenhuma informação.

Na tela que pede uma Apple ID, você deve clicar em “Configurar mais tarde” e em ignorar.

A partir desse ponto você pode confirmar tudo, criar sua conta de usuário e escolher seu tema. Eventualmente, depois de todo esse trabalho, você vai ter logado na sua máquina virtual e seu MacOS Catalina foi instalado com sucesso.

Instalando o VMWare Tools

Se você seguiu o tutorial corretamente até aqui, deve ter um ícone escrito VMWare Tools na sua área de trabalho. Dê um duplo clique nele para abrir a instalação e clique em “Install VMWare Tools”.

Continue confirmando o processo de instalação. Provavelmente, você vai precisar digitar sua senha algumas vezes.

A instalação vai ser bloqueada. Clique em Abrir Preferências de Segurança

Clique no cadeado localizado no canto inferior esquerdo para permitir edição.

Então, clique em “Permitir” e pode fechar a janela. A instalação vai continuar até o momento em que você vai receber a “tela de sucesso” abaixo

Confirme para reiniciar e aguarde.

O VMWare Tools foi instalado com sucesso. Você pode maximizar a janela e alterar a resolução da máquina virtual. Todo o sistema deve rodar consideravelmente mais rápido agora.

Obs: Nas configurações da Máquina Virtual você pode remover a imagem do Darwin para não carregar o instalador do VMWare Tools automaticamente toda vez. 

A partir desse ponto, você pode utilizar e instalar tudo que for necessário. Para começar a desenvolver um aplicativo para iOS, basta abrir a Apple Store e procurar por Xcode.

Para usar alguns recursos como a própria Apple Store pode ser necessário criar uma conta Apple e eventualmente adicionar um cartão de crédito. 

A partir daqui, a utilização fica por conta da necessidade individual de cada desenvolvedor. Sua máquina virtual está pronta para ser usada. Teve alguma dúvida ao longo do tutorial? Quer compartilhar sua experiência programando dessa forma? Deseja trocar uma ideia com a gente sobre algo relacionado? Manda ai um comentário que a gente responde!

Link para a pasta com todos os arquivos utilizados durante o tutorial.

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software.

Escrito por Arilton Aguilar. Revisado por Ramon Barros e Camilla Silva."
Lançamento: Conheça o Aplicativo Orquestra Ouro Preto,http://www2.decom.ufop.br/terralab/lancamento-conheca-o-aplicativo-orquestra-ouro-preto/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/08/blogChamada-730x350.png,"Nesta sexta feira (21/08/2020), às 20h30, a Orquestra Ouro Preto realizará, em uma live, o lançamento do aplicativo Orquestra Ouro Preto. Esse aplicativo foi desenvolvido pelo time TerraLAB em uma parceria com a Orquestra, buscando uma maior integração com o público e o envio de notícias e novidades em primeira mão.

O post de hoje servirá como um tutorial de uso, mas também uma espiada no interior do app e suas funcionalidades. 

A navegação dentro do aplicativo é feita a partir do botão no topo superior esquerdo ou arrastando o dedo da borda esquerda em direção ao centro da tela. Utilize a aba lateral para navegar entre as diversas opções do menu.

Na barra lateral, você também encontra as redes sociais da Orquestra (canto inferior esquerdo). Por lá você tem acesso ao Facebook, Youtube, Instagram, Spotify e ao site, tudo com poucos cliques e sem pesquisar nada. Não deixe de acompanhar a Orquestra em todas elas! 

Notícias: 

Você pode acessar as novidades mais recentes diretamente do app! Na página inicial, veja um carrossel com os destaques do momento. Ao clicar em “Mais notícias”, você poderá ver as novidades da Orquestra sempre, diretamente no seu celular!

Calendário:

Aqui você poderá verificar as datas de próximos eventos da Orquestra! Atualmente, todos os eventos estão suspensos devido à pandemia. Por enquanto, você pode conferir as músicas da Orquestra no Spotify. 

Apresentação:

Nessa página, você pode encontrar informações sobre quem é a Orquestra Ouro Preto, cidades visitadas anteriormente, objetivos, ficha técnica e muito mais! 

Amigos de Ouro:

Entenda melhor sobre o que é um Amigo de Ouro e como se tornar um! Saiba detalhes sobre a atuação da Orquestra, o programa Amigos de Ouro e como redirecionar parte do seu Imposto de Renda para a Orquestra.

TerraLAB:

Saiba mais sobre o TerraLAB e a parceria com a Orquestra. Siga nossas redes para não perder nossas novidades!

O aplicativo também contará com notificações enviadas diretamente pela Orquestra Ouro Preto! Novidades, notícias atualizadas e eventos: tudo direto no celular. 

O lançamento ocorrerá na sexta feira, durante a live da Orquestra, e você poderá testar essas funcionalidades! O aplicativo estará disponível para iOS e Android, diretamente na loja! 

Gostaríamos de agradecer imensamente à Usemobile pela mentoria e todo o apoio prestado durante a criação desse aplicativo. A parceria entre o TerraLAB e a Usemobile foi de valor imensurável para toda a equipe!

Siga o TerraLAB e a Orquestra Ouro Preto nas redes e não perca nenhuma notícia sobre esse lançamento!

Curta a página da Orquestra Ouro Preto no Facebook

Escrito por Arilton Aguilar e Koda Gabriel. Revisado por Ramon Barros, Prof. Tiago Carneiro e Camila Silva."
Concebendo aplicativos que se ajustam à necessidade do cliente,http://www2.decom.ufop.br/terralab/concebendo-aplicativos-que-se-ajustam-a-necessidade-do-cliente/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/08/ImagemPrincipal-Sem-Titulo-730x350.jpg,"Este artigo tem por objetivo explicar como a equipe do TerraLab se organiza e se comporta nos momentos iniciais de um projeto para o desenvolvimento de um aplicativo, quando é necessário ouvir a organização parceira (não temos clientes, formamos parcerias) e precisamos entender suas necessidades. Ele explica como fazemos para  capturar o conceito do aplicativo que nos é encomendado, identificar as funcionalidades que precisam ser oferecidas e, então, projetar o comportamento desejado para se obter cada funcionalidade. Finalmente, o artigo explica como dimensionamos nosso esforço, garantindo que a parceira terá visibilidade do andamento e acessibilidade a todas as informações ao longo do projeto, facilitando a gestão das expectativas da organização parceira com relação à nossa equipe.

Primeiro, é preciso dizer que não nos colocamos como autoridades no assunto e, por isso, não temos a pretensão de fazer deste artigo um tratado sobre “análise e especificação de requisitos de software”, tema em que diversos autores fizeram contribuições que moldaram o estado da arte conhecido pela academia e pela indústria. Não traremos verdades absolutas e imutáveis, pois nosso aprender é contínuo e optamos por mantê-lo sempre em movimento. No máximo, neste artigo compartilharemos algumas lições que aprendemos nas experiências colhidas ao longo dos anos, a partir de projetos executados com diversos parceiros, em diferentes domínios de conhecimento, passando por softwares puramente científicos em parceria com instituições como o INPE e a FIOCRUZ até, mais recentemente,  os aplicativos desenvolvidos em parceria com a Orquestra Ouro Preto e a Ouvidoria Feminina da UFOP.  

O público alvo deste artigo compreende profissionais e estudantes da área de computação que, apesar de saberem programar, ainda têm muitas dúvidas sobre como traduzir as conversas com um cliente potencial na especificação de um produto que atenda as necessidades deste cliente. Pode parecer uma tarefa trivial à primeira a vista, mas prazos ou orçamentos limitados e uma brincadeira de telefone sem fio trazem desafios que precisam ser vencidos para um projeto bem sucedido. Você deve estar se perguntando: – Que brincadeira é essa? O fato é que enviar toda a equipe do projeto para entrevistar o cliente pode ser caro, constrangedor ou coagir o cliente. Então, aquele membro da equipe que conversou com o cliente precisará explicar o produto encomendado ao demais. É nesta ocasião que muita informação é perdida! Além disso, muitas vezes o cliente não tem clareza daquilo que realmente precisa ou tem dificuldade em se expressar, enquanto que o entrevistador teria de ser absurdamente eficaz em compreender tudo que ouviu e transmitir na totalidade esse conhecimento a sua equipe. Consequentemente, a solução para esse desafio passa pela construção de um documento que registre tudo que foi compreendido e precisa ser lembrado. Ele tem diferentes nomes em cada empresas, nós o chamamos de Documento de Especificação de Requisitos (DER). 

 Existe uma dualidade no público alvo do DER. Do lado da equipe de desenvolvimento, seu conteúdo precisa ser formal o suficiente para permitir que todos entendam sem ambiguidades a missão do produto, as funcionalidades que deverão construir e testar ao longo projeto. Do lado do cliente que vai validar o documento, o conteúdo precisa ter uma linguagem simples e direta que para que ele não sofra com termos técnicos que atrapalham a compreensão daquilo que está sendo encomendado. 

A esta altura você pode estar se fazendo um monte de perguntas. É sério, essa turma está mesmo discutindo documentação! Mas e os métodos ágeis? Coisa mais obsoleta! Eu sempre fiz aplicativos e nunca precisei de um documento nestes moldes. Esse povo é doido? Calma…Você não está de todo errado, fique tranquilo! Nem nós, que alívio! Há sempre um ponto de equilíbrio, não é? Quando você tem alta rotatividade na sua equipe, membros entram e saem da equipe por diversas razões, ou quando você quer escalabilidade e manter diversas equipes atendendo diversos clientes, este documento registra o acordo firmado entre você e cada cliente, ele direciona e foca seus esforços e evita que você se repita ao longo do tempo, ao oferecer uma visão holística do produto ao cliente, aos analistas de UX, aos engenheiros de software e aos engenheiros de teste. Além disto, a nosso favor argumentamos que os métodos ágeis jamais condenaram a documentação, eles apenas privilegiam feedbacks rápidos a partir da avaliação contínua dos produtos pelos clientes. Então, o equilíbrio está em encontrar um DER que contenha informações mínimas, suficientes e necessárias para permitir o dimensionamento e esforço pela equipe de desenvolvimento e o acompanhamento do projeto pelo cliente, sem comprometer a agilidade do processo de desenvolvimento. À propósito, nós utilizamos uma versão customizada do processo SCRUM.

Sem mais demora, seguem algumas lições aprendidas nesta caminhada…

Lição 1 – Não se trata de fazer o aplicativo que tenha mais funcionalidades que os concorrentes. Se ele cumpre sua missão com menos funcionalidades…genial!

Atualmente, é muito difícil imaginar um aplicativo para o qual não exista um concorrente pronto. Logo o seu precisará de um diferencial, talvez será a simplicidade ou a praticidade. Não importa qual o diferencial, o fato é que ele precisa fazer tudo que o concorrente faz, certo? Não, na nossa opinião está errado! Uma causa comum de insucesso de projetos de software é, após a análise dos concorrentes, a equipe decidir por dotá-lo de todas as funcionalidades vistas nos concorrentes e torna-lo um monstro, enorme e formado por partes desconexas, difícil de entender, difícil de manter, difícil de evoluir e difícil de usar.

Outro ponto é, será que toda funcionalidade que o cliente pedir deve constar no aplicativo? Na nossa opinião, a resposta novamente é “não”, para a surpresa de muitos leitores! Afinal, o cliente não é o especialista em computação, ele precisa ser orientado sobre os impactos benéficos e maléficos que uma funcionalidade trará. O fato é que no início de um projeto, nem mesmo o cliente tem clareza daquilo que ele realmente necessita e sobre qual é realmente a origem do problema do qual ele padece. Então, tome nota de tudo, mantenha-se atento e ouça com cuidado, mas tenha em mente que você precisará de algum critério para determinar quais funcionalidades são realmente relevantes e, junto com o cliente, priorizá-las estabelecendo a ordem na qual serão desenvolvidas. 

 Na nossa opinião, a melhor estratégia para lidar com essa questão é: O quanto antes, defina a missão do aplicativo! Isto é, descubra qual problema específico do cliente ele tem de resolver. Registre no DER o enunciado deste problema, seja ele um problema de cunho econômico (aumentar custo ou reduzir gasto), um problema de cunho social (democratizar informação), um problema de cunho organizacional (integrar e manter atualizada informação de diversos departamentos) ou um problema operacional (coletar de forma integrada dados que são obtidos de forma separada por diversos dispositivos). Entenda e documente o problema do cliente em um exemplo tão concreto quanto possível. Muita vezes, esta documentação exigirá que você descreva o contexto organizacional do cliente em que a demanda pelo aplicativo surgiu. Como a organização do cliente funciona sem este aplicativo? Assim que entender bem o contexto organizacional e o problema do cliente, enuncie “a missão do meu aplicativo é resolver o seguinte problema específico:….”. Desta maneira, você poderá utilizar este enunciado para, junto com o cliente, avaliar as funcionalidades candidatas. Aqueles que forem de encontro à missão do produto estão dentro! Nós acreditamos que se um aplicativo resolve um mesmo problema com mais simplicidades (o que muita vezes significa menos funcionalidades), então, ele será mais fácil de aprender, usar, manter e evoluir. Portanto, mais propenso ao sucesso.  

Lição 2 – Não se trata de apenas entregar um produto. É preciso gerir as expectativas do cliente ao longo do projeto!

Ao longo de um projeto é comum que a equipe de desenvolvimento mude, com a entrada e saída de membros por diversas razões. Um pouco mais difícil de imaginar é que o cliente também muda. Não é incomum que os clientes de um aplicativo mudem de cargo, de profissão ou emprego, ou tirem férias, licença ou se aposentem. Além disto, quando o projeto de um aplicativo estiver próximo ao final, quando vocês estiverem próximos ao lançamento, os departamentos de marketing do cliente ou da sua própria empresa lhe pedirão informações sobre o aplicativo. Então, à todo momento, você precisará vender o conceito de um aplicativo e, estranhamente, até mesmo para a empresa que o encomendou. Por isso, sugerimos que você registre no DER os benefícios esperados que o aplicativo traga. Para cada categoria de usuário que irá interagir com o aplicativo, tente registrar que tipo de vantagens espera-se que o aplicativo lhe ofereça. Imagine o aplicativo Uber e responda: Quais são os benefícios para o motorista e para os passageiros?

Além dos benefícios, aplicativos sempre têm limitações, especialmente em suas primeiras versões. Como as pessoas em geral são bastante ocupadas e lidam com diferentes projetos/serviços o tempo todo, é comum que elas voltem a solicitar funcionalidades que já foram descartadas e/ou adiadas. Então, é saudável que você registre no DER o escopo negativo do projeto, identificando aquilo que o aplicativo não fará. Desta maneira, será mais fácil gerir as expectativas do cliente com relação ao aplicativo e a sua equipe de desenvolvimento.  

Lição 3 – Não se trata de “como” desenvolver o aplicativo. O desafio é descobrir “o quê” o cliente necessita. Esqueça que você é programador! 

Durante as conversas com organizações parceiras, especialmente quando elas não são de áreas relacionadas com a computação, notamos que no discurso das parceiras há uma mistura de tudo: Problemas que elas enfrentam, desejos de ter um aplicativo legal parecido com um que elas já viram, sugestões de tecnologias que elas gostariam que fossem utilizadas ou das quais já ouviram falar, insatisfações pessoais, divagações sobre a vida. É uma verdadeira mistura como é a vida e como é a mente humana. Então, cabe a você profissional treinado na “análise de requisitos” descobrir aquilo que realmente é relevante para o aplicativo que está sendo encomendado e identificar aquilo que realmente é importante para aquele momento. 

Alguma vezes, é importante até mesmo questionar se o problema colocado pode ser resolvido pela computação. Não adianta apenas oferecer um aplicativo, o importante é oferecer uma solução! A ética e foco em oferecer soluções devem prevalecer. Certa vez, uma pessoa veio a nós solicitando um aplicativo que equilibrasse a “energia” que ela dedicava a seus clientes e que os clientes dedicava ao tratamento que ela oferecia. Ouvimos com atenção e depois de entender o problema em seus detalhes, explicamos que as questões colocadas não poderia ser resolvidas por nenhum sistema computacional, seria preciso uma mudança em sua forma de se organizar e se relacionar com seus clientes. Para a organização de agenda e marketing sugerimos alguns aplicativos já existentes. Tempos depois, algumas parcerias surgiram porque esta pessoa nos recomendou a outras.

No momento de levantar os requisitos de um software, esqueça que você é um programador e que você domina algumas tecnologias! Não se distraia pensando em “como” implementar o aplicativo. Afinal de contas, não adianta aplicar computação de altíssimo nível e oferecer algo que não se ajusta às necessidades do cliente. É importante foco para descobrir “o que” o cliente realmente precisa e evitar contaminar a especificação do aplicativo com questões que podem ser decididas depois, com mais calma e em conjunto com profissionais especializados de sua equipe. 

Um bom caminho para seguir na especificação de um aplicativo é o seguinte. Antes de qualquer coisa, reconheça os tipos de usuários que irão utilizar seu aplicativo. Depois, identifique, para cada tipo de usuário, as funcionalidades que eles precisam. Então, agrupe essas funcionalidades em serviços. Caso haja alguma dúvida sobre uma funcionalidade, pergunte “para quê”  aquele tipo de usuário deseja esta funcionalidade. As respostas a esta pergunta podem esclarecer o critério de aceitação deste usuários, isto é, quais as condições para que estes usuários se dêem por satisfeitos com a implementação de uma funcionalidade.

Finalmente, resta uma informação importante, durante as conversas com o cliente não é o momento de oferecer soluções ou novas funcionalidades. Imagine-se comprando o carro, cujo o prazo de entrega e custo já estejam definidos e à todo momento o vendedor lhe pergunta: Você gostaria deste acessório? Que tal uma pintura metálica? Já viu esse novo equipamento de áudio? Nossa, que tal esse motor muito mais potente! É claro que para toda essas perguntas a sua resposta imediata será: Sim, eu quero! Aliás, preciso!!! Portanto, cuidado! No momento das conversas, limite-se a entender o problema e as necessidades do cliente, volte ao escritório e junto da equipe e de profissionais especializados, projete a solução que resolva com qualidade o problema e que demande menos esforço de toda a equipe. Na próxima reunião, ofereça ao cliente a solução abalizada por sua equipe.     

Lição 4 – Não se trata apenas de bons algoritmos e estruturas de dados. A experiência do usuário é muito importante! 

Imagine que você seja o “Ás da computação” e que seu aplicativo seja “PHoda!”. No que diz respeito à implementação das funcionalidades, ele utiliza os melhores algoritmos e estruturas de dados e que tenha sido construído com as mais altas tecnologias da atualidade.  Nada disso importa para a maioria dos clientes e sabe o porquê? Porque eles não conseguem ver isso, a experiência que eles têm no uso dos aplicativos encomendados em nada depende disso, na maioria das vezes. Então, no momento da concepção de um aplicativo esqueça toda a computação que você sabe e mantenha seu foco em um bom projeto de UX (User eXperience).

Nós temos em mente que, para conceber e projetar qualquer software, é preciso caminhar junto com cliente vindo “de fora para dentro”, isto é, vindo da visão dos usuários até que seja necessário discutir questões relativas à visão dos desenvolvedores. Evitamos a todo custo que o DER ou que as comunicações com o cliente envolvam jargões da computação. É preciso oferecer ao cliente e aos usuários acessibilidade completa a toda informação sobre o projeto, permitindo-o validar com facilidade todos os artefatos que geramos. Nos esforçamos em utilizar os jargões dos próprios clientes e usuários. O nosso objetivo é facilitar ao máximo as comunicações evitando de toda maneira mal entendidos.

Por isso, acreditamos que um dos mais importantes artefatos que utilizamos na concepção de um aplicativo seja o storyboard. O storyboard é um sequência de telas que captura o comportamento que os usuários esperam para o aplicativo. Ele apresenta em cada tela um conjunto campos, menus e botões que informam ao cliente onde e como as informações serão inseridas e onde e como os resultados serão apresentados. Ele também informa como acontecerão as transições delas. Como imagens dizem mais que mil palavras, as pessoas de fora da computação encontram mais facilidade em entender o projeto do aplicativo através dos storyboards e, de maneira muito intuitiva, conseguem perceber falhas de projeto e propor melhorias. Nós começamos com desenhos feitos à mão nos primeiros encontros com o cliente. A cada encontro, evoluímos os storyboards para terem exatamente a mesma aparência esperada para o aplicativo.

De uma forma geral, concebemos e projetamos software à partir da sua interface com o mundo, pois é somente esta interface que será percebida pelo mundo, não importando se esta interface é uma interface gráfica com o usuário final (GUI – Graphical User Interface), se é uma interface de programação (API – Application Programming Interface) que terá programadores como usuários ou se é uma interface de rede ou interface de hardware que será utilizada para comunicações com outros sistemas ou equipamentos.

Lição 5 – Não se trata de fazer o melhor aplicativo possível. Faça o suficiente para oferecer valor no prazo estimado e, depois, evolua!

Apesar de contra intuitivo, mesmo quem nunca desenvolveu um aplicativo, para agradar seu cliente, tenta desenvolver logo na primeira versão o melhor aplicativo que consegue. Este é um erro clássico de quem está inciando a carreira. Por nunca ter feito um carrinho de rolimã e tentar em seu primeiro projeto construir uma Ferrari, o ingênuo desenvolvedor acaba por estourar prazos e custos. Então, ele termina, na maioria das vezes, por fracassar em seu intuito e em obter um aplicativo que satisfaça seu cliente É preciso ter em mente um dito popular – O ótimo é inimigo do bom!

Diante deste cenário, mesmo tendo alguma experiência, nós adotamos e aconselhamos uma atitude mais conservadora. Ao invés de partirmos para o desenvolvimento do melhor aplicativo possível, preferimos fazer o suficiente para entregar valor ao cliente em ciclos curtos de desenvolvimento, para gradativamente o evoluirmos, inserindo novas funcionalidades ou melhorando sua UX. Para isso, ao final de cada ciclo de desenvolvimento, entregamos versões melhoradas dos aplicativos, solicitamos que os usuários as avaliem e colhemos os feedback de  todos. As mudanças solicitadas são incorporadas as próximas versões do aplicativo, de acordo com a prioridade estabelecida pelo cliente. 

Assim, podemos dizer que apesar de termos um DER que define o escopo do projeto e apresenta seu backlog geral, nós estamos sempre abertos a mudança e acreditamos em convergir o aplicativo para as necessidades que o cliente descobre a partir de muita experimentação. Enfim, não seguimos planos cegamente, sem reagir aos acontecimentos do caminho! Nós apenas entendemos que o planejamento inicial é uma linha mestra que nos mantém no rumo e nos permite dimensionar prazo e esforço. O destino é sempre resolver o problema do cliente, entregar valor e cumprir a missão do aplicativo. Então, manter o rumo requer ajustar o plano inicial e o aplicativo idealizado àquilo que o cliente realmente precisa. Fazemos aplicativos sob medida!

Lição 6 –  Não adianta ir rápido sem saber para onde. Colha feedbacks sempre que possível. 

É uma tendência comum que desenvolvedores iniciantes, principalmentes aqueles com fortes habilidades em programação e no uso de ferramentas para produtividade, se municiem das melhores ferramentas, processos e frameworks e comecem logo o desenvolvimento, solicitando agilidade de toda equipe. No entanto, é preciso ter em mente que o cliente irá entender suas próprias necessidades ao longo do projeto e que, no início, a maioria dos requisitos são instáveis e muito propensos a mudar conforme o cliente experimenta versões do aplicativo. Então, não adianta cansar a equipe sem a certeza de produzir os resultados desejados. Esta atitude poderia impactar negativamente a moral da equipe. 

O desafio aqui é não estafar a equipe ao solicitar retrabalho em grandes volumes por caminhar muito tempo na direção errada, simplesmente, por não colher o feedback do cliente. Por isso, nós utilizamos ciclos de desenvolvimento de duração fixa (uma semana), evitamos ficar sem a avaliação do cliente, adiar reuniões de validação e delongar entregas, esperando que as funcionalidades que gostaríamos de mostrar fiquem prontas. Nós sempre apresentamos tudo que pode ou precisa ser validado pelo cliente, mesmo em estágios intermediários de desenvolvimento. Apresentamos os artefatos que produzimos tão cedo quanto possível, para evitar que falhas de comunicação, concepção ou projeto resultem em mais custos e mais retrabalho. Assim, solicitamos que o cliente valide: Missão, escopo, storyboard, backlog, histórias de usuário, cenários de testes, componentes de software e versões do aplicativo. Temos em mente que o feedback do cliente é a bússola que nos mantém no rumo certo, que cumprir a missão do aplicativo é destino certo e que não adianta irmos rápido para o destino errado.

Bem pessoal, nós chegamos ao fim deste artigo e esperamos que as lições que colhemos lhes ajudem a conceber mais facilmente os aplicativos que têm em mente,  a cativar clientes e a construir um portfólio de projetos bem sucedidos. Que tal nos oferecer  um feedback? Ou nos dizer se estamos no caminho certo? Nos informe se estamos conseguindo ser úteis a vocês? Nos sentiríamos honrados com seu feedback e em receber encomenda de novos artigos. Nosso intuito é divulgar aquilo que fazemos e ajudar outros a fazerem também! Para incentivar essa cooperação e sermos mais eficazes em nosso intuito, colocamos aqui um DER completamente preenchido para que o tomem como exemplo e possam concretizar tudo que conversamos acima. Saiba como o projeto Batmóvel foi concebido! 

Exemplo_Especificação_Batmóvel_v1.4Baixar

Escrito por Prof. Tiago Carneiro e revisado por Ramon S. A. Barros e Camilla Silva."
Convertendo um Modelo de Inteligência Artificial para Web com TensorFlow js,http://www2.decom.ufop.br/terralab/convertendo-um-modelo-de-inteligencia-artificial-para-web-com-tensorflow-js/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/08/post-tfjs-final-blog-730x350.png,"Introdução

Existem diversas ferramentas e bibliotecas que nos auxiliam na implementação de modelos de inteligência artificial. A linguagem de programação Python é hoje uma das mais completas para esse tipo de aplicação graças à larga disponibilidade de bibliotecas para o aprendizado de máquina e para a análise e visualização de dados que possui. 

Entretanto, quando se trata de soluções para web e mobile, a linguagem mais utilizada é o Javascript. E é aí que surge a dúvida: Como incorporar um modelo de inteligência artificial, construído em Python, em uma aplicação web ou mobile, construída em Javascript?

Nesse post nós vamos aprender:

A criar um ambiente virtual Python para instalação das ferramentas necessárias
Instalar a biblioteca TensorFlow.js que realizará a conversão do modelo
Converter um modelo Python para ser usado em uma aplicação Javascript
Modelo para Triagem de COVID-19 em Exames Raio-X

Para a exemplificação desse post, nós iremos utilizar um modelo de aprendizado profundo (Deep Learning), desenvolvido pela Universidade Federal de Ouro Preto (UFOP), em parceria com a Universidade Federal do Paraná (UFPA). Deep Learning é uma das técnicas mais proeminentes no cenário de aprendizagem de máquina e representando o estado-da-arte para diversas tarefas de visão computacional. 

O objetivo deste modelo é diagnosticar pacientes com COVID-19 através de imagens de raio-x do tórax. 

Para o código do modelo, acesse <https://github.com/ufopcsilab/covid-19>. 

Para mais informações veja os pre-prints em <https://arxiv.org/abs/2004.05717v4> e <https://dx.doi.org/10.21203/rs.3.rs-37908/v1>

Tem-se por meio da web, uma das maneiras de permitir a utilização do modelo pelo público em geral. Onde os usuário podem ter acesso e realizar os diagnósticos através de uma interface amigável.

Entretanto, como pode ser observado no repositório Git, o modelo foi construído em Python, e, como dito anteriormente, para soluções web o JavaScript é a linguagem mais comum. 

Então como fazer com que as duas linguagens conversem?  

TensorFlow 

O TensorFlow é a principal biblioteca de código aberto para o desenvolvimento e criação de modelos de Deep Learning (DL). Ele possui um ecossistema abrangente e flexível de ferramentas, bibliotecas, e uma comunidade grande e ativa. O que permite as mais diversas aplicações de DL. 

Como realizar a conversão de modelos?

Apesar de existir uma versão para JavaScript, a vasta maioria das aplicações que utilizam o TensorFlow é desenvolvida em Python. Felizmente, o TensorFlow.js permite a conversão de um modelo criado em Python para um modelo compatível com o JavaScript.

Para prosseguir neste tutorial é interessante que se tenha o Python e o seu gerenciador de pacotes pip instalados. Não é necessário, mas é recomendado o uso de um ambiente virtual (indicamos o pipenv ou virtualenv). 

Então, bora lá:

   1 . Criação do ambiente virtual (passo opcional, mas recomendável)

Por que devemos usar um ambiente virtual?

No ambiente virtual podemos instalar bibliotecas somente nas dependências do projeto que a utiliza, evitando, assim, conflitos entre as bibliotecas e a instalação global. Além disso, ele permite o controle de versão, em que se pode ter mais de uma versão de uma mesma biblioteca instalada em projetos diferentes, sem divergências.       

Nesse exemplos, nós usaremos o virtualenv, para criar e utilizar um ambiente separado para conversão do modelo. 

Então: 

pip install virtualenv 
virtualenv <nome_do_ambiente>
<nome_do_ambiente>\Scripts\activate

Para usuários linux o comando deve ser:

pip install virtualenv 
virtualenv <nome_do_ambiente>
Source <nome_do_ambiente>\bin\activate
 2 . Instalação da biblioteca TensorFlow.js

A biblioteca TensorFlow.js irá permitir a conversão de modelos criados em Python para modelos que possam ser interpretados pela biblioteca e executado em navegadores, servidores node e aplicativos react-native. 

Atenção: Antes de instalar a biblioteca atente-se a versão do Python que está sendo executada, pode ser que não haja suporte para a última versão lançada ou para uma versão muito antiga. Na data de escrita deste texto foi utilizada a versão 3.7.

Para a instalação basta executar o seguinte comando:

pip install tensorflowjs
  3 . Conversão do modelo

Nós usaremos um modelo criado com o Keras, gerado pelo código utilizado para diagnosticar pacientes com COVID-19, como mencionado na seção “Modelo para Triagem de COVID-19 em Exames Raio-X”.

Para aplicar basta executar o comando tensorflowjs_converter no terminal, como a seguir:

tensorflowjs_converter --input_format=keras --weight_shard_size_bytes=n modeloPy/model_final_smart.hdf5  modeloJs
–input_format  keras: O comando “–input_format” é usado para determinar o tipo de modelo que está sendo convertido. “keras” diz à biblioteca que o modelo é do tipo Keras.
 modeloPy/model_final_smart.hdf5: é o diretório onde está o modelo à ser convertido. OBS: deve obrigatoriamente ser escrito antes do diretório onde será salvo o modelo convertido.
modeloJs: diretório já existente onde será salvo o modelo convertido.
–weight_shard_size_bytes=n : seta o tamanho máximo dos arquivos de peso para “n”. Para web e servidores é opcional, porém para react-native deve ser utilizado para que seja criado apenas um arquivo, para isto basta utilizar um valor alto para n, em bytes.

Ao  converter o modelo será criado um arquivo “model.json”, que contém o grafo referente ao modelo, e arquivos “group1-shard\*of\*”, que contém os pesos da rede neural artificial.

Para outros tipos de modelos a conversão segue a mesma lógica e os formatos disponíveis e outros argumentos opcionais para conversão são descritos na documentação do tensorFlowjs.

   4 . Agora é só usar o modelo

Com o modelo convertido agora basta importar a biblioteca do TensorFlowjs e o modelo em seu código. Em um próximo tutorial ensinaremos, utilizando essa mesma rede, como executar uma aplicação em Nodejs e HTML/CSS.

Vale ressaltar que algumas funcionalidades ainda não foram implementadas para Javascript. Assim, caso você obtenha um erro como o abaixo, uma possível solução pode ser converter o arquivo “keras .h5” para um arquivo “saved_model.pb”.

(node:11476) UnhandledPromiseRejectionWarning: Error: Unknown activation: swish. This may be due to one of the following reasons:
1. The activation is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.
2. The custom activation is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().


Para criar um modelo no formato “saved_model.pb” crie um arquivo .py com o seguinte código:

import tensorflow as tf 
import efficientnet.tfkeras #necessário para este modelo

model = tf.keras.models.load_model('diretorio/modelo.h5')
tf.saved_model.save(model,'diretorio/modelo_pb')

Este erro ocorreu pois o modelo é baseado na rede EfficientNet, que possui camadas customizadas, então ao converter redes personalizadas com TensorFlow atente-se ao formato de origem.

Conclusão

Com o avanço do desenvolvimento de modelos de Inteligência Artificial, se torna cada vez mais necessário soluções que permitem viabilizar sua aplicação em produtos finais. 

Hoje, o Python é a linguagem mais utilizada para a construção de modelos, graças a sua vasta gama de bibliotecas voltadas para esse tipo de aplicação. E uma das principais maneiras de transformar um modelo em um produto a ser consumido é através de aplicações web e mobile, que utilizam principalmente o Javascript. 

Esse exemplo buscou, através de um exemplo de aplicação real, mostrar como realizar a conversão de um modelo Python, para uma aplicação Javascript. E assim, permitir que ainda mais desenvolvedores criem soluções que cheguem até o público em geral.

REFERÊNCIAS

https://tecnoblog.net/263808/o-que-e-inteligencia-artificial/

https://virtualenv.pypa.io/en/latest/user_guide.html

https://www.tensorflow.org/js/tutorials

https://github.com/ufopcsilab/covid-19

https://pythonacademy.com.br/blog/python-e-virtualenv-como-programar-em-ambientes-virtuais

3 Linguagens para Inteligência Artificial"
Como realizar um teste automatizado em aplicativos Web utilizando a IDE do CukeTest?,http://www2.decom.ufop.br/terralab/como-realizar-um-teste-automatizado-em-aplicativos-web-utilizando-a-ide-do-cuketest/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/07/Simples-Foto-Selvagem-Post-para-Twitter-1-730x350.png,"Escrito por Ray da Silva Basílio e Vinícius Almeida de Mattos | Revisado por Diego Henrique e Arilton Aguilar

Os testes automatizados são utilizados na verificação das funcionalidades, para encontrar possíveis bugs e garantir a qualidade do software. Pode-se dizer que um software que passa por um detalhado teste de qualidade tem uma maior probabilidade de cumprir as suas especificações técnicas e deixar os clientes mais satisfeitos.

O objetivo deste tutorial é demonstrar o desenvolvimento de testes automatizados nos aplicativos Web implementados em JavaScript e React. Neste tutorial, utilizamos  a IDE do CukeTest que é gratuita e possui uma interface bastante amigável, principalmente para usuários iniciantes na área da automação de testes. Selenium WebdriverIO foi a biblioteca utilizada para inspecionar e interagir com os elementos da tela. Por fim, para o desenvolvimento dos cenários de testes utilizamos a biblioteca Cucumber.

Instalando e configurando o CukeTest

Primeiramente, o tester (nome dado ao profissional que desenvolve os cenários de teste) precisa fazer o download da IDE do CukeTest e configurá-lo para executar o teste “HelloWorld” (teste básico inicial) por meio do passo a passo descrito abaixo:

Depois de realizado o processo de download e instalação, abra a interface do CukeTest e você chegará na seguinte tela:

Clique em “File” e depois em “New Project”:

Vai abrir a seguinte tela:

Digite o nome do seu projeto no campo “Project Name”, selecione “WEB” no campo “Project Template”, selecione a pasta que você deseja salvar esse projeto por meio do campo “Project Path” e clique em “Create”:

O CukeTest abrirá a tela abaixo. No lado esquerdo estão as pastas e os arquivos do seu projeto como: features, step_definitions, support e etc. No centro da tela está localizado o cenário de teste (passo a passo que será executado na realização de um teste) e no lado direito fica o espaço reservado para a programação das funções, comandos e as demais configurações dos testes:

É de extrema importância instalar a pasta node_modules, que contém as dependências e arquivos fundamentais para a execução dos testes. Sem essa instalação os testes não funcionam. Para isso, passe o mouse na barra com o nome do projeto (neste caso, ”TesteWeb”) que está acima da pasta “features”, clique com o botão direito e clique na opção “Show in Cmd Window” para abrir o terminal do CMD na pasta do projeto:

Agora, instalaremos a node_modules. Considerando que o gerenciador de pacotes do “Yarn” esteja instalado em seu computador, basta rodar o comando: “yarn” no terminal e quando a instalação terminar, a tela do CMD deverá ficar aproximadamente da seguinte maneira:

Considerando também que o seu navegador Google Chrome está na versão mais atualizada, digite “yarn add chromedriver@latest” no terminal do CMD e aperte “Enter”. Caso contrário, você pode instalá-lo seguindo as recomendações do site da documentação do Yarn.

Agora, é possível visualizar a pasta “node_modules” na raiz do projeto: 

Executando um Cenário de Teste

O cenário de teste apresentado abaixo foi desenvolvido para navegar em uma determinada página Web (neste caso na página do Bing). 

Para executá-lo, basta clicar no botão de Play que exibe a mensagem “run this scenario” quando o usuário passa o mouse sobre ele. Depois disso, vai aparecer a janela do Google Chrome no site do Bing:

O navegador abrirá e fechará após o teste, por causa do comando “return driver.quit()” localizado no arquivo “hooks.js”, dentro da pasta “support”, que fecha a janela do Google Chrome no site do Bing. 

Feita essa configuração de teste inicial, pode-se desenvolver um código de teste mais elaborado, explorando mais funções e comandos visando atender diferentes tipos de funcionalidades dos diversos projetos de aplicativos Web. A seguir, apresentamos um exemplo básico de teste, que aborda essa estrutura de programação.

Exemplo básico de teste automatizado 

A seguir será implementado um cenário de teste genérico para navegar, clicar em botões, verificar a exibição de componentes (textos, nomes de usuários e etc…) e preencher campos de texto nas páginas Web. Assim, a implementação demonstrada nas imagens abaixo pode ser utilizada para testar diversas funcionalidades em qualquer sistema de aplicações Web.

Criando a feature do teste inicial 

 Para criar uma nova feature clique em “File” e logo em seguida em “New Feature”:

Para renomear o arquivo da feature, basta clicar duas vezes no seu nome e digitar o nome desejado, como por exemplo: “feature1”.

Logo em seguida dê um click duplo em “<Feature Name>” e coloque o nome desejado. Aqui nesta demonstração será colocado o nome: “Teste Padrão”. Da mesma forma será criada uma descrição para a Feature em “feature description” e por fim clique em “Add new scenario”:

Feito isso, a tela do CukeTest ficará assim:

Agora, clique em “Save” para salvar o código e coloque o nome desejado na feature, como, por exemplo, “Teste_Padrao”:

Agora é necessário adicionar os passos do cenário de teste da seguinte maneira. Para isso, clique nos passos e adapte-os para a página e os componentes presentes do seu projeto:

O primeiro comando do cenário de teste foi feito para acessar o site de um determinado aplicativo Web, navegando com o browser até o site do mesmo:

Depois disso, é possível clicar em um botão da página Web do aplicativo com propriedade ID genérica:

E quando a página ou qualquer componente desejado for carregado:

Então, o campo que tiver um ID será preenchido com o texto a ser inserido:

OBS: É importante, ao implementar os testes, utilizar o mesmo padrão na escrita dos comandos 1 a 4 listados acima, pois dessa forma você poderá reutilizar essas etapas facilmente, necessitando apenas de: trocar os termos entre aspas (identificadores dos botões, textos a serem inseridos nos campos, url de sites e etc…); e implementar esse comando padrão no arquivo “definitions.js”. Assim, você pode testar várias funcionalidades e todas elas conseguirão acessar esse mesmo comando apenas trocando o campo dos termos variáveis. Exemplo: com a implementação padrão de um comando para navegar em páginas Web no arquivo “definitions.js”, você poderá utilizar essa etapa no cenário de teste para navegar em qualquer site desejado. O primeiro comando do cenário de teste abaixo, é utilizado para navegar no site do Bing e o segundo no site do Google.

Para a implementação do código dos 4 comandos discutidos acima, desenvolveu-se no CukeTest os seguintes comandos no arquivo “definitions” da pasta step_definitions:

Na parte da implementação do arquivo “definitions.js” temos dois argumentos “arg1 e arg2” que correspondem respectivamente às strings entre aspas da parte do “Scenario”, podendo representar o ID do botão, o nome e o ID do campo de texto, e o texto que indica o nome ou componente da página. Esses args são associados às strings do cenário de teste pela posição, de acordo com a ordem em que eles foram inicializados no escopo da função dos comandos. Como o cenário de teste segue a metodologia do BDD, são necessários três comandos básicos, que são o Given (Dado), When (Quando) e Then (Então). O comando “await” é utilizado para fazer com que o teste aguarde até que uma determinada ação aconteça.

Explicando o código do arquivo “definitions.js”:

drive.get(url): faz com que o CukeTest abra um navegador e acesse a página com o url digitado no cenário de teste;
driver.findElement({id: arg1}).click(): procura um elemento na página do navegador com a propriedade ID denominada de  “arg1” e clica no mesmo”;
driver.findElement({id: arg1}): procura um elemento na página do navegador com a propriedade ID denominada de  “arg1”;
drive.sleep (long_time ou short_time): faz o navegador aguardar um determinado tempo em milisegundos;
driver.findElement({id: arg1}).sendKeys(arg2): procura um elemento na página do navegador com a propriedade ID denominada de  “arg1” e preenche nele o texto de “arg2”.

Cada funcionalidade (Login, cadastro de usuários e etc) deve ser desenvolvida um arquivo “.feature” diferente para promover a organização do código e facilitar a localização dos erros encontrados na realização dos testes.

Mesmo que os comandos dos cenários de teste estejam em features diferentes, a implementação de tais comandos deve ser desenvolvida em um único arquivo “definitions.js”, pois todas as definições das etapas dos testes ficam nele.

Feito isso, você poderá realizar diversos tipos de testes em aplicativos Web!!!

Considerações finais

Visando facilitar a verificação de funcionalidades dos aplicativos web, as soluções de testes automatizados se apresentam como abordagens bastante práticas, viáveis e econômicas. Elas são capazes de possibilitar correções preventivas nos defeitos de software, reduzir os esforços necessários corrigir as falhas, e consequentemente, minimizar os custos de manutenção nos aplicativos, além de serem reutilizáveis, fazendo com que sejam aplicáveis em diversos tipos de projetos. 

Este artigo apresentou um passo a passo de como utilizar a IDE do CukeTest para a realização de testes em aplicativos Web e um exemplo genérico de implementação de teste automatizado para navegar, clicar em botões, verificar se algum componente (mensagem de texto, nome da página) foi carregado e preencher campos de texto em uma página Web.

Assim, é possível aumentar a eficiência das suas aplicações, agregar mais valor, qualidade e confiabilidade aos produtos finais, fazendo com que os clientes fiquem bastante satisfeitos com as suas soluções de software desenvolvidas.

Este post te ajudou a compreender o funcionamento e a implementação de um teste automatizado em aplicativos Web? Você já desenvolveu um teste automatizado? Conte pra gente quais são os desafios e experiências que você tem com a automação de testes Web. 

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software.

Referências

CUKETEST. Visão Geral do CukeTest.Disponivel em: http://cuketest.com/zh-cn/bdd/.Acesso em: 04 jul. 2020.
SELENIUM. Getting started. Disponível em: https://www.selenium.dev/documentation/en/getting_started/. Acesso em: 15 jul. 2020.
WEBDRIVER.IO. API Docs. Disponível em: https://webdriver.io/docs/api.html. Acesso em: 15 jul. 2020.
YARN. Instalação. Disponível em: https://classic.yarnpkg.com/pt-BR/docs/install/#windows-stable. Acesso em: 04 jul. 2020."
Atenção ao escolher um framework de estilos para seu projeto WEB,http://www2.decom.ufop.br/terralab/atencao-ao-escolher-um-framework-de-estilos-para-seu-projeto-web/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/07/Atenção-ao-escolher-um-framework-de-estilos-para-seu-projeto-WEB-1-730x350.png,"Escrito por Diego Matos | Revisado por Koda Faria

Um framework é um facilitador no desenvolvimento de diversas aplicações, e sem dúvida, sua utilização poupa tempo e custos para quem o utiliza. De forma mais básica, são padrões de artefatos e código que provêm diversas funções que podem ser reutilizadas em quaisquer novos projetos, conferindo produtividade à equipe e qualidade aos resultados. Porém, a utilização indiscriminada de frameworks pode gerar problemas. Devemos portanto seguir alguns passos para evitar esses problemas e, caso sejam inevitáveis, buscar fórmulas já testadas para resolvê-los. Este artigo trata especificamente de como o TerraLAB pensa seu framework para desenvolvimento de Frontends WEB. 

O que são estilos e estilização?

Um site construído apenas com HTML com certeza causaria um grande estranhamento devido sua ausência de estilização. É importante para qualquer aplicação WEB que haja estilos. Além de deixar a aplicação mais bonita, melhora a experiência do usuário. 

Geralmente os arquivos de estilização são construídos utilizando CSS (Cascading Style Sheets) onde são utilizadas tags para identificar elementos HTML e aplicar cada estilo a sua tag correta.

Todas as tags que representam elementos HTML existem dentro de uma estrutura de dados chamada Árvore DOM (Document Object Model). Essa estrutura permite algumas facilidades com uso do JavaScript, mas que não serão abordadas neste post. A árvore DOM permite uma busca por qualquer elemento ser intuitiva. Ela é construída por tags que podem ser pai ou filho de outras tags. Se o elemento <p> está dentro do elemento <div> então <div> é pai de <p> e <p> é filho de <div>.

Como escolher um framework de estilos

No TerraLAB, julgamos ser importante escolher apenas um framework de estilização (bootstrap, material ui, material io, materializecss, etc…) para um dado projeto. Solicitamos nossos colaboradores que dêem preferência ao framework que mais converge com os estilos determinados pelo cliente e pelo protótipo do produto. Um importante critério é escolher um que seja completo, o que evitará a procura de componentes presentes em outros frameworks. 

O principal fator para escolher apenas um framework é que ele modifica internamente quase todo seu projeto CSS. Se instalado conjuntamente com outro framework, a chance desses estilos criarem conflitos e sobreposições é muito grande, gerando componentes quebrados e esquisitos.

Mesmo assim, caso você decida pegar um componente presente em outro framework, há uma forma de evitar erros. Ao instalar esse novo componente você deve identificá-lo na árvore DOM de seu navegador e o examinar o mais “profundamente” possível. Assim você poderá pegar informações importantes como quais estilos estão se sobrepondo e quais os parâmetros como “className, id …” do componente.

Como a reutilização de estilos é feita na prática

Utilizaremos figuras para ilustrar essa situação:

Imagem 1- A linha azul escuro, na base da figura, destaca o objeto desejado identificado na árvore DOM. Também é possível ver, no canto superior direito, a área destacada em azul claro, onde o objeto (componentes estilizado) deveria ter aparecido na interface gráfica.

Imagem 2 – Aparência correta e esperada do componente com estilização


Fica claro observando as duas imagens que algo não está certo. O que está acontecendo é que a biblioteca principal do projeto (Materializecss) está sobrepondo o estilo desse componente de outra biblioteca (Bootstrap).

Imagem 3 – O componente na árvore DOM  aparece grifado em vermelho e a sua direita os estilos aplicados a ele.


Ao identificar o elemento devemos observar quais estilos estão sendo aplicados a ele. À direita é possível encontrar a tag que está sobrescrevendo seu estilo. É possível desativar esse estilo dentro do navegador e ver ele se realmente se aplicava ao componente. Ao desativá-lo, pode-se observar na imagem que o elemento fica em seu estado esperado.

Ótimo, agora encontramos o erro e é só resolvê-lo. Para tal devemos “diferenciá-lo”.

Primeiramente, vamos criar um “className” ou “Id” para o componente.

Imagem 4 (Componente agora tem um ClassName=“star-rating”)


Com isso feito vamos usar o “CSS selector” para selecionar exatamente a estilização que queremos alterar. Vimos na imagem 3 que o que queremos mudar é o estilo da tag “Label” e que agora está dentro do className “star-rating”.

Agora vamos no nosso arquivo CSS, de preferência o “GLOBAL.css” e selecionamos o estilo a ser alterado.

Imagem 5 (Arquivo .css selecionando o campo a ser estilizado)


Usando o atributo “unset”, retiramos qualquer estilo anteriormente utilizado para esses elemento. Sendo assim o componente deixa de ser sobreposto e o estilo dele volta a ser o esperado.

Imagem 6 (Estado final do componente)


Seu componente agora está pronto para o uso e sem riscos de quebrar algo em outra parte do código.

Considerações finais

A fim de evitar problemas é preferível utilizar apenas um framework de estilos devido a padronização dos elementos presentes fazendo tudo se “encaixar” bem. Se utilizarmos da solução apresentada nesse post evitaremos vários desses problemas de conflitos."
Propriedade Industrial Patente: O que é? E por que é importante?,http://www2.decom.ufop.br/terralab/propriedade-industrial-patente-o-que-e-e-por-que-e-importante/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/07/15-07-Propriedade-Industrial-Patente-Redes-Sociais-730x350.png,"Escrito por Camilla Silva

No dia 15 de maio de 1997 entrou em vigor a lei nº 9279, que diz a respeito da propriedade industrial, sendo comumente conhecida como a lei das patentes. Essa regula direitos e deveres referidos à propriedade industrial, sendo esses a concessão e utilização não somente das patentes (invenção ou modelo de utilidade ou melhorias), mas como também de desenhos industriais, marcas e indicações geográficas.  Contudo, neste post o enfoque se embasará somente nas patentes.

Invenção ou modelo de utilidade?

De acordo com essa legislação apenas a invenção ou o modelo de utilidade são suscetíveis de patenteabilidade. Os três requisitos básicos para a patenteabilidade são: (1) a novidade, ou seja, deve ser algo novo, em outras palavras a invenção ou modelo de utilidade não deverá estar em uso público ou em conhecimento por pessoas; (2) utilidade e aplicação industrial, a grosso modo dizendo deve ser útil, a invenção deverá realizar exatamente o que foi designado no pedido de patente e as funções especificadas no modelo de utilidade devem atingir resultados benéficos; (3) e a não obviedade ou atividade inventiva, sendo esse o termo que explica que a invenção ou modelo de utilidade não deve ser óbvia para qualquer indivíduo de conhecimento médio científico ou técnico consiga reproduzir e alcançar os mesmos resultados.

A invenção configura-se como algo novo, do zero. Já o modelo de utilidade pode ser explicado como algo novo dentro de algo que já existe, um exemplo que pode ser citado para melhor entender é a invenção de aparelhos telefônicos, sua primeira patente deu-se em 1876 por Alexander Graham Bell, ele “inventou” um aparelho por transmissão de voz. Já como modelo de utilidade temos a proteção das melhorias funcionais no uso  ou fabricação de aparelhos telefônicos. A invenção ou modelo de utilidade patenteada garante o direito de comercialização que o que foi patenteado ou que receba royalties caso alguém usufrute do mesmo, porém somente por um período de tempo.

A duração da patente por invenção é de 20 anos contados a partir do depósito, independente se a aprovação ocorreu meses depois, e a de modelo de utilidade de 15 anos. A patenteabilidade deve atender aos requisitos de novidade, atividade inventiva e aplicação industrial, cumulativamente e não alternativamente.

Atividade inventiva:

Art. 13. A invenção é dotada de atividade inventiva sempre que, para um técnico no assunto, não decorra de maneira evidente ou óbvia do estado da técnica.

Art. 14. O modelo de utilidade é dotado de ato inventivo sempre que, para um técnico no assunto, não decorra de maneira comum ou vulgar do estado da técnica.

Industriabilidade:

Art. 15. A invenção e o modelo de utilidade são considerados suscetíveis de aplicação industrial quando possam ser utilizados ou produzidos em qualquer tipo de indústria

Depósito do pedido de patente

De acordo com o artigo 19 da lei de propriedade industrial, para  depósito do pedido da patente, deve-se constar no pedido: (I) requerimento; (II) relatório descritivo; (III) reivindicações; (IV) desenhos, se for o caso; (V) resumo; e (VI) comprovante do pagamento da retribuição relativa ao depósito.

Após esse pedido, será submetido a um exame que se aprovado será protocolizado. A data do depósito é a da apresentação do pedido. Pedidos que não atenderem a todos os requisitos previstos no art 19, mas que contiverem dados relativos ao objeto, ao depositante e ao inventor, poderão ser entregues mediante o recibo datado, logo serão dispostas exigências a serem cumpridas num prazo de 30 dias, podendo ocorrer a devolução ou arquivamento da documentação.

E por que é importante?

De acordo com Waldo Fazzio, o uso, a venda, a exposição à venda, quanto ao processo patenteado, sem a comprovação do possuidor ou proprietário mediante determinação judicial, se caracteriza como violação. É assegurado ao detentor da patente o recebimento de indenização. Em situação da não aprovação da patente, não se comprova ao autor como dono da invenção ou modelo de utilidade, podendo ocasionar a cópias do mesmo. 

Para saber mais 

 Nos dias 17 e 24 de julho, o Laboratório XR4Good da UFOP promoverá, em parceria com o NITE/UFOP e INPI um minicurso acerca dos aspectos técnicos sobre busca e redação de patentes em dois módulos. Para se inscrever basta acessar a página de instagram do laboratório (XRLGOODLAB) e acessar o link da bio. Não fique de fora, vale a pena conferir!

Bibliografia

http://www.planalto.gov.br/ccivil_03/leis/l9279.htm

https://www.youtube.com/results?search_query=lei+9279%2F96+resumo

https://jus.com.br/artigos/618/lei-9279-96-nova-lei-de-propriedade-industrial

https://www.direitocom.com/lei-da-propriedade-industrial-comentada/titulo-i-das-patentes-art-06-a-93/capitulo-i-da-titularidade-art-06-a-07

Fazzio, Waldo. Manual de direito comercial.12º edição. Editora Atlas. São Paulo, 2011."
Integração contínua e a entrega contínua (CI/CD) no GitLab – O caso de um Frontend Web,http://www2.decom.ufop.br/terralab/integracao-continua-e-a-entrega-continua-ci-cd-no-gitlab-o-caso-de-um-frontend-web/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/07/08-07-Integração-Contnua-e-Entrega-Contínua-Frontend-Redes-Sociais.png,"Escrito por Guilherme Carolino e Arilton Aguilar

Este tutorial busca demonstrar como o TerraLAB utiliza o GitLAB para realizar a integração contínua e a entrega contínua de seus produtos. Este é o segundo tutorial do total de três, sobre o tema CI/CD, desta vez iremos cobrir o caso de um frontend web.  

Alguns dos conceitos apresentados já foram explicados no primeiro tutorial e não serão explicados em detalhes novamente. Qualquer dúvida, caso ainda não tenha visto, você pode ler o  primeiro tutorial desta série aqui no blog! Ele apresenta os conceitos básicos do tema e explica como fazemos CI/CD em backends..

Este  tutorial oferece uma visão dos passos de um pipeline CI/CD completo para projetos de frontends web desenvolvidos em React e finaliza com o deploy no serviço de nuvem AWS. Daremos uma atenção maior aos scripts utilizados no pipeline do que nas ferramentas e dependências ao redor deles, algo já tratado na primeira parte da série de CI/CD.

PREPARANDO O GITLAB

Primeiramente, começaremos preparando nosso ambiente no arquivo .gitlab-ci.yml

Caso esse arquivo não exista no seu repositório, basta criar um novo arquivo com esse nome. 

Utilizaremos a imagem docker node:10 como padrão para nosso script e colocaremos quatro stages para nosso script percorrer. A imagem do node pode variar de acordo com o seu projeto ou você pode usar node:latest para ter sempre a última versão.

ESTÁGIO 1: testBuild

O primeiro stage do projeto é o stage testBuild, no qual é feito o build do projeto para teste e preparação dos arquivos do próprio teste.

Nesse script, começamos utilizando a palavra reservada *only-default ,para o estágio rodar apenas para a brach master, para merge requests ou tags. 

Logo abaixo, definimos que esse stage é o stage de testBuild e, mais abaixo, é possível utilizar uma outra palavra reservada “tags:”, para definirmos um runner específico onde o pipeline irá rodar. Nesse CI/CD utilizaremos os runners disponíveis no próprio GitLAB e por conta disso nós comentamos a linha contendo “tags”.

Depois é utilizado outra palavra reservada que é o “before_script:” onde definimos tarefas que vão ser executadas no nosso projeto antes do script principal. No caso do nosso before_script, estamos instalando o yarn, o gerenciador de pacotes que utilizamos.

Depois de definido o before_script, definimos o próprio script. Como apresentado na imagem acima, utilizamos o comando yarn para fazer o download das dependências do projeto e em seguida fazemos o seu build. Navegamos então para a pasta onde nossos testes feitos em selenium estão localizados e, com outro yarn, realizamos o download das dependências dos testes. 

A última coisa que fazemos é especificar como artefatos de saída desse estágio do pipeline as pastas build e node_modules dos testes. É necessário especificar esses artefatos pois eles serão utilizados em outro estágio do pipeline. Caso não os especifiquemos, essas pastas se perdem ao fim da execução desse estágio. 

Todo estágio do pipeline que possui artefatos vai expor esses artefatos para download no final de toda execução. Esses artefatos ocupam espaço de armazenamento no seu git e, dependendo no número de vezes que esse CI/CD for executado, este armazenamento pode rapidamente se esgotar. Para evitar problemas, especificamos então um tempo de existência para os artefatos. No nosso caso, os artefatos irão existir por uma hora e, ao fim desse tempo, serão eliminados da memória. 

ESTÁGIO 2: testDeploy

O segundo estágio é o testDeploy, no qual o deploy do site é feito em nosso servidor de teste na Firebase. Optamos por hospedar o site de testes na Firebase pois, em diversos casos, ter uma interface visual para os desenvolvedores pode ajudar na identificação de eventuais problemas. 

Diversos detalhes são comuns em todos os scripts então iremos nos concentrar em explicar o que cada um tem de único. 

Aqui, nosso before_script faz a instalação do firebase-tools globalmente. É necessário para podermos fazer o deploy do frontend web na Firebase.

O script executa o comando para o deploy utilizando um FIREBASE_TOKEN previamente cadastrado como variavel de ambiente no gitlab. 

Uma característica desse estágio é que ele é diretamente dependente do estágio anterior, isto é, ele precisa de arquivos produzidos no estágio anterior. Especificamos então em “dependencies” que precisamos utilizar os artefatos produzidos pelo stage testBuild.

Ao final da execução deste estágio do pipeline, nosso site já estará hospedado no servidor de testes.

ESTÁGIO 3: testRun

O terceiro estágio é o testRun, onde iremos rodar o teste automatizado.

Aqui, nós estamos novamente produzindo artefatos, desta vez com o intuito de manter os resultados dos testes contidos na pasta reports salvos por sete dias no gitlab. 

O before_script navega para a pasta de testes onde realizamos a instalação do yarn, chrome, java, cucumber, chromedriver e as dependências para os arquivos Debian. Algumas dessas ferramentas são obtidas pelo próprio node utilizando o yarn, outras precisam ser baixadas externamente e instaladas. 

Ao final da instalação de todas as dependências, podemos finalmente executar o nosso script de testes que consiste em um simples yarn cucumber. 

Lembrando que já tínhamos feito o build do teste no primeiro estágio do pipeline, então é necessário especificar este estágio como dependência. 

ESTÁGIO 4: productionDeploy

O quarto e último estágio é o productionDeploy, que só é feito caso tenha um merge request para nossa branch Master e o teste executado no stage anterior tenha sido executado com sucesso. 

A forma como fazemos o deploy para AWS é semelhante ao backend, apresentado na primeira parte desta série de tutoriais. 

A única diferença aqui está no updateAndRestart.sh onde damos o build do projeto dentro da AWS e em seguida rodamos essa build. 

Esperamos que este tutorial lhe ajude a implementar seu próprio pipeline! Nesse segundo tutorial nós apresentamos como lidamos com o CI/CD do front-end web, mostramos exemplos de como cada um dos estágios (stages) do pipeline devem ser customizados.. Existem diversas formas de realizar esse procedimento e esta é uma delas. E aí, você conhece outras? Quer conversar sobre elas? Teve alguma dúvida sobre nossa explicação? Comentá aí para podermos conversar sobre!"
Licenças e direitos autorais: Por que são importantes para os projetos de software?,http://www2.decom.ufop.br/terralab/licencas-e-direitos-autorais-por-que-sao-importantes-para-os-projetos-de-software/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/06/24-06-Licencas-e-Direitos-autorais-Blog.png,"Escrito por Igor Muzetti

Este artigo busca desmistificar o assunto, mostrando os principais aspectos a respeito de direitos autorais e licenças de software. Este é um artigo de suma importância para o TerraLAB que, desde 2002, 18 anos atrás, desenvolve e distribui gratuitamente sob a licença LGPL o simulador de nome TerraME (www.terrame.org). Este desenvolvido em parceria com o Instituto Nacional de Pesquisas Espaciais (INPE) permite ao Brasil entender melhor os processos de ocupação do solo na região da Amazônia Legal (luccme.ccst.inpe.br), estimar com precisão suas emissões anuais de gases de efeito estufa (inpe-em.ccst.inpe.br) e, apoiado sobre bases científicas, apoiar a definição de políticas públicas voltadas para estas questões. 

Qual a relação entre direito autoral e licença de distribuição de software? 

Direito Autoral é um ramo da propriedade intelectual que incide, dentre outras obras intelectuais, sobre expressões literais, e que proíbe que terceiros as copiem, alterem, redistribuam ou vendam sem a autorização do Titular do Direito dos ativos intangíveis criados. O termo copyright é comumente utilizado para expressar tal direito. 

Ao contrário do que algumas pessoas pensam, o copyright é um direito que acompanha o software durante a sua vida, desde que tais direitos não sejam revogados por lei ou abdicados pelos autores. Assim, mesmo no caso de um software livre ou open source, quando alguém reaproveitar um código, os nomes dos autores originais não podem ser removidos. Caso você seja autor de um software e receba  contribuição de terceiros, em princípio, você e os demais autores “primários” deverão ser citados em copyright, pois possuem mérito intelectual. No entanto, poderá haver exceções tratadas pela licença do software.

Licenças de Software definem o que alguém pode ou não fazer com um determinado software. Isso envolve o conceito de direitos e deveres dos usuários e, muitas vezes, do próprio autor. As licenças se aplicam ao software na sua forma binária e, caso esteja disponível, ao seu código fonte. Todo software, para ser útil, deve possuir uma licença. A licença deve ser clara nas suas permissões e proibições. Não é permitido fazer algo que não esteja expressamente declarado. 

Leis para proteção à propriedade intelectual 

A Lei do Direito Autoral, Lei Nº 9.610, de 19 de fevereiro de 1998, define, no seu art. 7º, quais as obras intelectuais protegidas por meio da criação de espírito. No inciso XII deste artigo são inseridos também os programas de computador, como obra protegida. O § 1º rege que o programa de computador é objeto de legislação específica. 

A proteção à propriedade intelectual dos programas de computador é estabelecida pela Lei Nº 9.609, de 19 de fevereiro de 1998. Esta lei é conhecida popularmente como Lei de Software. A Lei de Software e a Lei de Propriedade Industrial (LPI), Nº 9.279 de 14 de maio de 1996, oferecem diferentes modos de proteção. A proteção dada pela primeira abrange apenas as expressões contidas no código utilizado, não os procedimentos ou métodos. Estes podem ser protegidos pela LPI, considerada uma proteção mais abrangente. 

Vale ressaltar que a proteção aos direitos relativos ao programa de computador independe de registro. No entanto, registrar o programa no Instituto Nacional de Propriedade Industrial (INPI) garante maior segurança jurídica ao seu detentor, caso haja, por exemplo, demanda judicial para comprovar a autoria ou titularidade do programa. Além disso, a proteção não é territorial como no caso das patentes, de outra forma, sua abrangência é internacional, compreendendo todos os 176 países signatários da Convenção de Berna (1886). No Estado brasileiro, o registro de software é feito no INPI e os direitos do titular são assegurados internacionalmente. O processo de registro é feito totalmente on-line, sem burocracia e com decisão totalmente automatizada. Maiores informações sobre o processo de registro você encontra no Portal do INPI.

Para que servem as Licenças de Software e como funcionam? 

Um software sem licença não pode ser utilizado por alguém ou empacotado por uma distribuição. Com certeza, você já notou que softwares proprietários de grandes organizações exibem na tela, durante a instalação, a sua licença. Há também licenças de documentação. Similarmente às leis, licenças dependem de interpretação, podendo gerar litígios e ações jurídicas. 

No entanto, licenças não são leis e não estão acima delas. Licenças são acordos entre partes. Outro conhecimento importante é o conceito de trabalho derivado. Trabalhos derivados são trabalhos que são feitos a partir de um outro já existente, dando continuidade a ele ou aproveitando partes dele para um novo trabalho. 

Ainda há o conceito de licença viral. A viralização ocorre quando uma licença faz com que qualquer software ou trabalho derivado tenha que usar a mesma licença de origem (Exemplo: General Public License – GPL).

Existe também o domínio público. No domínio público, uma obra (software, música, livro) pode ser utilizada livremente, sem a permissão ou pagamento de direitos ao autor. Cada país possui uma legislação específica, muitas vezes diferentes para livros, softwares e indústria. Não há um padrão internacional. Enfim, existem várias condições que definem se uma obra estará em domínio público ou não. 

O termo copyleft é usado por alguns, incluindo a Free Software Foundation (FSF), para exigir que um software (ou outra obra) tenham todas as suas versões diretas ou derivadas livres. A palavra é um trocadilho para copyright, mas não suspende os direitos cobertos por este. Geralmente, o copyleft gera viralização de licença. Por exemplo, se você criar um trabalho derivado, este trabalho também tem que ser distribuído como GPL. Compilar ou usar componentes licenciados como GPL pode ser considerado como trabalho derivado. Combinar dois módulos em um software significa conectá-los de maneira que eles agora formam um único programa maior. Se qualquer das partes for coberta pela GPL, toda a combinação também precisa ser disponibilizada sob a GPL. Se você não puder ou não quiser fazer isso, então você não pode combiná-las. O princípio geral é: se você usar software livre, licenciado com copyleft, em parte ou todo, para criar outro software, e pretender distribuí-lo, mesmo gratuitamente, terá que fazer sob algum tipo de licença copyleft, disponibilizando o código fonte e os avisos das licenças.

Tipos de Licença de Software 

Existem várias classificações de licenças. As licenças proprietárias impõem condições que cerceiam liberdades de alguém que tenha contato com o software. Exemplos: licença do Microsoft Windows e do Dropbox. Não fornecer o código fonte já é um cerceamento de liberdade. Licenças livres permissivas dão ampla liberdade para se fazer o que quiser com o código-fonte e o binário por ele gerado. Pode-se até fechar o código e torná-lo comercial (Ex: Windows e Mac OS usaram código FreeBSD com licença BSD). Exemplos de permissivas: BSD de 2 ou 3 cláusulas, a MIT e a Public Domain. As licenças livres restritivas não permitem que determinadas operações ocorram, com o fim de manter a eterna liberdade do software e dos seus descendentes. Um exemplo de licença livre restritiva é a GPL, que requer que trabalhos derivados também sejam GPL. As licenças restritivas, muitas vezes são conhecidas como copyleft. Há a possibilidade de multilicenciamento (Ex: Um software sob GPL 2 e/ou BSD de 3 cláusulas tem que seguir as duas). Nesse caso não poderá haver conflitos entre as licenças envolvidas.

Algumas licenças conflitam entre si e não podem ser utilizadas juntas em um mesmo projeto. Exemplo: a GPL 2 não é compatível com a licença Apache 2.0. A GPL 3 é compatível com a licença Apache-2.0. A GPL 2 e GPL 3 não são compatíveis com a BSD de 4 cláusulas. As GPL 2 e GPL 3 são compatíveis com a BSD de 2 cláusulas e MIT.

Alguns softwares proprietários e de código fechado possuem versões gratuitas. Por exemplo, o Avast Free Antivirus, o AVG Gree Antivirus e o CCleaner Free. Porém está escrito na licenças destes softwares que nenhuma solução do cliente é fornecida ou licenciada para uso por: (i) pessoa física para fins comerciais; ou (ii) empresa, sociedade, entidade governamental, organização não governamental ou outra entidade sem fins lucrativos ou instituição educacional.

No mundo Free and Open Source Software (FOSS), geralmente, as licenças são aplicadas a cada versão do software em questão. O autor do software pode, na maioria dos casos, modificar o licenciamento de uma nova versão. Mas se for BSD ou MIT? Pode! E se for GPL? Para mudar o licenciamento, todos os autores envolvidos devem estar de acordo, pois há copyright. O assunto é complexo. Pode haver mais de uma interpretação para uma mesma licença. As licenças são estipuladas por versão de software e os autores podem fazer modificações

As mais comuns de licenças para Software de Código Aberto

A Figura abaixo apresenta o resultado de uma pesquisa da WhiteSource sobre as licenças de código aberto mais populares de 2019.

Figura : Principais licenças de código aberto em 2019

A licença do MIT permanece no topo de popularização de licenças para código-aberto desde 2018 com 27% de uso. Isso não é uma surpresa, pois tem sido uma tendência no GitHub desde 2015. Ben Balter, advogado, desenvolvedor de open source e gerente de produto sênior do GitHub, disse que os desenvolvedores escolhem a licença do MIT porque “Ela é curta e objetiva. Ela informa aos usuários a jusante o que eles não podem fazer, inclui um aviso de direitos autorais e se isenta de garantias implícitas. É claramente uma licença otimizada para desenvolvedores. Você não precisa de um diploma em direito para entendê-la, e a implementação é simples.” De acordo com o site choosealicense.com do GitHub, a licença do MIT “permite que as pessoas façam o que quiserem com o seu código, desde que elas lhe devolvam a atribuição e não se responsabilizem”. Há dois anos, o Facebook substituiu publicamente a controversa licença do React por uma licença do MIT. Outros projetos populares de código aberto licenciados pelo MIT são Angular.js, Rails e .NET Core.

Seguem as descrições dos mais comuns  tipos de licença:

GPL (GNU General Public License): Ela reforça as quatro liberdades, e impede que o código-fonte (ou suas derivações) sejam apoderados por terceiros, virando software proprietário. É a que garante realmente as quatro liberdades, porém, restringe o software derivado através do copyleft.
LGPL (GNU Lesser General Public License): Apesar de ser baseada na GPL, permite o uso do software, neste caso bibliotecas de código, com software proprietário, desde que seja mantido o aviso de copyright. “Copyleft fraco”.
BSD (Berkeley Software Distribution): É utilizada pelo Unix derivado da universidade de Berlkeley, e é muito permissiva, não impondo o princípio de copyleft.
Apache (Apache Software Foundation): É utilizada pelos produtos da ASF, e é permissiva, não impondo o princípio de copyleft.
Mozilla (Licença pública Mozilla): É utilizada por vários produtos Mozilla, como o navegador Firefox. Permite exceções ao copyleft. “Copyleft fraco”.
MIT: Criada pelo Instituto de Tecnologia de Massachusetts (MIT). Ela é uma licença permissiva utilizada tanto em software livre quanto em software proprietário.

Considerações Finais

Este artigo teve como objetivo esclarecer as questões relativas à propriedades intelectual e ao licenciamento de produtos de software. Enumeramos as leis que regem o tema, os discutimos principais tipos de licença em uso na atualidade e descrevemos brevemente o funcionamento de algumas licenças mais comuns. Desta maneira, esperamos ter lhe ajudado a fazer a melhor escolha para sua equipe e parceiros. E como será que funcionam as licenças e direitos para o modelo de computação nas nuvens como o SaaS (Software as a Service)? Este assunto será tema de um post futuro aqui no blog!

Referências

http://www.planalto.gov.br/ccivil_03/leis/L9609.htm
http://www.planalto.gov.br/ccivil_03/leis/L9610.htm
https://www.youtube.com/watch?v=iySaBWeQt3E&t=3689s
https://people.debian.org/~bap/dfsg-faq.html
https://www.copyrightlaws.com/what-is-the-public-domain/
https://www.conjur.com.br/1999-abr-14/protecao_legal_programas_computador
https://www.gnu.org/licenses/"
Como contribuir para um software livre ou de código aberto pode impulsionar sua carreira?,http://www2.decom.ufop.br/terralab/como-contribuir-para-um-software-livre-ou-de-codigo-aberto-pode-impulsionar-sua-carreira/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/06/10-06-Carreira-DEV-Blog-scaled.jpg,"Escrito por Igor Muzetti

No texto a seguir veremos que investir seu tempo em um software livre ou de código aberto pode alavancar sua carreira.

Contribua para projetos de software livre ou de código aberto e ganhe novas habilidades.

Ganhe novas habilidades

Adquirir habilidades técnicas e pessoais (hard e soft skills), obter oportunidades profissionais e concluir seu curso, podem ser considerados desejos de uma grande parte dos estudantes em computação durante seus cursos de graduação e pós. Alguns alunos conseguem experiências de pesquisa e/ou de desenvolvimento de software participando de projetos de pesquisa, laboratórios, projetos de extensão, empresas júniores, estágios, entre outras oportunidades que as Universidades propiciam. Outros alunos podem ter mais dificuldade em obter oportunidades como essas. Muitos fatores induzem essa dificuldade como: concorrência, localização geográfica dos cursos, estrutura das universidades, problemas sociais, entre outros. Contudo, é comum o mercado de trabalho exigir até dos recém-formados, experiências em diferentes tipos de tecnologias. Através de entrevistas e análise de currículos, organizações selecionam candidatos buscando perfis técnicos e pessoais que mais se adéquem com suas culturas e planos.

Neste contexto, projetos de software livre e código aberto podem ser considerados como um complemento e diferencial para sua próxima oportunidade profissional. O diploma é importante, mas ser capaz de demonstrar trabalhos realizados em paralelo ao período de graduação (ou pós) através da experiência de trabalhos em equipe e mostrar vontade de colaborar com a sociedade utilizando suas hard e soft skills, também é importante. Várias tecnologias de software livre e código aberto oferecem ensejos de experiência, como: comunidades de distribuições de sistemas operacionais, de bibliotecas, de ferramentas e de linguagens de programação. Contribuidores dessas comunidades normalmente apresentam e desenvolvem diversas habilidades técnicas e pessoais e podem se destacar aos olhos de futuros empregadores.

Formas de contribuir

Comunidades de software livre e código aberto sempre querem contribuidores. Qualquer pessoa pode contribuir para diversos projetos desta natureza, independentemente do seu nível de conhecimento. É comum existirem contribuições anônimas ou registradas nas bases de dados dos projetos. Existem diferentes maneiras de contribuição nesses projetos, são elas:

Empacotamento de software. Um pacote é uma coleção de arquivos que permite aplicativos ou bibliotecas serem distribuídos através do sistema de gerenciamento de pacotes. O objetivo de criar pacotes é permitir a automação da instalação, atualização, configuração e remoção de programas para os projetos de forma consistente e precisa. Um pacote normalmente consiste em um componente fonte e um ou mais componentes binários.
Instale e use o sistema operacional ou ferramenta. Ao instalar e usar, você terá testado o instalador, aumentará a base de usuários e divulgação, além de ficar familiarizado com os sistemas.
Reportar falhas. Ao encontrar uma falha nos sistemas, envie um bug report (relatório de erros) para que os Desenvolvedores fiquem sabendo e possam corrigi-lo. É possível acompanhar o ciclo das falhas reportadas.
Enviar correções. Se você sabe como resolver um bug, você pode enviar uma requisição de conserto. Os desenvolvedores irão analisar e se tudo estiver correto, eles aplicarão a sua solução.
Documentação. Você pode produzir documentação escrevendo manuais, tutoriais, HOWTOs, FAQs. Diferentes níveis de documentos como iniciante, intermediário ou avançado. Publicação escrita ou em vídeo.
Suporte a outros usuários. Ajude a tirar dúvidas de outros usuários. Existem muitos locais que você pode participar como listas de e-mails, canais no IRC, fóruns e grupos do Facebook, Telegram.
Divulgação. Ajudar a divulgar os sistemas em escolas, universidades, congressos, encontros, redes sociais, etc. Comprar produtos das lojas oficiais que ajudam na manutenção dos projetos.
Publicidade. Os projetos possuem um time de publicidade que elabora textos de notícias e mantém alguns sites e perfis de redes sociais. Você pode contribuir com o time de publicidade ajudando a escrever as notícias ou a revisar os textos. 
Organização de eventos. Esses eventos podem ser desde desde um encontro em um bar ou restaurante até um evento com palestras e oficinas relacionadas aos projetos. O importante é promover encontros da comunidade local para celebrar o projeto.
Produção de material gráfico. Você pode produzir materiais gráficos e disponibilizá-los para que outras pessoas utilizem livremente.
Tradução. Traduzir páginas dos sites, instaladores, descrição de pacotes, documentação, notícias, alertas de segurança, entre outros. Tornar os sistemas usáveis por mais pessoas também tem o objetivo de atrair mais contribuidores para o projeto. Quando você traduz do Inglês para o Português por exemplo algum artefato, acredite, essa é uma excelente maneira de melhorar seus idiomas.

Participar de projetos de software livre e código aberto possibilita interagir com um público grande, sem exigências de renda. Sistemas oriundos desses projetos oferecem o requisito não-funcional da portabilidade. Este requisito trata da capacidade de trocar informações ou executar o software entre diferentes plataformas. Exemplo: Até poucos anos, o IRPF só podia ser declarado em computadores que rodavam Microsoft Windows. Hoje, pode ser declarado em quase todos os tipos de equipamento.  

“Software Livre” e “Software de Código Aberto” são coisas diferentes

Vamos apresentar então brevemente a história de alguns elementos essenciais para entendermos melhor os conceitos de software livre e código aberto (open source). Em 1965 Ken Thompson e Dennis Ritcie (Bell labs, MIT, At&T) começaram a desenvolver o sistema operacional Multics (algo como múltiplos). Mas a junção dessas empresas não deu muito certo. Cada uma possuía interesses específicos e contraditórios. Em 1969 começaram a escrever um outro sistema operacional chamado Unics e depois renomeado para Unix. Escreveram usando a linguagem Assembly. Em 1973 reescreveram o Unix em linguagem C. Ken Thompson criou a linguagem B, que antecedeu a linguagem C. Para se ter uma ideia do quanto atual ele permanece, ele é um dos criadores da linguagem Go.

GNU é um sistema operacional tipo Unix criado por Richard Stallman, cujo objetivo desde sua concepção é oferecer um sistema operacional completo e totalmente composto por software livre. O nome “GNU” é um acrônimo recursivo para “GNU’s Not Unix!” (em português, é traduzido como “GNU Não é Unix!”). “GNU” é pronunciado como “menu”, com “g” em vez de “me”, e como o animal de origem africana “gnu”. O desenvolvimento do GNU, que começou em Janeiro de 1984, é conhecido como Projeto GNU. Muitos dos programas em GNU são lançados sob responsabilidade do Projeto GNU (os pacotes GNU). Stallman também criou um mecanismo legal de garantia para que todos pudessem desfrutar dos direitos de copiar, redistribuir e modificar software, o que deu origem a General Public License (GPL). E para institucionalizar o Projeto GNU, Stallman fundou a Free Software Fundation (FSF). O software em um sistema semelhante ao Unix que aloca recursos da máquina e conversa com o hardware é chamado “kernel”. Linus Torvalds em 1991 começou o desenvolvimento do kernel Linux. GNU é tipicamente usado com um kernel Linux. Essa combinação é o Sistema Operacional GNU/Linux. 

Software livre (free software) e código aberto (open source) são dois termos que geram muita confusão. Software livre é um movimento social, criado pela FSF, que visa a manutenção de quatro liberdades básicas de software para as pessoas. Trata-se de um filosofia. Um software é dito livre se os usuários possuem as quatro liberdades essenciais:

A liberdade de executar o programa, para qualquer propósito (liberdade 0).
A liberdade de estudar como o programa funciona, e adaptá-lo às suas necessidades (liberdade 1). Para tanto, acesso ao código-fonte é um pré-requisito.
A liberdade de redistribuir cópias de modo que você possa ajudar ao próximo (liberdade 2).
A liberdade de distribuir cópias de suas versões modificadas a outros (liberdade 3). Desta forma, você pode dar a toda comunidade a chance de beneficiar de suas mudanças. Para tanto, acesso ao código-fonte é um pré-requisito.

Assim sendo, software livre é uma questão de liberdade, não de preço. Para entender o conceito, pense em “liberdade de expressão”, não em “cerveja grátis”.

Open source é uma metodologia de desenvolvimento, criada pela Open Source Initiative (OSI), que respeita os mesmos princípios do software livre. Trata-se de uma visão mais técnica do que filosófica. De uma maneira geral, os dois termos descrevem quase a mesma categoria de software, porém eles referem-se a visões baseadas em valores fundamentalmente diferentes. Mas a visão da FSF e da OSI são diferentes em relação a algumas licenças. Assim sendo, há divergências. Muitas vezes trata-se de interpretação de texto. O conceito de open source veio diretamente da Debian Free Sofwtare Guidelines (DFSG). Open source são programas cujo código fonte é disponibilizado pelo Desenvolvedor. Mas isto não quer dizer que sejam considerados como software livre. Existe o termo FOSS: Free and Open Source Software. Um programa open source pode não ser FOSS. Um programa FOSS pode não ser software livre.  Em suma, para ser software livre, as quatro liberdades essenciais devem ser possíveis. 

Freeware é um software disponibilizado para uso, sem a exigência de pagamento de licença de uso, ou com licença opcional. Não significa que o código fonte estará disponível, e, se estiver, não significa que você tenha as liberdades de alterá-lo e redistribuí-lo. Licenciamento de software sempre foi um tema polêmico ou nebuloso para muitos. A licença de uso é quem diz se é um software livre ou não. Software livre, geralmente, usa uma licença do tipo CopyLeft. Este tipo é uma maneira de usar a legislação de proteção dos direitos autorais com o objetivo de retirar barreiras à utilização, difusão e modificação de uma obra criativa devido à aplicação clássica das normas de propriedade intelectual, exigindo que as mesmas liberdades sejam preservadas em versões modificadas. 

Software para o bem de todos

O TerraLAB desde sua concepção utiliza ideias, conceitos e soluções amplamente distribuídas pela comunidade de software livre. Geotecnologias como o sistema de informações geográficas Spring, o visualizador de dados espaciais TerraView, a biblioteca C ++ GIS de classes e funções TerraLib  são exemplos brasileiros, todos desenvolvidos pelo Instituto Nacional de Pesquisas Espaciais (INPE), o Spring com mais de quatro décadas de existência.

Há 18 anos, desde 2002, o TerraLAB em parceria com o INPE desenvolve e distribui o software livre e de código aberto chamado TerraME – Terra Modeling Environment (terrame.org).

Atualmente, o TerraME serve como base tecnológica para projetos nacionais desenvolvidos pelo Centro de Ciência do Sistema Terrestre (CCST) do INPE. O projeto LuccME (luccme.ccst.inpe.br) busca entender os processos de ocupação do solo na região da Amazônia Legal e no Cerrado brasileiro. O projeto INPE-EM (inpe-em.ccst.inpe.br) permite ao Brasil estimar sua emissões anuais de gases de efeito estufa devido às queimadas.

Para seu funcionamento, o TerraME estende a linguagem de programação LUA, um dos mais bem sucedidos projetos de software livre brasileiro, desenvolvido pelo Pontifícia Universidade Católica (PUC-RJ), para fornecer a pesquisadores uma linguagem de programação destinada à modelagem e à simulação das dinâmicas espaciais dos sistema sociais e naturais .

Além destes projetos de software livre, diversas ferramentas que fazem parte do pipeline de implantação DevOps do TerraLAB são softwares livres e de código aberto. Este assunto inclusive será abordado com maiores detalhes em um post futuro aqui no blog!

Referências

https://www.fsf.org/
https://opensource.com/article/17/11/open-source-or-free-software
https://www.gnu.org/
https://www.debian.org/social_contract.pt.html#guidelines
Carneiro, T.G.S., Lima, T.F., Faria, S.D, TerraLAB – Using Free Software for Earth System Research and Free Software Development, Workshop de Software Livre – WSL 2009, Fórum Internacional Software Livre, 2009, Porto Alegre, RS."
Integração contínua e a entrega contínua (CI/CD) no GitLAB – O caso de um Backend NodeJS,http://www2.decom.ufop.br/terralab/integracao-continua-e-a-entrega-continua-ci-cd-no-gitlab-o-caso-de-um-backend-nodejs/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/06/10-06-Integração-Contnua-e-Entrega-Contínua-Blog.png,"Escrito por Guilherme Carolino e Arilton Nunes

No TerraLAB, a ferramenta de versionamento  oferecida pelo GitLab é essencial e desempenha um papel muito importante na produção de software. Utilizamos seu versionamento de código e também fazemos uso intensivo da sua estrutura de issues, como foi tratado no último post. Nesse post, iremos mostrar outra ferramenta de grande importância do GitLab: o CI/CD. 

Este artigo busca ensinar como o TerraLAB utiliza o GitLAB para realizar a integração contínua e a entrega contínua de seus produtos de software. Trata-se do primeiro artigo da série CI/CD, um assunto extenso e rico em detalhes, que somente muita experimentação e pesquisa nos permitiu tratar. O artigo oferece uma visão geral dos conceitos e ferramentas utilizado no pipeline CI/CD, depois, ilustra seu uso a partir de um estudo de caso que ensina a realizar o CI/CD de um backend em NodeJS.

O CI/CD se define em integração contínua e entrega contínua. Com a integração contínua, é mais fácil e ágil testar uma nova feature em um ambiente semelhante ao de produção, e só fazer o deploy depois de todos os testes passarem, fazendo com que a qualidade e a velocidade de entrega de softwares seja maior no ambiente. 

Pipeline de desenvolvimento com CI\CD

Os testes regressivos garantem uma maior qualidade no produto e uma certeza maior de que novos bugs ou falhas não estão sendo inseridos na versão principal do software.

O “caminho” que o software vai passar ao longo do CI/CD é chamado pipeline e esse pipeline precisa ser executado em algum lugar. É aí que entra o conceito de Runner. Um Runner é uma máquina onde o CI/CD vai ser rodado. Ele pode estar localizado em alguma nuvem, como AWS, algum servidor ou até mesmo local no seu computador. 

Um pipeline de integração contínua pode ser feito de diversas formas, dependendo muito das ferramentas que o projeto necessita. Nesse caso, trabalharemos utilizando o Docker. Não importando onde você for rodar o Runner, nesse mesmo ambiente é necessário ter o docker instalado.

No link abaixo é possível baixar e instalar o docker para o seu sistema operacional:

https://www.docker.com/products/docker-desktop

A instalação é bem simples e não vai ser abordada nesse post. Não é necessário criar nenhum docker manualmente, o próprio Runner se encarrega disso. 

Um tutorial completo de instalação pode ser encontrado no site oficial:

https://docs.docker.com/engine/install/

É importante notar que, enquanto é possível utilizar um docker Linux em cima de um sistema operacional Windows, algumas funcionalidades são limitadas, por exemplo, a virtualização que é necessária para rodar emuladores como o de Android dentro do docker. Caso o seu CI/CD precise de um emulador em algum momento, seus sistema operacional base deve ser Linux. 

Abaixo tem-se um exemplo de tela do Gitlab. 

Projeto Gitlab com arquivo “.gitlab-ci.yml”

O primeiro passo para começar um pipeline de integração contínua é colocar um arquivo em seu repositório do chamado “.gitlab-ci.yml”. Esse arquivo será responsável pelo controle de como o nosso pipeline vai funcionar. 

Código inicial do arquivo “.gitlab-ci.yml”

O pipeline do CI possui três estágios padrões, porém não obrigatórios. Estes se definem em:

Build
Test
Deploy

Na imagem acima podemos ver um exemplo de arquivo de integração contínua que apenas faz o estágio de build (Utilizamos um script sem funcionamento para ilustrar um “stage” da pipeline). Cada stage do pipeline deve ser explicitamente declarado abaixo da tag “stages:” exatamente como o build foi. 

É importante notar que o Gitlab reconhece a indentação como parte da linguagem, então qualquer erro de indentação vai fazer o seu arquivo não funcionar. 

Como dito antes, o GitLab  nos permite rodar nossos pipelines em Runners. Ao executar o pipeline, esses Runners recebem automaticamente o conteúdo do seu repositório e executam tarefas que você designou conforme seu arquivo YML. Por padrão os Runners utilizados são Runners do GitLab, que além de possuirem cotas de utilização possuem certas limitações pois não são ambientes totalmente controlados pelo usuário e pode ocorrer de nos colocarmos em situações onde não temos a permissão necessária para alguma execução. Para tornarmos nosso pipeline mais interessante e controlável,  configuramos nossos próprios Runners.

Runners padrão do Gitlab

A figura acima é um exemplo dos “Runners” oferecidos pelo GitLab (Podem ser encontrados nas configurações do seu projeto,na camada escrita CI/CD  e posteriormente em Runners).

Supondo que já temos o Docker instalado em nossa máquina vamos registrar nosso Runner. Existem diferentes formas diferentes de configurar nosso Runner. Para esse CI/CD, o Runner ficará no próprio servidor e utilizará o Docker para executar as tarefas.

Para configurarmos o Runner temos dois passos, o primeiro é baixar o GitLab Runner e instalá-lo em seu computador e o segundo passo é pegar o token disponível no site para atrelar o Runner ao seu projeto.

Podemos conferir o passo a passo para instalar o Runner de acordo com seu sistema operacional nesse link: https://docs.gitlab.com/Runner/install/. Para rodar o Runner, você deve ter o Git instalado e configurado no seu computador.

Após a instalação do Runner, é necessário que seja feito o registro do mesmo. Alguns dados sobre o seu projeto serão requisitados durante o processo. Eles podem ser encontrados na aba CI/CD do repositório, em “Set up a specific Runner manually”. 

Caso utilize um sistema operacional diferente do Ubuntu para o qual peguei de exemplo siga o passo a passo da documentação do GitLab https://docs.gitlab.com/Runner/register/ que não enfrentará problemas. O registro é extremamente semelhante em todos os sistemas operacionais.

Configurando um Runner personalizado

Começaremos dando o comando para registrar o Runner: 

sudo gitlab-Runner register 

logo após executarmos esse comando é pedido uma URL do GitLab e entraremos com a padrão:

https://gitlab.com

Logo depois de colocarmos a URL ele irá pedir o token do seu projeto. É usando o token que ele vai vincular o Runner criado ao projeto de origem. O token pode ser encontrado na aba CI/CD do seu projeto, como demonstrado na imagem mais acima.

Após isso,é pedido uma descrição para o seu “Runner”, e fica de forma arbitrária o padrão de descrição a se seguir. É uma boa prática colocar uma descrição que detalhe um pouco sobre o funcionamento deste Runner. Logo depois uma Label é pedida. É importante colocar uma Label pois na hora de criar seu arquivo YML é utilizando as tags que você vai escolher em qual Runner o pipeline vai ser executado. 

Exemplos de tags: “Runner-produção”, ”Runner-homologação”, ”Runner-teste”.

Após a label, nos é perguntado o executor do Runner, e colocaremos “Docker”, seguindo o modelo abaixo:

Please enter the executor: ssh, docker+machine, docker-ssh+machine, kubernetes, docker, parallels, virtualbox, docker-ssh, shell:

docker

Como escolhemos o Docker como executor, seremos perguntados sobre uma imagem padrão para usarmos em nossos projetos, esta parte varia muito de acordo com o seu projeto e das ferramentas dele, dependendo da linguagem, banco de dados e suas dependências. Como faremos para uma aplicação em Node.Js utilizaremos a imagem “node:latest”. Essa imagem garante que possamos utilizar no docker funções atribuídas a última versão do Node.Js. 

Acabando o registro, basta inicializar o Runner com o comando:

.\gitlab-runner.exe start

Pronto , nosso Runner está atrelado a nosso projeto e vai aparecer como ativo na página de Runners.

Página do Gitlab mostrando os runners ativados para o projeto (e os runners compartilhados disponíveis, à direita)

Ficaremos com o seguinte resultado no site do GitLab (nesse caso existem dois Runners rodando), não se preocupe se a “circunferência verde” demorar um pouco para aparecer, demora um certo tempo até o GitLab reconhecer que o Runner está online.

Agora que instalamos tudo que precisávamos para nosso CI/CD acontecer vamos a parte mais importante que é a configuração de nosso arquivo YML. O arquivo YML deve ser feito de forma adaptada para nosso projeto, então seus serviços devem estar alinhadas com as dependências do projeto.

CI/CD para o Backend de um projeto Node.Js

Arquivo “.yml” do nosso projeto, definindo os estágios de Build e Teste

Nesse arquivo YML acima está sendo executado dois estágios, o estágio de build e o de teste. Colocamos no início de nosso arquivo a imagem node:10 como a imagem a ser utilizada no Runner, e utilizamos outra palavra reservada “stages”, que define quais etapas serão executadas em seu arquivo. Utilizamos na linha 15 outra palavra reservada “cache” que serve para o compartilhamento de arquivos entre estágios, porque a cada estágio que é executado o Runner não salva por padrão o resultado deles, apenas os logs. Nesse caso, salvamos o “node_modules/”, que será importante tanto no estágio de build, quanto no estágio de teste. Vale ressaltar também que nosso script de teste utiliza de um banco de dados, e nesse caso é o MONGODB, então deixamos como serviço na linha 3 uma imagem do mongodb.

No estágio de build (linha 20) , definidos o stage em que estamos. Escrevemos build na linha 20 por convenção, porém esse nome não está vinculado ao estágio realizado. Poderíamos, por exemplo, chamar de build:android.  

Na linha 22 descrevemos as tags utilizadas. É aqui que especificamos em qual Runner o estágio co CI/CD vai rodar. No nosso caso, queremos rodar no “Runner-deploy-aws”, que é o nome que dei para meu Runner.

Logo após na linha 25, temos a palavra script. É aqui que o script de build é executado. No caso do nosso backend, é possível realizar o build simplesmente executando npm install. Caso tudo ocorra de maneira positiva, a próxima tarefa será executada, o teste. 

No teste também definimos nosso stage, nossa tag para selecionarmos o Runner e também utilizamos outra palavra reservada chamada artifacts, que serve para algo similar ao cache, porém ela salva o que você seleciona e deixa disponível para download depois que a tarefa é concluída. Por último o script de teste é chamado. Nosso teste é feito em Jest e pode ser executado com o comando “npm test”. 

A parte dos script é extremamente semelhante a execução local do backend, com exceção da necessidade de salvar em cache algumas coisas. 

Desta forma concluímos um pipeline de Ci, que incluia duas tarefas uma de build e outra de test. Os estágios do pipeline podem ser vizualizados em tempo real quando acessamos essa aba e podemos vizualizar os logs de tarefa por tarefa, facilitando assim a depuração de erros e soluções para eles.

Status dos pipeline do projeto
Terminal de execução exibindo o Log de um estágio do projeto

Exemplo de LOG do um stage de teste do script.

O ultimo stage é o de deploy. Esse estágio pode ser estruturado de diversas maneiras diferentes e depende muito da sua aplicação e onde ela vai ser hospedada. Essas condições irão modelar o arquivo de diferentes maneiras. No exemplo abaixo a aplicação está hospedada na nuvem da AWS e iremos acessar a instância via SSH. Quando realizada a conexão iremos rodar um Shell Script para colocarmos nossa aplicação no ar.

Definindo o estágio de Deploy

Nesse caso estamos utilizando o mesmo Runner que é o “Runner-deploy-aws” para fazer o deploy. Como já citado anteriormente existem diversas formas de executar o deploy e uma delas é deixando um Runner específico apenas para isso. O script para deploy dessa aplicação começa fazendo uma verificação do openssh-client e verifica se o Runner já possui ele instalado, se não ele roda um comando de update e vai instalar o openssh-client. Depois ele da permissão para o arquivo disableHostKeyChecking.sh rodar e por último chama um outro arquivo chamado deploy.sh.

Ambos os arquivos disableHostKeyChecking.sh e deploy.sh estão na raiz do nosso Git. 

Script Bash de configuração para o deploy

O arquivo deploy.sh configura nossa private_key para acessar a aws através de uma variável global no Git. Em seguida ele roda o arquivo ./disableHostKeyChecking.sh e então faz a conexão com a AWS passando para execução o script no arquivo updateAndRestar.sh

As variáveis de ambiente $PRIVATE_KEY e $DEPLOY_SERVERS foram criadas no Gitlab na página abaixo. Deploy_servers contem o endereço da nossa máquina na AWS, no caso, toda a parte depois do @. Private_key contém o conteúdo do nosso arquivo “.pem” de acesso. Essas variáveis são secretas e não ficam expostas durante a execução do pipeline. 

definindo as variáveis de ambiente no Gitlab

Por último, abaixo está nosso arquivo updateAndRestart.sh. Inicialmente ele verifica se já existe uma aplicação na porta onde nossa aplicação ficará ouvindo, caso positivo, ele pega o PID e mata a aplicação. Logo depois ele atualiza o diretório através do git pull e roda os scripts npm install e npm start que são responsáveis por colocar a aplicação no ar.

Script Bash para atualização e reinicialização do projeto

Com isso, o CI/CD está quase completo. Para garantir a segurança do nosso projeto, realizamos outra configuração no Git. 

Configurações do Git

Em Settings > General, marcamos dentro de “Merge Requests” a opção “Pipeline must succeed”. Isso garante que um merge request só pode ser aprovado se o CI/CD for bem sucedido. 

Além disso, fazemos uma última alteração no nosso arquivo YML. Quando alguém cria uma branch a partir da master que contém o YML, o arquivo é levado junto. Você não quer que sua aplicação fique fazendo deploy toda vez que um commit for feito. 

Arquivo “.yml” final

Na linha 15 criamos o comportamento possível para os estágios do pipeline. Nas linhas 22 e 30 especificamos que aqueles estágios serão executados sempre que forem executados a partir das especificações da linha 15. Na linha 41 acrescentamos a palavra reservada “only” indicando que aquele estágio só deve ser executado caso ocorra um merge request.

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software."
"Estudo do INPE com apoio do TerraLAB/UFOP orienta contenção da COVID-19 nos municípios do Vale do Paraíba e Litoral Norte, SP",http://www2.decom.ufop.br/terralab/estudo-do-inpe-com-apoio-do-terralab-ufop-orienta-contencao-da-covid-19-nos-municipios-do-vale-do-paraiba-e-litoral-norte-sp/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/06/INPE_Blog.png,"Escrito por Tiago Carneiro

No dia 01 de maio de 2020, um grupo de cientistas de diferentes instituições de pesquisa e ensino publicaram uma Nota Técnica sobre a situação da COVID-19 na RMVPLN – Região Metropolitana do Vale do Paraíba e Litoral Norte. Esta região integra 39 municípios entre as duas regiões metropolitanas mais importantes do país: São Paulo e Rio de Janeiro .  O mapa a seguir mostra a região e sua estrutura de conectividade.

A nota técnica liderada pelo Laboratório de Investigação de Sistemas Socioambientais (Liss) da Coordenação Geral de Observação da Terra (CGOBT), do Instituto Nacional de Pesquisas Espaciais (INPE), é assinada conjuntamente por pesquisadores do laboratório TerraLAB da UFOP, do Grupo de Métodos Analíticos em Vigilância Epidemiológica (MAVE) que reúne pesquisadores da FIOCRUZ e Escola de Matemática Aplicada da Fundação Getúlio Vargas – RJ, e do Centro Nacional de Monitoramento e Alerta de Desastres Naturais (CEMADEN). 

O TerraLAB tem importante papel na definição e Implementação de modelos computacionais espacialmente-explícitos que permitem simular a Dinâmica Regional da COVID-19 para analisar cenários onde são avaliadas diferentes estratégias de contenção da doença. Este modelos partem da hipótese de que, conhecendo-se a mobilidade regional, é possível simular e entender as possibilidades de espalhamento da epidemia no espaço metropolitano, além de entender o papel relativo de cada município na rede regional, em função de seu grau de conectividade. 

Neste sentido, a nota técnica destaca a  situação de 4 municípios. O município de Areias por apresentar a condição de vulnerabilidade mais alta entre os 39 que formam a RMVPLN. Os municípios de Ubatuba e São Luiz do Paraitinga por apresentarem condição de vulnerabilidade intermediária. E, finalmente, Taubaté por, apesar de apresentar condição de vulnerabilidade baixa, ser altamente conectado à municípios que apresentam epidemia instalada e com quantidades significativas de casos e óbitos confirmados, como São José dos Campos, São Paulo e Campinas.

 Assim, os pesquisadores explicam porque nenhum município sozinho e isoladamente pode ser efetivo no enfrentamento  à  pandemia de COVID-19 e concluem a nota técnica recomendando a imediata criação de um Comitê Técnico-Científico para o Enfrentamento da COVID-19 na RMVPLN.  Eles reiteram que esse comitê deve ser independente, formado por profissionais de notório saber no campo da saúde pública e do planejamento territorial. O comitê deve ter como foco a coordenação das ações para que se possa produzir as melhores informações, baseadas em evidências, para auxiliar a tomada de decisões em situação de incertezas acumuladas."
Entendendo o Funcionamento do Issue Boards no GitLab,http://www2.decom.ufop.br/terralab/entendendo-o-funcionamento-do-issue-boards-no-gitlab/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/05/IssueBoard-Blog-I.png,"Escrito por Arilton Junior e Tiago Carneiro

No TerraLAB, nós utilizamos outros artefatos na nossa linha de produção que são geridos de forma integrada através do serviço de versionamento de software oferecido pelo GitLAB. Além de utilizamos sua capacidade de controle de versão e armazenamento compartilhado de código fonte, fazemos uso intensivo da sua estrutura de issues (“demanda”, numa tradução livre para o Português) organizadas em boards (“painéis”, no Português).

Ao abrir o grupo do projeto, essa é a primeira tela que o usuário tem contato.

Nela podemos visualizar todos os subgrupos que fazem parte de um projeto específico. No exemplo acima, o projeto é o Geospotted. Os subgrupos utilizados no lab frequentemente são divididos em API, Frontend Mobile e Frontend Web.

Ao abrir um subgrupo, chega-se à tela abaixo:

Nesse momento, toda a barra lateral esquerda faz referência somente ao subprojeto ativo. No caso, GeoSpottedAndroid. Nosso interesse aqui está nas Issues. Passando o mouse sobre o menu Issues, é possível acessar um submenu com várias opções, dentre elas Boards. É em torno das Boards que nosso processo de produção se estrutura.

A figura acima mostra o Issue Board para o subprojeto GeoSpottedReactWeb do projeto Geospotted. No início, qualquer nova issue é inserida no board mais à equerda e, ao longo do projeto, vai avançando até chegar ao board mais à direita, indicando que a issue foi realizada com sucesso, aprovada pelo cliente do projeto e pela equipe por meio de testes regressivos automatizados.  No TerraLAB, os board têm a seguinte sequência não-natural! Rsrsrs

Open > Backlog > Sprint Backlog > Doing > Waiting Acceptance > Done > Automating Tests > Deploy > Waiting Validation (externally) > Closed

Você pode estar se perguntando porque “não-natural”!  É assim porque, naturalmente, as soluções desenvolvidas para a issues falham e, então, as issues que falharam retrocedem nos boards, lamentavelmente!

Para ilustrar, imagine que qualquer ideia nova é inserida como uma issue no board “open”. Ela ainda é só uma ideia, não faz parte do trabalho a ser realizado. Somente o Product Owner pode movê-la para o board “backlog” indicando que a ideia inicial já foi reescrita na forma de uma issue de acordo com um formato padronizado (em conteúdo e formato).  Agora sim, a issue é parte do trabalho que será feito para cumprir a missão do projeto. Então, quando um Product Owner move uma issue para o “Sprint Backlog” ele está, na verdade, solicitando que essa issue seja resolvida na próxima sprint. Quando qualquer membro da equipe de projeto fica ocioso, ele vem até esse board, move uma das issues para o board “Doing” e começa a resolvê-la. Quando ele acha que a resolveu, ele a move para o próximo board “Waiting Acceptance” e assim por diante. A figura abaixo mostra o significado de cada Board:

Observação de funcionamento: É possível arrastar uma Issue de um board para outra utilizando o mouse.

Agora, reparem que existem balões coloridos nas issues de um board. Esses balões são labels. Labels são marcações que a equipe de projeto adiciona às issues para indicar a sua natureza.  Da natureza de uma issue dependem a prioridade e importância que a ela é dada e o rigor técnico que é exigido na sua especificação (em forma e conteúdo). A figura abaixo apresenta as labels atualmente utilizados no TerraLAB.

A seguir, são descritos com detalhes cada um dos boards que estruturam o processo de desenvolvimento de software do TerraLAB:

 – Open: Aqui encontramos atividades abertas que não entram ou ainda não entraram no backlog, mas surgiram nas discussões da equipe de projeto com clientes e parceiros. Por exemplo: partes do projeto que os desenvolvedores não sabem como realizar e será necessário mais pesquisas entram aqui com ao label “research”. Sugestões de melhoria no projeto são criadas com a label “enhancement”, etc.

– Backlog: Aqui ficam todas as funcionalidades do backlog do produto. O backlog que foi levantado no “Documento de Especificação de Software” é inserido no git na forma de issues. Cada funcionalidade se torna uma issue no board “Backlog”. Outros itens eventualmente acabam parando no board “Backlog”, tais como sugestões de “enhancement” aprovadas pelo Product Owner. Os “bugs” relatados são tratados com máxima prioridade e, quando relatados, entram direto no board “Sprint Backlog”!

– Sprint Backlog: Aqui ficam as issues que serão trabalhadas na sprint atual ou, no máximo, na próxima. Antes de arrastar uma issue do “Backlog” para o “Sprint Backlog”, o Product Owner do projeto vai completar a especificação da issue com, pelo menos, uma “História de Usuário”. No GitLAB, é possível visualizar uma janela contendo mais detalhes de uma issue. Muitos destes detalhes podem ser editados. Ao especificar “Histórias de Usuários” e o “Cenário de Testes” como na figura abaixo, o Product Owner e o Tester Engineer, respectivamente, acrescentam os labels “Storie” e “Scenario” indicando que estes artefatos foram elaborados, revisados e aprovados.

No canto superior desta Janela, é possível atribuir uma issue a um ou mais membros da equipe responsabilizando-os por sua resolução. No TerraLAB, nos esforçamos para atribuir uma issue a apenas um profissional. Assim, em geral, após a criação de uma issue, após a revisão de sua solução, é natural de uma issue seja atribuída a um profissional específico. Ao longo das sprints, as issues são atribuídas a diferentes membros da equipe, sejam para solicitar que eles resolvam parte da issue ou para solicitar que eles revisem a solução desenvolvida por outro membro.

– Doing: São as issues que o desenvolvedor está trabalhando no exato momento.

– Waiting Acceptance: São issues que foram implementadas pelo desenvolvedor e estão passando por testes feitos pelos testers.

– Done: São issues que passaram pelos testes (previamente validados pelo Product Owner) e cuja implementação é considerada concluída.

– Automating Tests e Deploy: São boards especiais referentes as partes de CI – Continuous Integration e CD – Continuous Deployment do projeto. Quando uma issue é movida para“Done”, geralmente, isto significa que uma nova release do projeto pode ser liberada incluindo esta nova funcionalidade. Para isso, o projeto vai passar por uma série de testes automatizados, o CI, e durante esse tempo as issues que estão entrando nesse novo release irão ficar no board “Automating Tests”. Ao passar por todos os testes de integração, caso as soluções destas issues não injetem falhas no produto, estas soluções são transferidas para o board “Deploy” onde ocorre o CD.

– Waiting Validation (externally): Uma vez realizado o CI e o CD, um novo release do produto foi gerado no ambiente de teste e é, então, submetido à validação dos clientes e/ou usuários reais do produto, isto é, pessoas externas equipe de desenvolvimento e que, muitas vezes, são os encomendantes do projeto. Caso nenhuma falha ou melhoria seja relatada, essa issue finalmente é movida para o próximo board.

Done: Issues que passaram por todo processo de produção e foram validadas pelos clientes dos produtos e já podem ser implantadas no ambiente de produção dos clientes do produto.

Detalhes sobre a utilização do Issue Board

O Product Owner é o principal responsável por gerenciar o Issue Board. É ele quem define as “histórias de usuário” e é quem valida os “cenários de teste” especificados pelo Tester. Ele é também o principal responsável por coordenar as as atribuições de issues aos membros da equipe de um projeto, em consonância com seus perfis profissionais, ocupação e habilidades.

Sempre que um bug for encontrado, ele deve ser relatado imeditamente ao Product Owner que vai criar uma nova Issue com a label Bug.

Caso alguém (equipe ou cliente) tenha uma ideia de melhoria para o projeto, ele vai relatar ao Product Owner que irá criar uma Issue com a label “enhancement”.

Caso um membro da equipe tenha dificuldade ao implementar algo e precise pesquisar sobre o assunto, ele deve inserir uma nova issue com a label “research” no board “Open”. Ela será atribuída para um membro de equipe pelo Product Owner de acordo com a prioridade que este último estabelecer em comum acordo com quem relatou a necessidade de pesquisa. Enquanto uma pesquisa estiver em andamento, ela deverá permanecer com ela no board “Doing”. Assim, não é dificil imagina que um membro de equipe normalmente possui várias issues a ele atribuídas.

Existem outras visualizações para as issues além da “Board”.

Como a figura acima mostra, List apresenta todas as issues em formato de lista. Pode ser interessante trabalhar com a lista de issues quando é necessário ordenar as issues de forma específica, por data de criação por exemplo.

Também é possível visualizar as issues de um projeto inteiro. Nas figuras anteriores, estávamos visualizando as issues especificas de um subprojeto, porque é uma organização mais fácil de ser entendida e gerenciada. Contudo, quando necessário ver de forma geral todas as Issues, basta navegar para a raiz do projeto e utilizar novamente as opções disponíveis para visualização de issues.

O print acima mostra todas as Issues para o projeto GeoSpotted, reunindo issues dos subprojetos: API, Frontend Mobile e Frontend Web.

CRIANDO OS BOARDS DO SEU PROJETO

Criando as Labels

A primeira coisa a se fazer é abrir a raiz do seu projeto. A pasta que contém todos os subprojetos, como demonstrado abaixo. Em seguida, basta ir em issues e labels. Vamos criar as labels utilizadas.

Os projetos começam sem nenhuma label e todas devem ser criadas de acordo com a tabela de labels do TerraLAB. Essa tabela é encontrada aqui. Devem seguindo o padrão de cores e a descrição.

Criando os Boards

Após criarem as labels gerais para todo o projeto, entrem no subprojeto específico que querem criar os boards e acessem o issue board dele. Vocês verão uma tela igual a de baixo:

Caso já exista algum board, basta utilizar a opção de excluir e deixar igual a tela acima. A opção de excluir fica ao lado do título, no canto direito do board. Os boards “Open” e “Closed” não podem ser excluídos.

Conforme a figura abaixo, cliquem no botão “Add List” e escolham a label para gerar o board.

Os boards devem ser gerados na seguinte ordem:

Backlog > Sprint Backlog > Doing > Waiting Acceptance > Done > Automating Tests > Deploy > Waiting Validation (externally)

Após gerados, o Issue Board vai ficar assim:

Criando as Issues

A primeira coisa a ser feita depois de criar os boards é criar as issues. Todo o desenvolvimento começa no primeiro board, o Backlog. Aqui, pegaremos diretamente o backlog estabelecido no “Documento de Especificação de Software” e iremos passar tudo para o git.

Ao clicar em “Add Issues” como circulado na figura acima, a tela de issues vai se abrir e então clicamos em “New Issue”.

A tela abaixo vai se abrir. Em Title colocamos o título da issue. Em Description, uma descrição dela. Em Labels, escolhemos em qual coluna ela vai ser criada.

Todo o resto pode ficar da forma que está. Ao fim, basta clicar em “Submit Issue”.

Quando todas as funcionalidades do backlog estiverem sido acrescentadas como issues, o board de Backlog vai ficar preenchido, como na imagem abaixo:

Trabalhando o backlog

Como o projeto que vocês estão assumindo já estava em andamento, é necessário trabalhar esse backlog para refletir o estado atual do produto.

Primeiro, vocês devem acrescentar as histórias de usuário e cenários de teste para todas as issues que já tiverem esses dados especificados. Toda issue que já foi levada para o sprint backlog deve ter, no mínimo, uma história de usuário.

Quando preencher a issue com esses dados, caso os testes já tenham sido implementados e a issue passou nos testes, ela vai ser movida para “Waiting Acceptance”.

Se a issue que estiver aguardando aceitação for aprovada pelo cliente interno, ela vai ser movida para “Done”. Isso quer dizer que o cliente interno concorda com o teste implementado e confirma que tudo está certo.

Se nenhum teste estiver sido implementado ou a issue ainda não estiver passando nos testes, ela deve ser movida para a coluna “Doing”.

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software."
10 Apps Úteis no Home Office durante o Isolamento Social,http://www2.decom.ufop.br/terralab/10-apps-uteis-no-home-office-durante-o-isolamento-social/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/05/22-05-DEZ-APPS-HOME-OFFICE-Blog-min-scaled.jpg,"Escrito por Fabrício Pereira | Revisado por Camila Silva

Devido ao cenário atual, o home office deixou de ser uma alternativa e passou a ser a melhor opção. Uma vez que há a possibilidade da equipe de trabalho concentrar suas atividades sem se expor ao risco de um possível contágio pelo Coronavírus. Com o objetivo de potencializar a produtividade das tarefas em home office, listamos abaixo 10 ferramentas que auxiliam tanto no gerenciamento do trabalho quanto a manter o foco nas atividades.

Google Meet

Como destaque na categoria de aplicações de videoconferências, cita-se o Google Meet disponibilizado gratuitamente até o dia 1º de julho. A ferramenta se diferencia por proporcionar sessões com até 250 pessoas e  se integra ao ecossistema de aplicativos da Google. O usuário pode agendar suas reuniões com a Google Agenda e compartilhar arquivos do Google Drive de maneira mais nativa e simplificada.

Forest

O celular pode ser uma poderosa ferramenta no gerenciamento de tarefas e na execução de planos de trabalho. O problema passa a aparecer quando as notificações de redes sociais e outros aplicativos, roubam a atenção e como consequência ocupam o tempo que deveria ser investido em trabalho. O Forest foi pensando justamente para ser um aliado nessas horas. A proposta do app é trazer elementos inspirado em games com uma lógica que obriga o usuário a se concentrar na atividade que realmente importa e se manter longe de distrações e do aparelho eletrônico. Como isso seria possível? Dentro da aplicação o usuário cultiva plantas e quanto mais se distanciar do celular, mais as plantações irão crescer e lhe render pontos. Caso contrário, as plantas irão morrer pela falta de foco do usuário ou até mesmo o app faz com que o smartphone toque músicas constrangedoras em altos volumes.

Pomodoro

A técnica Pomodoro consiste em um método de estudos/trabalho famoso pela sua efetividade em proporcionar foco. Criada nos anos 1980, a estratégia consiste em separar as atividades em intervalos de 25 minutos com pausas de 5 minutos e outra longa de 20 a 30 minutos. Várias são as ferramentas que utilizam o Pomodoro, destacamos aqui o site TomatoTimer. Sua interface é simples e intuitiva, permite o usuário controlar o timer tanto atráves de botões na tela como por teclas de atalho, receber notificações mesmo enquanto navega em outras abas ou softwares além de administrar as pausas e resetar o relógio caso deseje.

Evernote

Essencial para o cotidiano do profissional que lida com muitas tarefas e ideias ao mesmo tempo, o Evernote funciona como um bloco de notas capaz de manter na nuvem tudo o que o usuário deseja anotar e salvar. Ele pode, através de tags, planejar seu dia de trabalho com antecedência, agendar lembretes com clientes, ter informações sobre eles e montar o briefing de seus pedidos além de compartilhar tudo com colegas de trabalho.

Notion

Com uma interface mais minimalista, o Notion, similarmente se configura como uma ótima solução para se organizar através de anotações. Sua particularidade centra-se na capacidade de transformar rapidamente uma simples nota de texto a uma página de um caderno digital. Dessa forma o usuário é livre para organizar suas ideias e tarefas em esquemas que se aproximam de um quadro de avisos ou uma grande mesa de trabalho.

Pocket

Se o que você deseja é simplesmente salvar links de páginas da WEB para ler em outra hora, o Pocket pode se tornar o seu mais novo app favorito. Além de armazenar links, o Pocket disponibiliza as páginas que você salvou para ler mesmo sem uma conexão de internet. Além dessa funcionalidade, segmenta suas preferências e lhe recomenda conteúdos de acordo com elas e possibilita “seguir” colegas de trabalho a fim de ter acesso ao que eles leem. Com isso, sua equipe de trabalho pode ficar em sintonia compartilhando das mesmas ideias e inspirações para realizar as tarefas com maior empenho e motivação.

Microsoft To Do

O aplicativo Microsoft To Do é uma eficiente ferramenta no gerenciamento de tarefas diárias, semanais e mensais. Com a função “Meu dia”, o usuário pode planejar antecipadamente seus afazeres diários de maneira a tornar seu dia mais produtivo e organizado ou se planejar a médio e longo prazo com a função “Planejado”. O uso da ferramenta não se limita ao planejamento de tarefas de trabalho já que é possível diferenciá-las das pessoais, por exemplo, por meio da criação e gestão de listas.

Trello

Sente falta de um quadro de avisos que confere dinamismo e mantém todo o time consciente de tarefas e prazos? Inspirado em metodologias ágeis de desenvolvimento, o Trello se sobressai ao permitir a criação de inúmeros quadros para um mesmo time. Dentro de cada quadro, o membro do time pode criar listas de tarefas, separá-las por etapa de desenvolvimento (se estão pendentes, em andamento ou concluídas), e determinar prazos e quem será o responsável por cada tarefa. 

Github

Sinônimo de gerenciamento de projetos e versões de códigos, o GitHub também funciona como uma plataforma de rede social para desenvolvedores. Nela, você pode trabalhar de modo colaborativo com seus colegas ao mesmo tempo que acompanha  o andamento de seus projetos através do versionamento de seus softwares ou até se inspirar em produções desenvolvidas por inúmeros usuários do mundo todo.

Calm

Foco no trabalho e emocional abalado não são aliados a uma boa produtividade. Especialmente durante a atual crise provocada pelo Coronavírus é essencial valorizar o bem estar e a saúde mental. Por isso, recomendamos o Calm para momentos de estresse a fim de proporcionar horas de relaxamento. A partir da aplicação. é possível selecionar tempos de dois a 30 minutos, conforme sua disponibilidade e objetivos: dormir, relaxar ou simplesmente curtir o momento. 

Conhece uma ferramenta que merecia estar listada acima? Compartilhe com a gente nos comentários. "
Processo Seletivo TerraLAB 2020.1: Encerramento,http://www2.decom.ufop.br/terralab/processo-trainee-terralab-2020-1-encerramento/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/05/13-05-Encerramento-Processo-Trainee-Blog.png,"Escrito por Fabrício Pereira | Revisado por Palloma Brito

Após 8 semanas de muito comprometimento e aprendizado, o Processo Seletivo TerraLAB 2020.1 se encerrou na semana passada. Mesmo com o cenário mundial em crise pelo Coronavírus, os 29 aprovados persistiram nos seus trabalhos através de home office com motivação de sobra.

Divididos em 5 equipes (squads), os trainees vivenciaram uma experiência semelhante ao mercado de trabalho, desenvolvendo 5 aplicativos WEB e Mobile ao todo. Por meio de desafios semanais, aprenderam a utilizar tecnologias como Angular JS, React e NodeJS através de cursos online e mentoria oferecidos por empresas de destaque no setor tecnológico da região como: Cachaça Gestor, Gerencianet, Stilingue e Usemobile. Conheça a seguir um pouco de como foi essa experiência e dos resultados:

Squad Amarelo

“O processo foi bem engrandecedor já que tive que aprender algumas tecnologias que ainda não havia tido contato onde consegui passar por quase todas as funções do squad (exceto Mobile), lidar com adversidades também foi bom para pensar em soluções e se utilizar do companheirismo do squad.” destaca Diego Henrique Marques Matos do squad amarelo. A equipe desenvolveu o app GeoLarica no qual o usuário pode avaliar estabelecimentos alimentícios da região, recomendando ou não os serviços. Confira algumas telas do projeto:

GeoLarica - Tela de Login
« ‹
 de 5
› »
Squad Branco

O squad branco desenvolveu o app WindSun com o qual o usuário pode conferir a localização das estações meteorológicas brasileiras. Além disso, são disponibilizados dados de cada localidade como a velocidade média do vento. Sobre o processo seletivo, Larissa Viana da Silva relata: “Entrei no processo sem entender direito do que se tratava. Já no workshop, tive a sensação de que valeria a pena. Ouvir todos os palestrantes, principalmente os ex alunos da UFOP contando a experiência deles, me motivou a seguir em frente e encarar o desafio. Foram semanas intensas, de muito trabalho e esforço, mas também de muito aprendizado. Tive contato com as ferramentas de desenvolvimento e pude compreender na prática como funciona a construção de uma aplicação WEB e Mobile. A equipe do TerraLab sempre esteve a disposição para tirar nossas dúvidas, nos auxiliando durante todo o processo seletivo. Assim, entendo como é fazer parte do time e que pertencer ao laboratório poderia me agregar tanto profissionalmente, quanto pessoalmente.”. Veja a seguir algumas telas da ferramenta:

WindSun - Tela de Início
« ‹
 de 2
› »
Squad Laranja

Responsável por desenvolver uma aplicação que auxilie a população local em meio a crise provocada pelo Coronavírus, o time laranja produziu o Covid Zone. O aplicativo permite ao usuário uma rápida avaliação de sua saúde via preenchimento de seus sintomas; dispõe de gráficos com os casos confirmados, recuperados e as mortes causados pela COVID-19; mostra ao usuário a localização dos casos confirmados dentro de sua região além de disponibilizar dicas de prevenção da doença. Sobre a experiência vivenciada junto ao seu grupo, Diego Henrique Marques Matos declara: “Esse squad em particular me deixou bastante animado com o projeto pois há muitos problemas que ainda não vimos sendo implementados e ter um viés social é animador.” Abaixo, as telas do app:

Covid Zone - Tela Inicial
« ‹
 de 6
› »
Squad Verde

Encarregados de desenvolver uma aplicação para a Orquestra Ouro Preto, o squad verde reuniu notícias e curiosidades sobre o grupo musical, sua agenda cultural, vídeos e a possibilidade de se entrar em contato com a orquestra. O usuário também pode adicionar os eventos da Orquestra a uma agenda própria dentro do sistema. Ailton Sávio Sacramento Júnior, membro do squad, nos conta como foi a sua experiência dentro do nosso processo seletivo: “Foi prazerosa, deu pra aprender e melhorar em muita coisa, tanto na programação de software em si quanto também (e acho que principalmente) nas famosas “soft skills”. Meu squad conseguiu desenvolver um software de alta qualidade na minha opinião, que possa fazer parte do meu portfólio e quem sabe um dia me ajudar em alguma entrevista de emprego ou algo do tipo.”. Logo abaixo, você confere as telas do aplicativo:

Orquestra Ouro Preto - Tela Inicial
« ‹
 de 5
› »
Squad Voluta

A empresa júnior do curso de Ciência da Computação, Voluta SD, também esteve presente no processo seletivo. Motivado em diminuir o crescente número de casos de violência contra mulheres durante o isolamento causado pelo Coronavírus, o time desenvolveu uma poderosa ferramenta. Dentro da aplicação a usuária pode registrar onde ocorreu a agressão e contatar os órgãos locais responsáveis pela sua segurança como a Ouvidoria Feminina da Universidade Federal de Ouro Preto (UFOP). Esses registros são disponibilizados em um mapa para outras usuárias da plataforma no intuito de instituir e fortalecer uma rede de proteção à saúde da mulher. Sobre a imersão promovida pelo TerraLAB durante essas semanas, Marcus Vinicius Souza Fernandes ressalta: “Está sendo muito benéfico adquirir este aprendizado para compartilhar com os demais membros da Voluta, para que assim, possamos adaptar alguns processos e trocar nosso framework. Todos os membros estão ganhando com isso, não só os que estão participando.”. Seguem as telas do projeto:

Aplicativo de Segurança da Mulher - Tela de Login
« ‹
 de 4
› »

Parabenizamos e agradecemos cada participante que fez parte do nosso time durante essas semanas. A troca de conhecimentos e experiências promovida foi enriquecedora para cada um de nós. Aos que não foram aprovados, desejamos sucesso em suas trajetórias. O TerraLAB terá imensa alegria em recebê-los nas próximas edições de nosso processo seletivo.

Você ficou curioso sobre um dos projetos? Não deixe de acompanhar nosso blog e redes sociais para saber mais sobre cada um!"
Como Funciona o Versionamento de Software e como criar Tags no Git,http://www2.decom.ufop.br/terralab/como-funciona-o-versionamento-de-software-e-como-criar-tags-no-git/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/05/06-05-Versionamento-e-Git-3.png,"Escrito por Koda Faria | Revisado por Fabrício Pereira

Quando estamos desenvolvendo um software grande ou uma API é preciso ter em mente que outras pessoas utilizarão o produto e isso explica o quão importante é manter compatibilidade de software. Entretanto, nem sempre conseguimos manter as novas versões com compatibilidade. Por isso é importante fazer o versionamento!

As palavras must, must not, should, should not e may fazem parte de um guia de boas práticas utilizado ao se falar de softwares e seus pré-requisitos, seja pessoalmente, em documentações ou pela internet:

MUST (DEVE): significa que algo é essencialmente necessário para a especificação ou projeto.
MUST NOT (NÃO DEVE): significa que algo é essencialmente proibido para a especificação ou projeto.
SHOULD (DEVERIA): significa que algo pode ser ignorado em situações específicas de uso, mas as consequências devem ser medidas e avaliadas antes de seguir um curso diferente do proposto.
SHOULD NOT (NÃO DEVERIA): significa que algo pode ser aceitável ou útil em situações específicas de uso, mas as consequências devem ser medidas e avaliadas antes de seguir um curso diferente do proposto.
MAY (TALVEZ): significa que um item é essencialmente opcional.
Regras para Uso da Semântica de Versionamento
Um software que utiliza a semântica de versionamento DEVE declarar uma API pública. Essa API pode ser declarada no próprio código ou existir em uma documentação. Independente da forma, ela DEVE ser precisa e de fácil compreensão.
Um número de versão padrão DEVE ter o formato X.Y.Z onde X, Y e Z são números inteiros não negativos e não devem conter zeros à esquerda.
X indica a versão major (principal)
Y indica a versão minor (secundária)
Z indica a versão de patch (correção de bugs)
Todos os elementos DEVEM sempre aumentar.
Exemplo: 1.9.0 → 1.10.0 → 1.11.0
Ao se lançar um pacote versionado o conteúdo desse pacote NÃO DEVE ser modificado. Todas as modificações DEVEM ser lançadas como uma nova versão.
A versão principal zero (0.y.z) é para desenvolvimento inicial. Tudo pode ser modificado a qualquer momento. A API pública dessa versão NÃO DEVERIA ser considerada estável.
A versão 1.0.0 define a API pública. A forma como o versionamento é modificado após esse lançamento depende inteiramente dessa API e de como ela é modificada.
O número de patch Z (x.y.Z | x > 0) DEVE ser incrementado apenas se bugs antigos e retrocompatíveis forem corrigidos. Uma correção de bug é definida como uma modificação de código que corrige comportamento inadequado.
O número minor (x.Y.z | x > 0) DEVE ser incrementado se novas funcionalidades retrocompatíveis forem inseridas na API pública. DEVE ser incrementado se qualquer funcionalidade da API pública for decretada como deprecated. TALVEZ seja incrementado se funcionalidades substanciais ou grandes melhorias são feitas em código privado. TALVEZ possua modificações a nível de patch. O número de patch DEVE começar em 0 sempre que um número minor for incrementado.
O número major (X.y.z | x > 0) DEVE ser incrementado se qualquer modificação NÃO retrocompatível for inserida na API pública. TALVEZ possua modificações a nível de número minor e patch. Os números minor e de patch DEVEM começar em 0 sempre que um número major for incrementado.
Versões pré-lançamento TALVEZ possuam um apêndice alfanumérico imediatamente após o número de patch. Pode conter apenas caracteres ASCII alfanuméricos e hífen.
Exemplo: 1.0.0-alpha
O que são Tags do Git?

As tags no git funcionam como marcadores para commits específicos. Sâo como branches, exceto que tags não são deletadas. São constantemente usadas para demarcar versões de software.

Existem dois tipos de tag: as lightweight tags e as annotated tags.

Lightweight Tag (tag leve)
Uma lightweight tag serve como um indicador de commit. É basicamente um indicador de branch que nunca muda. É apenas um ponteiro que indica qual commit representa essa tag. Contém também um arquivo checksum associado.
Geralmente é utilizada internamente pela equipe para dar nomes e indicadores para commits importantes internamente, mas que não farão parte de releases oficiais.
Annotated tag (tag com notação)
Annotated tags são salvas como objetos completos no banco de dados do git. Elas contém um arquivo checksum, o nome do criador da tag, email do criador e a data de criação. Possuem também uma mensagem associada.
São utilizadas em lançamentos públicos do software e impactam diretamente no usuário do software. Podem ou não vir acompanhadas de mudanças na API pública.
Como Criar uma Tag no Git? (Versão Linha de Comando)
Lightweight Tag
Para criar uma lightweight tag execute o seguinte comando:
git tag my_lightweight_tag_name
Annotated tag
Para criar uma annotated tag execute o seguinte comando:
git tag -a number_of_version -m ‘This is the number_of_version version’
-a indica a anotação da tag
-m indica a mensagem associada a tag

Os dois tipos de tag são criados por padrão na sua branch e commits atuais. Para que as tags criadas localmente sejam sincronizadas com o repositório online, execute o seguinte comando.

git push origin –tags

Para criar tags para commits antigos é necessário saber o código hash do commit. Cada vez que um commit é feito, um código hash associado é criado. Você pode encontrá-los digitando o seguinte código na linha de comando

git log –pretty=oneline

Para criar tags associadas a um desses commits antigos, copie o código hash e utilize o seguinte comando

git tag -a number_of_version hash_of_commit_here

COMO CRIAR UMA TAG NO GIT? (VERSÃO SITE DO GITLAB)

Para criar tags no site do GitLab, acesse primeiramente a página do projeto desejado. Clique em +, e em seguida em New Tag.

Na próxima página, digite o nome desejado para a tag e selecione a branch desejada.

Lightweight Tag
Não preencha o campo message. Release notes são opcionais.
Annotated tag
Preencha obrigatoriamente o campo message. Release notes são opcionais, mas fortemente recomendadas.

Clique em create tag e pronto!

As tags do projeto podem ser visualizadas ao clicar em tags na página principal do projeto.

Referências:

Semântica de versionamento

Como se tornar um mestre das tag Git

Git Basics – Tagging (Documentação oficial)

Key words for use in RFCs to Indicate Requirement Levels

Já passou por uma situação em que o versionamento seria de extrema valia para solucionar um problema de implementação? Nos conte nos comentários! Tem dúvidas sobre boas práticas de uso de Git e Github? Não deixe de nos acompanhar nas nossas redes sociais e nos mandar uma mensagem!

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software."
Como Publicar um Aplicativo feito em React Native na Play Store,http://www2.decom.ufop.br/terralab/como-publicar-um-aplicativo-feito-em-react-native-na-play-store/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/05/01-05-React-Native-Play-Store-5.jpg,"Escrito por Koda Faria | Revisado por Fabrício Pereira

Para publicar um aplicativo na Google Play Store é preciso que ele seja assinado com uma release key que precisa ser utilizada para todas as versões futuras do aplicativo.

Desde 2017 é possível que a própria Google Play gerencie essas chaves graças a funcionalidade de Assinatura de Apps da Google Play. Entretanto, antes de fazer upload do app é necessário assiná-lo com uma chave de upload.

Para ver o processo por completo e em detalhes, você pode acessar a página oficial “assinando seus aplicativos” na documentação do Android Developers. Esse guia é uma descrição simples do passo a passo necessário.

Observação: todas as localizações descritas são relativas à pasta principal do projeto, exceto quando outro caminho for indicado.

Gerando uma Chave de Upload

Você pode gerar uma chave privada utilizando o keytool. No Windows, você precisa executar o keytool a partir da pasta “C:\Program Files\Java\jdkx.x.x_x\bin” com o Prompt de Comando em modo de administrador.

Para gerar a chave, execute o seguinte código:

keytool -genkeypair -v -keystore my-upload-key.keystore -alias my-key-alias -keyalg RSA -keysize 2048 -validity 10000

Substitua conforme o projeto:

my-upload-key: nome da sua key;
my-key-alias: apelido da sua key;
-validity 10000: quantidade de dias da validade da key.

Esse comando te pedirá várias informações, conforme imagem abaixo. Preencha-as de acordo com o contexto de lançamento do seu aplicativo.

Nota: lembre-se que sua key store é privada. Caso a key seja comprometida de alguma forma, ou você a perca, siga essas instruções.
Configurando as Variáveis no Gradle
Mova o seu arquivo “minha-chave-upload.keystore” para a pasta “android/app”;
Edite o arquivo “~/.gradle/gradle.properties” ou “android/gradle.properties” e adicione as seguintes linhas, substituindo as informações pelas cadastradas no passo anterior:
 MYAPP_UPLOAD_STORE_FILE=my-upload-key.keystore

 MYAPP_UPLOAD_KEY_ALIAS=my-key-alias

 MYAPP_UPLOAD_STORE_PASSWORD=*****

 MYAPP_UPLOAD_KEY_PASSWORD=*****

Essas se tornarão variáveis globais no Gradle e poderão ser utilizadas posteriormente para configurar a assinatura do aplicativo.

Nota de segurança: Se você não gosta de manter sua senha em texto simples e está em um OSX você pode armazenhar suas senhas em um app de Keychain Access

Adicionando Configurações de Assinatura à Configuração do Gradle do seu APP

Edite o arquivo “android/app/build.gradle” e adicione a configuração de assinatura:

...

android {

   ...

   defaultConfig { ... }

   signingConfigs {

       release {

           if (project.hasProperty('MYAPP_UPLOAD_STORE_FILE')) {

               storeFile file(MYAPP_UPLOAD_STORE_FILE)

               storePassword MYAPP_UPLOAD_STORE_PASSWORD

               keyAlias MYAPP_UPLOAD_KEY_ALIAS

               keyPassword MYAPP_UPLOAD_KEY_PASSWORD

           }

       }

   }

   buildTypes {

       release {

           ...

           signingConfig signingConfigs.release

       }

   }

}

...
Gerando o AAB de Lançamento

Execute o código em um terminal:

cd android && gradlew clean && gradlew bundleRelease

A função bundleRelease criará um bundle de todos os javascripts necessários para rodar sua aplicação em um AAB (Android App Bundle).

O arquivo AAB gerado pode ser encontrado na pasta “android/app/build/outputs/bundle/release/app.aab” e agora está pronto para ser enviado para a Google Play.

Nota: para que o Google Play aceite o formato AAB o App Signing by Google Play precisa previamente ser configurado para sua aplicação no Google Play Console. Se você for atualizar um aplicativo já existente que não utiliza ainda o App Signing by Google Play cheque a seção de migração para mais informações.

Para cada AAB gerado que for ser enviado como uma nova versão é necessário que se modifiquem as variáveis “versionCode” (some +1 a cada novo aab) e “versionNumber” no arquivo “android/app/build.gradle” (suba conforme versionamento. Leia sobre versionamento aqui.) conforme mostrado abaixo:

android {

   defaultConfig {

       versionCode 1

       versionName “1.0.0”

       {…}

   }

   {…}

}

Mais informações:

Publishing to Google Play Store

Atualizando o número de versão de um app React Native

Ainda lhe resta alguma dúvida? Conhece alguma dica valiosa que deveria ser mencionada aqui? Não deixe de deixar nos comentários e nos seguir nas redes sociais!

Você sabia que este artigo foi escrito por um trainee do TerraLAB? Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software."
COVID-19: Como Projetos de Pesquisa Auxiliam no Combate a Doença?,http://www2.decom.ufop.br/terralab/covid-19-como-projetos-de-pesquisa-auxiliam-no-combate-a-doenca/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/04/Projetos-de-Pesquisa-COVID19-1.png,"Escrito por Fabrício Pereira | Revisado por Palloma Brito

Ao lado de outros dois pilares de uma instituição pública de ensino superior, os projetos de pesquisa representam significativo papel científico dentro da sociedade. São esses trabalhos que contribuem para avanços e criação de tecnologias que impactam diretamente o cotidiano e o bem estar dos indivíduos.

Em um momento de pandemia global como o atual, cabe a pesquisa desenvolver meios e estudos para gerir e erradicar o Coronavírus Sars-Cov-2, além de fornecer auxílio a população contra a patologia provocada por ele, a COVID-19.

A Universidade Federal de Ouro Preto (UFOP) não fica de fora desse movimento. O “CuidaIdoso” tem o objetivo de disponibilizar gratuitamente plataformas computacionais para suporte e orientação da população durante a pandemia. Através do site, Instagram e Facebook o grupo divulga diversas informações ligadas aos cuidados com os idosos, especialmente durante a atual pandemia. O conteúdo criado destaca-se pelo espaço dedicado não só ao monitoramento físico e psicológico do idoso mas também por orientações on-line e a criação de uma rede de colaboração. A ideia é que esses dados sejam disponibilizados em tempo real e usados para a determinação de regiões de risco, para que apoiem decisões do sistema de saúde sobre políticas de apoio à população.

A parceria entre integrantes do Departamento de Computação (DECOM) e Departamento de Análises Clínicas (DEACL) pretende explorar novos canais além do site e redes sociais. O interesse da equipe é abranger o conteúdo também em forma de um aplicativo que permitirá a conexão entre as pessoas e o apoio ao monitoramento de seu estado de saúde.

Com o objetivo de analisar a subnotificação de casos de COVID-19 no Brasil, pesquisadores da UFOP e da Universidade Federal de Minas Gerais (UFMG) publicaram um artigo com os resultados obtidos. O estudo que tomou como base o número de síndromes respiratórias registradas pelo Sistema Público de Saúde (SUS) ao longo dos últimos dez anos teve como resultado a proporção 7,7:1, ou seja, o número de casos reais no país deve ser de pelo menos sete vezes o número de casos divulgados. O cálculo serve como um alarme para a curva crescente de contágios da doença e a importância da prevenção por parte de toda a população.

Ainda no território mineiro, a Universidade Federal de Viçosa (UFV) através de seus seis laboratórios investe na detectação do coronavírus. O teste realizado é denominado RT-PCR e é feito em tempo real. Trata-se da coleta de uma amostra de secreção nasal e da garganta do paciente que é levada ao laboratório para uma busca pelo material genético do vírus. A expectativa dos coordenadores do projeto autorizado pela Secretária de Estado de Saúde de Minas Gerais é de 200 testes realizados ao dia. Tal iniciativa é de suma importância para o desafogamento do sistema de saúde local.

A Universidade também se destaca pelo projeto focado na produção de equipamentos de proteção individual. Em falta, os chamados EPIs são utilizados pelos profissionais de saúde no acolhimento de pacientes que apresentam sintomas da COVID-19. Diante disso, os pesquisadores da UFV se empenham em produzir EPIs do tipo máscara de proteção feitas por impressoras 3D. Com a estimativa de 25 EPIs produzidas por dia, o grupo planeja atender toda a região e aceita doações recebidas por formulário.

Mobilização semelhante a esta é a que acontece em quatro universidades paulistas. Os mais de 140 pesquisadores das universidades de São Paulo (USP), Estadual de Campinas (Unicamp), Estadual Paulista (Unesp) e Federal de São Paulo (Unifesp) estão divididos em projetos desde a produção de ventiladores pulmonares de baixo custo, testes de diagnósticos da patologia a pesquisa sobre a qualidade do ar atmosférico e sobre os impactos na economia.

Acontece também na USP um teste de medicamentos contra a COVID-19. Em parceria com a Eurofarma, o grupo avalia a atividade antiviral de compostos em células infectadas com o SARS-CoV-2, em testes in vitro. As análises automatizadas, acontecem de maneira simultânea e usam dezenas de milhares de fármacos toda semana. Os pesquisadores envolvidos estimam que mais de 2.500 compostos fiquem prontos em apenas algumas semanas.

O Ministério da Educação (MEC) se mostra ativo neste movimento ao promover dois editais pela Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES). O edital nº 11/2020 é voltado exclusivamente ao combate à pandemia, com foco no estudo de fármacos, vacinas, produtos imunológicos e temas correlatos. Já o edital nº 12/2020 refere-se ao desenvolvimento de estudos, procedimentos e inovações tecnológicas em telemedicina e análise de dados médicos não só para o combate ao coronavírus mas também para temas afins.

É também de autoria do MEC uma plataforma de monitoramento das instituições de ensino. A ferramenta dispõe de listas das ações de enfrentamento desenvolvidas pelos centros acadêmicos reunindo números de instituições por ação; o número de ações realizadas; dos médicos, enfermeiros, farmacêuticos e fisioterapeutas dedicados a elas. São encontradas também um conjunto rico de informações acerca dos inúmeros projetos, como os acolhimentos psicológicos virtuais e o Tele Coronavírus, uma rede de atendimento mantida pela Universidade Federal da Bahia (UFBA) destinada a orientar a população sobre o coronavírus. A plataforma também dá ao visitante a possibilidade de saber sobre o funcionamento dos institutos e universidades federais durante a pandemia. As datas da suspensão das aulas de cada instituição e universidade, assim como as que oferecem aulas remotamente, são disponibilizadas no sistema.

Em resumo, a ciência deve ser valorizada especialmente em momentos como este. Por meio dela que avanços tecnológicos na área da saúde são possíveis e permite, ao lado dos profissionais de cada setor, progredir rumo ao combate à crise.

Conhece algum projeto que merece ser reconhecido pelo seu valor no cenário contemporâneo? Nos conte abaixo nos comentários! Não deixe também de acompanhar nossas publicações aqui no blog e nas nossas redes sociais."
Tecnologia Vs Coronavírus: Conheça 5 Tecnologias que Agregam no Combate ao Vírus,http://www2.decom.ufop.br/terralab/tecnologia-vs-coronavirus-saiba-5-ferramentas-que-agregam-no-combate-ao-virus/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/03/COVID-Blog.png,"Escrito por Fabrício Pereira | Revisado por Palloma Brito

Diversas são as maneiras para evitar e combater o novo coronavírus. Numerosas também são as possibilidades da Computação contribuir para a causa. Conheça abaixo cinco dessas tecnologias:


O Auxílio de Drones para a Conscientização Social

Instituições governamentais europeias e asiáticas aplicaram medidas para a conscientização populacional sobre o perigo da aglomeração durante a atual pandemia. Pode se citar como exemplo a Espanha, local em que foi promulgado um decreto que ordena o isolamento por 15 dias. Lá a polícia informa, por meio de drones, os que insistem em desobedecer o decreto. Através de um aviso sonoro, esses drones ajudam na contenção dos casos no país.

Esses veículos aéreos transportam amostras médicas entre o centros de controle da epidemia na China. O primeiro país a sofrer com os impactos da doença se beneficia desse movimento para a contenção de contágios entre seus habitantes.

O Suporte da Inteligência Artificial

A China também usa a inteligência artificial desenvolvida pela Infervision no diagnóstico do COVID-19. A plataforma escaneia imagens de tomografia dos pulmões dos pacientes e facilita que os sinais da patologia sejam facilmente observados pelos médicos.

Um projeto apoiado pela Microsoft em Seattle – EUA também se utiliza dos benefícios da inteligência artificial em testes da doença. Os usuários fazem a avaliação em casa e recebem o resultado em até dois dias. Em caso afirmativo, deve ser respondida uma série de perguntas sobre os locais onde a pessoa esteve e os entes que teve contato.

A Biometria em favor da Detecção de Sintomas

A chinesa ZKTeco distribuiu terminais capazes de rastrear sintomas do COVID-19 através do reconhecimento facial em suas fábricas. O equipamento leva frações de segundo para determinar a temperatura de uma pessoa. Esta aferição é feita pela palma da mão à distância, a fim de se evitar que o aparelho fique contaminado e contagie outras pessoas.

A Geolocalização a Serviço da Telemedicina

Quando se trata de visualização de ocorrências em territórios, a geolocalização de dispositivos é uma poderosa aliada à telemedicina. Por intermédio dela a startup paranaense SIGA disponibiliza, a partir da coleta dos dados fornecidos pelo Ministério da Saúde, o mapa da evolução da pandemia no Brasil. Nele, é possível visualizar os casos confirmados em círculos vermelhos e os de morte em círculos pretos junto dos números em cada estado brasileiro.

Ainda em nível nacional, se destaca o CovidZero. A plataforma é desenvolvida por profissionais tanto do setor de tecnologia da informação quanto de comunicação e acumula mais de 1200 voluntários. Através dela o usuário se informa sobre os números dos casos confirmados e de óbitos em cada um dos estados; filtra esses casos por cidades e, por meio da geolocalização de seu dispositivo, do local onde estiver situado. São disponibilizados também inúmeros artigos sobre a origem da patologia, medidas de proteção e demais informações relevantes sobre o tema. 

Projetos de instituições de ensino fornecem também potentes recursos na situação atual. O dashboard desenvolvido por Johnnatan Messias, ex aluno de Ciência da Computação pela Universidade Federal de Ouro Preto (UFOP), cobre a pandemia em nível global. Atualizado em tempo real, o site dispõe tabelas e gráficos com os casos confirmados, recuperados e de óbitos dos países mais afetados pelo coronavírus. Destaque também para o dashboard implementado pela Universidade da Virgínia que contempla tanto os itens citados como os organiza em uma linha do tempo pelo globo terrestre.

O Investimento do Governo Brasileiro em Telemedicina

Promovido pelo Ministério da Saúde, o aplicativo Coronavírus-SUS reúne informações e orientações acerca da doença. Mediante um formulário de autoexame, o usuário elenca os seus sintomas, dessa forma o app verifica a possível contaminação da patologia. Disponível para Android e iOS, o app já conta com mais de 500 mil downloads.

O que você tem feito para evitar a contaminação deste vírus? Compartilhe com a gente nos comentários e não se esqueça de acompanhar nossas redes sociais."
Processo Trainee TerraLAB 2020.1: Update,http://www2.decom.ufop.br/terralab/processo-trainee-terralab-2020-1-update/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/03/ProcessoSeletivo-Blog-1.png,"Escrito por Fabrício Pereira

O primeiro evento do Processo Trainee TerraLAB 2020.1 aconteceu no dia 13 de março – o Workshop TerraLAB 2020.1. Nossas parceiras de sempre – Stilingue, Usemobile, Cachaça Gestor e Gerencianet – nos brindaram com a sua presença em palestras proveitosas sobre arquitetura de softwares, desenvolvimento para Android, uso do AngularJS e NodeJS em aplicações WEB, gestão de pessoas, processos ágeis, etc. Lotando o auditório do Departamento de Computação (DECOM) de 08h até às 18h, o evento também contou com a participação ilustre da equipe de Quality Assurance do iFood compartilhando conosco poderosas dicas de como, através de testes automatizados, é possível se garantir a qualidade de um produto ou serviço dentro do mercado. O Dr. Igor Soares, ex engenheiro de software da Google por 10 anos, nos prestigiou com uma bela palestra sobre sua enorme vivência nessa indústria.

Auditório do Departamento de Computação (DECOM) durante o Workshop TerraLAB 2020.1
« ‹
 de 9
› »

O Processo Trainee recebeu ao todo 39 inscrições de graduandos e pós-graduandos em Ciência da Computação, Engenharia de Controle e Automação, Engenharia Mecânica, Engenharia de Minas, Arquitetura e Urbanismo, Direito, Engenharia de Materiais, Jornalismo e Estatística até sexta-feira, dia 13 de março. O TerraLAB iniciou o processo seletivo na segunda-feira seguinte ao workshop. Mesmo com a ameaça do Coronavírus, o grupo se manteve ativo via home office, sendo dividido nas seguintes equipes: Análise de Negócios, Análise de Geoprocessamento, Jurídico, Infraestrutura, Design e Marketing, Gerência de Projeto, Engenharia de Software Backend, Engenharia de Software Frontend WEB e Mobile; e Engenharia de Testes WEB e Mobile. Cada uma dessas equipes ou capítulos vem recebendo a mentoria de profissionais reconhecidos pelo mercado e liderados por integrantes mais experientes do laboratório.


Objetivando a troca de conhecimentos e experiências adquiridas tanto na trajetória acadêmica e profissional de cada um, como também por meio de cursos online oferecidos pelas empresas parceiras do TerraLAB, os inscritos são convidados a cumprir desafios semanais que os levará ao desenvolvimento de seis projetos de software que resultarão em aplicativos WEB e Mobile inovadores.


Inicialmente, o processo trainee teria a duração de três semanas. Com a atual situação, a data para seu término permanece em aberto. Toda atualização será postada aqui no blog. Por isso, continue acompanhando este blog e nossas redes sociais!"
Workshop TerraLAB 2020.1,http://www2.decom.ufop.br/terralab/workshop-terralab/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/03/WhatsApp-Image-2020-03-07-at-5.52.53-PM-2.jpeg,"No dia 13 de março de 2020, sexta feira, o TerraLAB irá realizar um Workshop com várias palestras voltadas para o desenvolvimento de software e como melhorar suas habilidades.

O Workshop é também a primeira atividade obrigatória para os inscritos no Processo Seletivo Trainee 2020/1.

Confira nossa programação:

8h – Apresentação Programa Trainee (Professor Tiago Carneiro)
Nesta palestra, o professor Tiago Carneiro apresentará o formato e cronograma do processo seletivo do TerraLAB. Ele também apresentará as empresas parceiras desta iniciativa e falará sobre as perspectivas futuras da construcão de um “Programa de Residência em Software” e um “Programa de Mestrado Profissional em Computação Aplicada” no DECOM .

9h – Processos ágeis, gestão de pessoas, pipeline de produtos (Stilingue)
Nesta palestra a Stilingue nos apresenta uma filosofia que acaba por incentivar o maior trabalho em equipe, a auto-organização, a comunicação frequente, o foco no cliente e a entrega de valor.

10h – Noções básicas de desenvolvimento de aplicativos Android (Usemobile)
Nessa palestra será apresentado pela Usemobile o conceito básico de desenvolvimento mobile e as principais tecnologias que envolvem a construção de uma aplicação Android.

11h – Noções básicas de uma aplicação web utilizando AngularJS e Node (CRUD Soluções)
Nessa palestra serão apresentadas pela CRUD Soluções duas das principais tecnologias para o desenvolvimento de serviços WEB: AngularJS para front-end e Node para back-end.

13h – Arquitetura de Software na Gerencianet (Gerencianet)
Nesta Palestra, Fracisco (Gerencianet) falará sobre motivações e benefícios dos modelos arquiteturais de software utilizados na Gerencianet.

14h – Garantindo a qualidade do seu produto/serviço com testes automatizados (iFood)
Nesta palestra de Igor Resende (iFood) você irá ouvir sobre conceitos básicos a arte na automação de testes para garantir a qualidade de aplicações front e backend

15h – Vivências na indústria de software e sua carreira (Dr. Igor Prata)
Informação: de Gargamel a surfista <Shift F6>. Calma! Eu sei que não parece, mas é isso mesmo. Sabe esse espanto/desconforto que você sentiu? Pois é, a palestra é sobre ele 

16h – O que consigo fazer no Kubernetes? (Stilingue)
Nesta palestra a Stilingue irá falar sobre o uso da tecnologia Kubernetes na orquestração de containeres na Google Cloud: Deploy Automatizado de Aplicações, Gestão e Escalabiliade.

17h – Transformação digital na Indústria 4.0 (Vallourec)
Nesta palestra, o engenheiro Júlio César e time irão compartilhar as experiências da equipe no desenvolvimento da estratégia de implantação de soluções inteligentes para otimização de processos industriais e no desenvolvimento de produtos digitais da VSB.

Venha prestigiar conosco esse evento!"
Aberto o Processo Trainee TerraLAB 2020/1,http://www2.decom.ufop.br/terralab/aberto-o-processo-trainee-terralab-20201/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/03/POSTER-ATUALIZADO.png," 

Inscrições de 02 a 13 de março de 2020

Aprenda as tecnologias mais pedidas no mercado e desenvolva suas habilidades trabalhando com profissionais experientes e qualificados. Você desenvolverá aplicativos mobiles e web, ambos integrados a serviços na nuvem! Dentre outras tecnologias, você vai ter contato com React, React Native, Node JS, Jest, Cucumber e Selenium, Apium, AWS e mais.

Graduandos e pós-graduandos da UFOP: Aumente as suas chances de conseguir uma vaga de emprego e domine as tecnologias utilizadas nas indústrias de software.

Nosso objetivo é aumentar sua empregabilidade e, ao mesmo tempo, reduzir o custo e tempo da seleção e treinamento de recursos humanos por parte da indústria.

Os estudantes serão capacitados em um ambiente profissionalizante, similar ao de uma fábrica de software, onde o estudante pode vivenciar os papéis existentes no ecossistema dessa indústria.

Nominalmente, os estudantes podem vivenciar os seguintes papeis: Analista de Negócio, Gerente de Projeto, Analista de Sistema, Engenheiro de Teste, Engenheiro de Software, Designer Gráfico, Analista de Geoprocessamento, Analista de TI, Análise de marketing, Analista contábil e Analista jurídico.

Coordenaçao: O laboratório TerraLAB é coordenada pelo professor Dr. Tiago Garcia de Senna Carneiro, do Departamento de Computação (DECOM) da Universidade Federal de Ouro Preto (UFOP).

Mentoria: Os estudantes selecionados receberão mentoria de profissionais experientes que atuam em empresas parceiras. Estas empresas, de antemão, expressaram interesse em contratar, em médio prazo, os estudantes que apresentarem bom desempenho na realização das atividades propostas nesta inciativa.

Processo seletivo: Após o período de inscrição, no dia 13 de março, sexta, faremos um workshop onde as empresas parceiras apresentarão seus cases e ambientes de desenvolvimento aos estudantes inscritos. Depois, os estudantes terão aproximadamente 3 semanas para realizarem auto-treinamento por meio dos cursos online oferecidos pelas empresas parceira sobre as atuais tecnologias por elas utilizadas para o desenvolvimento de frontends (web e mobile), backends (em nuvem) e testes (web, mobilie e backend). Durante esse tempo, serão propostos desafios para os estudantes, que devem utilizar os conhecimentos adquiridos para resolvê-los. Um deles será o desenvolvimento de um aplicativo, utilizando-se das tecnologias descritas acima.

Pré-requisitos: É essencial que os candidatos sejam pessoas automotivadas e comprometidas com o sucesso dos projetos com os quais se envolvem. Apesar dos projetos pedirem demandas técnicas específicas, os candidatos receberão treinamento e mentoria para desenvolvimento de suas tarefas. No ambiente de fábrica de software, é clara a necessidade de profissionais que atuam para apoiar o processo de produção de software e que não são programadores, este é o caso do Analista de TI que mantém a rede de comunicação e servidores linux em funcionamento, além do Designer Graficos que cuida das peças (imagens, ícones, telas e textos) de comunicação associadas aos produtos e o Gerente de Projetos que tem como incumbência administrar os resultados, prazos, custos e recursos dos projetos. Então, se você ainda não é um programador no paradigma orientado por objetos, pense em nos ajudar desempenhando um desses papéis.

Inscrição: De 02 a 13 de março de 2020. Os interessados devem enviar currículo LATTES e fornecer todas as informações solicitadas no seguinte formulário .

Bolsa: Neste semestre, os estudantes deverão se voluntariar e receberão como contrapartida a mentoria e o treinamento nas tecnologias supracitadas. Após esse período de tempo, os estudantes com bom desempenho poderão receber bolsas para a continuidade dos projetos que iniciaram neste semestre.

Carga Horária: 20h/semanais

Cooperação voluntária: Todos os estudantes inscritos no processo seletivo estão desde já convidados a ajudar na preparação do espaço físico, de hardware e de software do TerraLAB.

Número de vagas : Não há um número máximo de vagas em aberto, todos os estudantes que tiverem bom desempenho no processo seletivo serão convidados a tomar parte nesta iniciativa. No passado, o laboratório TerraLAB já teve 36 colaboradores atuando simultaneamente em diversos projetos.

As inscrições para o processo seletivo podem ser feitas do dia 2 ao dia 13 de março de 2020.

A proposta está aberta aos estudantes de todos os cursos de graduação e pós-graduação da UFOP.

A primeira atividade obrigatória do processo seletivo é a presença durante todo o Workshop TerraLAB que irá ocorrer no dia 13 de março de 2020, sexta feira. 

Os horários e temas do evento podem ser encontrados no fim dessa postagem.

Ajude-nos a divulgar essa iniciativa!

 "
O que as Empresas Parceiras Pensam sobre a Atuação do TerraLAB em 2019,http://www2.decom.ufop.br/terralab/feedback-das-empresas-mentoras-do-terralab/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2020/01/ae5c77_772dd6465cdc40b98dfa441b3d3e6346_mv2.jpeg,"Em razão da necessidade de simular um ambiente empresarial e familiarizar os alunos do curso de Ciência da Computação com as principais tecnologias exigidas pelo mercado de tecnologia, o TerraLAB começa o ano de 2020 estruturando o processo seletivo do seu  Programa de Trainee 2020/1 e  apresentando o feedback das empresas parceiras que ofereceram mentoria aos estudantes que fizeram parte de todo o processo de desenvolvimento de software proposto para no Programa de Trainees 2019/2.

 

Julio Cesar Ferrerira – Gerente de Datascience da Vallourec
Pedro Gomide – Engenheiro de Soluções Digitais da Vallourec

“A percepção dos nossos times de Datascience e de Soluções Digitais para a Industria 4.0 é de que os trainees do TerraLAB entenderam muito claramente o timming e o pragmatismo com os quais os trabalhos são desenvolvidos nas empresas.

Nós acreditamos que o Programa de Trainee do TerraLAB irá gerar excelentes resultados sobre a formação dos alunos.

A interação entre trainees e profissionais vem acontecendo de forma muito espontânea e os trainees vem assimilando muitos bem as questões tratadas nestas interações, devido à leveza na qual essas questões vêm sendo tratadas, apesar no alto nível técnico das exigências.

O ambiente do TerraLAB é certamente um ambiente de cooperação.

Assim, a perspectiva é de que os trainees se graduem com um entendimento muito aprimorado de como as soluções devem ser desenvolvidas nas empresas.

Por essa razões, eles irão começar a vida profissional vários passos à frente dos estudantes que não passam por uma experiência como essa.”

————

Conrado Carneiro,  CEO na Usemobile – Ex-aluno do DECOM e do TerraLAB

“Projeto como esse liderado pelo TerraLab é fundamental para fomentar dentro da universidade um ambiente empresarial, proporcionando aos alunos experiências de como desenvolver produtos.Senti falta dessa oportunidade durante meu período de estudo na graduação. A maioria das iniciativas eram voltadas para pesquisas e muito distante do mercado de trabalho.

Durante a apresentação dos resultados fiquei surpreso ao perceber que os produtos desenvolvidos (GeoSpotted e Batcaverna) em menos de 3 meses, utilizaram tecnologias e processos que gastamos mais de 1 ano para criar na Usemobile. Desde a qualidade de software e testes até a aparência e a experiência proporcionada aos usuários. Um produto em sua primeira versão, digno de lançamento ao mercado.

Acredito que em breve colheremos bons frutos preparando essa turma para atender as exigências do mercado de trabalho.

Parabéns contem com a Usemobile para dar continuidade nesse projeto.”

————-

 

Filipe Mata,  Analista de Sistemas na Gerencianet Pagamentos do Brasil 

“Achei bem interessante pois todos os alunos demonstraram imensa gratidão ao Tiago e ao laboratório, uma vez que todos afirmaram terem ganhado conhecimento e experiência que jamais teriam ganhado somente no curso de Ciência da Computação.

Por menor que fosse o tempo da apresentação, pude perceber o como os alunos cresceram profissionalmente e tecnicamente falando.

Fiquei muito feliz ao ver que estes alunos tiveram a oportunidade de aprender um acervo de tecnologias demandadas pelo mercado, e que se empenharam em desenvolver algo com o que aprenderam. Segundo o próprio Tiago, foram 6 semanas de trabalho duro e aprendizado.

Eu particularmente me impressionei com a qualidade do produto que eles fizeram em tão pouco tempo e usando tantas tecnologias que usamos aqui e que outras empresas, como a Usemobile, utilizam.

Mais do que capacidade técnica, pude enxergar nos alunos um forte espirito de equipe, já que durante o processo cerca de 17 alunos desistiram do projeto, mas aqueles que restaram realmente souberam interagir entre si, trabalhar em equipe e lidar com as pressões de atender a um prazo de entrega.

O que me deixou mais abismado é que os alunos mantiveram a empolgação mesmo diante das adversidades.”

————

 

Itálo Milagres – Engenheiro de Software na Cachaça Gestor – Ex-aluno da UFOP e do TerraLAB

“O programa de Trainee que acontece no TerraLab, idealizado pelo professor Tiago, é uma “ferramenta” importante na formação dos alunos graduandos em Computação, visto que o mesmo abrange áreas que o currículo acadêmico não alcança.

Sendo graduado também pela UFOP no ano de 2015, eu e vários colegas, sentimos, e pelo o que eu pude observar os alunos atuais sentem também, um certo medo quando nós chegamos perto de concluir o curso por não nos acharmos preparados para o mercado de trabalho, visto que tivemos muito pouco – ou nenhum – contato com a maioria dos requisitos solicitados em vagas de empregos da nossa área, durante a nossa graduação. Por esse “medo” eu entrei no mestrado de para-quedas e fiz um período até eu abandonar e estudar por conta própria as tecnologias do mercado.

No pouco tempo de estruturação e execução da primeira turma do projeto de trainee já tivemos resultados satisfatórios tanto para os mentores quanto para os alunos. Ao fim de três semanas de projeto os alunos conseguiram desenvolver aplicações tanto web quanto android, com a implementação de testes, gerência de projetos, cooperação entre as equipes e muito mais. Coisa que o mercado necessita e muito!

Com certeza os alunos que participarem e concluírem este treinamento sairão muito mais preparados para o mercado do que aqueles que apenas frequentaram as aulas e tiraram notas boas nas disciplinas.

Vejo que tal projeto, além de preparar melhor os alunos para o mercado, também conseguirá diminuir a evasão escolar, que é tão grande no nosso curso, pois 1) os alunos irão trabalhar efetivamente em um projeto real, 2) irão aprender tecnologias que são amplamente utilizadas pelas grandes empresas, 3) a sinergia entre alunos e empresas reais irá de fato acontecer, e com isso os mesmos ficarão mais motivados para continuar no curso e na carreira.

Quem dera se eu pudesse ter passado por algo similar durante a minha graduação.

Hoje fico muito feliz em poder ajudar os novos alunos a verem um outro mundo da computação dentro da Universidade.”

————

 

Novamente, o nosso muito obrigado às empresas parceiras ! 

Siga nossa redes sociais para mais informações de como abaixar a versão Beta do Geospotted e ficar por dentro das novidades do processo seletivo Trainee TerraLAB 2020/1."
Visualização de modelos espaciais no Repast,http://www2.decom.ufop.br/terralab/visualizacao-de-modelos-espaciais-no-repast/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/10/googleSIGHabitar.png,"Este é o primeiro blog de uma série que abordará a visualização de modelos espaciais em algumas das principais ferramentas de modelagem. Ele será dividido em duas partes, iniciamos dando uma visão geral da plataforma de modelagem e a finalizamos com um pequeno tutorial sobre o desenvolvimento de modelos espaciais.

Repast

O simulador Repast foi criado no laboratório de pesquisa computacional em Ciências Sociais da Universidade de Chicago e atualmente é gerenciado pelo Repast Organisation for Architecture and Development (ROAD) [Crooks, 2007]. Ele é uma plataforma de modelagem baseada em agentes (ABM) desenvolvida em Java, de código livre e atualmente esta na versão Simphony 2.0[1]. O nome é um acrônimo para Recursive Porous Agent Simulator Toolkit.

O Repast foi concebido como uma biblioteca Java para trabalhar em conjunto com o simulador Swarm[2] e facilitar seu uso. No entanto, devido ao potencial vislumbrado pelos pesquisadores tornou-se um projeto independente [Collier, 2002].

Tutorial – Criando um modelo em Repast

Nesse tutorial, criaremos um modelo de drenagem, onde a chuva ocorre nas áreas mais altas (altitudes superiores a 200 metros) e escorre para as áreas mais baixas. No resultado final, as linhas de drenagem emergem e mostram o percurso feito pela água, como apresentado na Figura 1. Como ocorre com qualquer ambiente de modelagem é importante que o usuário tenha algum conhecimento de lógica de programação. No Repast, também é necessário um conhecimento básico da linguagem Java e por isso não entraremos em muitos detalhes acerca do uso desta linguagem e sua interface de programação.

Figura 1- Resultado final do modelo de drenagem

 

O simulador pode ser encontrado na página http://repast.sourceforge.net/download.html. Nesse tutorial utilizaremos a versão Simphony 2.0 para Window 7. Informações sobre a instalação em outras plataformas estão disponíveis no endereço acima.

Após o download e a instalação, abra o Repast Simphony. Essa versão do simulador esta integrada ao ambiente Eclipse 3.7.1.

Modelos em Repast podem ser criados utilizando fluxogramas, linguagem Groovy e linguagem Java. Fluxogramas facilitam o desenvolvimento de modelos por meio do uso de componentes gráficos que representam passos da simulação (Figura 2). A linguagem Groovy[3] é uma linguagem de programação ágil e dinâmica para a plataforma Java e inspirada em outras linguagens como Python e Ruby (Figura 3). E, a linguagem Java foco desse tutorial.

Figura 2 – Modelagem via fluxogramas (Fonte: FlowZombies disponível junto com o simulador)

Figura 3 – Código de inicialização do modelo na linguagem Groovy (Fonte: FlowZombies disponível junto com o simulador)

 

Para iniciar, vamos criar um projeto Repast Simphony:

Clique no menu File > New > Other…

Surgirá uma janela como apresentado na Figura 4.

Selecione a opção “Repast Simphony Project” e depois clique no botão Next.

Na próxima tela, dê o nome “drenagem” ao projeto e depois clique em Finish.

 

 

Figura 4 – Assistente de criação de projetos do Repast

 

O ambiente vem, por padrão, com a perspectiva ReLogo ativada, para alterar para a Java basta clicar no botão “Java” na parte superior direita da aplicação (Figura 5). Para projeto em Java, é necessário ativar o autobuilding que por padrão vem desativado. Para isso, vá ao menu Project e clique em Build Automatically.

Figura 5 – Perspectiva Java ativada

 

Neste momento, o Repast deverá apresentar uma tela parecida com a Figura 6. Em destaque nessa figura são arquivos gerados automaticamente pela plataforma e não serão usados. Logo, podem ser apagados.

Figura 6 – Estado do Repast Simphony após a criação do projeto

 

Após apagar os arquivos “ModelInitializer.groovy” e ModelInitializer.agent”, iremos criar a classe que será responsável pelo ambiente de execução do modelo. Então, clique no menu File > New > Class, será exibido uma tela similar à Figura 6. Insira o nome “Drenagem” para classe e clique no botão Finish.

Figura 7 – Assistente de criação de classes

 

Nesse momento a classe “Drenagem” já esta criada mas ela ainda é somente uma simples classe Java.

A estrutura padrão de um modelo no Repast é baseada em dois objetos Context e Projection. Context é um container de objetos e representa uma população abstrata de um modelo porém, sem o conceito de espaço ou de relações. Internamente, ele mantém informações sobre o ambiente em que os objetos interagem, não estando diretamente relacionado a um espaço físico [Allan, 2010] [Howe et al 2006]. Projections são estruturas que definem algum tipo de relação entre os agentes pertencentes a um Context possibilitando a interação entre eles [Allan, 2010]. Um objeto Context pode relacionar com diversos objetos Projection porém, um Projection relaciona apenas com um objeto Context.

Para que possamos utilizá-la no Repast para manter o ambiente de simulação, é necessário que ela implemente a interface Java Context e sobrecarregue o método build(). Para isso, altere o código da classe para este apresentado no Código 1. Nós também declaramos algumas variáveis e constantes que serão usadas durante a simulação.

package drenagem;

 

import java.util.ArrayList;

import repast.simphony.context.Context;

import repast.simphony.dataLoader.ContextBuilder;

 

publicclass Drenagem implements ContextBuilder<Object>

{

         // Constantes

        final static int TAMANHO = 100;        // tamanho do espaço (área modelada)

        final static double INFILTRACAO = 0.0;         // coeficiente de infiltração

        final static double CHUVA_POR_TEMPO = 10.0;    // quantidade de chuva por tempo

        final static double ALTITUDE_MIN_CHUVA = 200.0; // altitude mínima onde ocorrerá chuva

        // Variáveis

        private double[] arquivoAltimetria;

        public static ArrayList<Object> celulas;

       

        // Método de construção do contexto da simulação

        public Context<Object> build(Context<Object> context)

        {

                // Define uma identificação para o contexto

                context.setId(“drenagem”);

                return context;

        }

}

Código 1 – Classe Drenagem com a implementação da interface Context, declaração de algumas constantes e variáveis

Neste ponto, temos nossa classe “Drenagem” interagindo com o Repast e mantendo o ambiente de simulação.

Leitura do arquivo de altimetria

Modelaremos uma área de 100 m² que será fragmentada em células que ocupam áreas de 1 m² e possuem altitude definida pelo arquivo “altimetria.csv“. Iniciaremos carregando este arquivo para a memória e em seguida, associaremos o valor encontrado em cada linha do arquivo a uma célula.

Para codificar o método de leitura, faça download do arquivo (altimetria.csv) e salve no diretório definido como workspace do Repast Simphony (Diretório padrão é “C:\RepastSimphony-2.0\workspace”). Copie o método leArquivoAltimetria() do Código 2 e insira após o método build().

  // Método de leitura do arquivo de altimetria

  private void leArquivoAltimetria(String nome) throws IOException

  {

        arquivoAltimetria = newdouble[TAMANHO * TAMANHO];

        try

        {

                // Abre o arquivo altimetria.csv

                BufferedReader in = new BufferedReader(new FileReader(nome));

                int contaLinha = 0;

                String linha;

                // Lê cada linha e armazena o valor no array arquivoAltimetria

                while ((linha = in.readLine()) != null )

                {

                        arquivoAltimetria[contaLinha] = Double.parseDouble(linha);

                        contaLinha++;

                }

        }

        catch (FileNotFoundException e)

        {

                e.printStackTrace();

        }

}

Código 2- Método leArquivoAltimetria. Iniciamos o array arquivoAltimetria para armazenar o conteúdo do arquivo de altimetria

Criando o agente Célula

Como já comentado, o espaço é fragmentado em células e cada célula ocupará uma posição no espaço.

Dando continuidade ao tutorial, agora criaremos o objeto “Celula”. Para começar, vá ao menu File > New > Class… Na tela que surge, no campo Name, digite “Celula” e clique em Finish.

Copie o Código 3 e sobrescreva o aquele gerado pela automaticamente pela plataforma.

package drenagem;

 

import java.util.List;

import repast.simphony.query.space.grid.GridCell;

import repast.simphony.query.space.grid.GridCellNgh;

import repast.simphony.space.grid.GridPoint;

publicclass Celula {

        // Variáveis

        public double altitude, qtdAgua, fluxo;

        public int x, y;

 

        // Construtor

        public Celula(int x, int y, double altitude){

                this.x = x;

                this.y = y;

                this.altitude = altitude;

                qtdAgua = 0.0;

                fluxo = 0.0;

        }

 

        // Método que será executado pelo escalonador

        publicvoid execute() {

                // Recupera a localização no espaço

                GridPoint pt = Drenagem.grid.getLocation(this);

                // Cria uma grade de celulas vizinhas

                GridCellNgh<Celula> gradeViz = new  GridCellNgh<Celula>(Drenagem.grid, pt, Celula.class, 1, 1);

               

                // Cria uma lista de grade de celulas vizinhas

                List<GridCell<Celula> > resultado = gradeViz.getNeighborhood(true);

                int contVizinhos = 0;

                // Calcula a quantidade de água

                qtdAgua = qtdAgua – Drenagem.INFILTRACAO * qtdAgua;

 

                // Conta o número de vizinhos mais baixos

                for (GridCell<Celula> gradeCel : resultado){

                        Iterable<Celula> iteradorItens = gradeCel.items();

                        for (Celula vizinho : iteradorItens){

                               if ( (this != vizinho) && ( altitude >= vizinho.altitude) )

                                       contVizinhos++;

                        }

                }

                if (contVizinhos > 0){

                        // Divide a quantidade de agua igualmente entre os vizinhos

                        double fluxo = qtdAgua / contVizinhos;

                        // Envia agua para os vizinhos mais baixos

                        for (GridCell<Celula> gradeCel : resultado){

                               Iterable<Celula> iteradorItens = gradeCel.items();

                               for (Celula vizinho : iteradorItens){

                                       if ( (this != vizinho)

                                          && ( altitude >= vizinho.altitude) ){

                                               vizinho.fluxo = vizinho.fluxo + fluxo;

                                       }

                               }

                        }

                }

        }

}

Código 3 – Código completo do objeto Celula

 

Criando o agente Chuva

Esse agente simulará a chuva que apenas ocorre nas células mais altas.

Para continuarmos, crie uma nova classe chamada “Chuva” e sobrescreva o conteúdo pelo apresentado no Código 4.

package drenagem;

 

publicclass Chuva {

 

        public Chuva()

        {

        }

       

        // Método chuva

        publicvoid execute()

        {

                // Percorre todas as células

                for(Celula c : Drenagem.celulas)

                {                      

                        // A chuva ocorre somente nas células com

                        // altitude superior a 200 metros

                        if (c.altitude > Drenagem.ALTITUDE_MIN_CHUVA)

                               c.qtdAgua = c.qtdAgua + Drenagem.CHUVA_POR_TEMPO;

                }

        }

}

Código 4 – Código do objeto “Chuva”

Criando o agente Fluxo

O agente fluxo permitirá a transferência entre a quantidade de água das células mais altas para aquelas mais baixas.

Crie uma nova classe com o nome “Fluxo” e sobrescreva o conteúdo pelo Código 5.

package drenagem;

 

public class Fluxo {

        public Fluxo()

        {              

        }                                              

        // Método fluxo superficial

        public void execute()

        {

                // Percorre todas as células e a quantidade de água

                // é, a cada execução, sobrescrita pela água recebida

                // de células mais altas

                for (Celula c : Drenagem.celulas)

                {

                        c.qtdAgua = c.fluxo;

                        c.fluxo = 0.0;

                }

        }

}

Código 5 – Código do objeto “Fluxo”

Agora, voltemos para o código do objeto “Drenagem” para descrevermos o código de criação dos objetos “Celula”. Então, vá ao Package Explorer e dê um duplo clique nele, como apresentado na Figura 1 e insira o Código 6 no método build().

Figura 8 – Localização do Package Explorer e o arquivo Drenagem.java

 

// Método de construção do contexto da simulação

public Context<Object> build(Context<Object> context)

{

        // Define uma identificação para o contexto

        context.setId(“drenagem”);

 

        // Inicializa o container de celulas

        celulas = new ArrayList<Celula>();

       

        // Adiciona a chuva ao contexto da simulação

        context.add(new Chuva());

 

        // Le o arquivo de altimetrias

        try {

                leArquivoAltimetria(

                        “C:/RepastSimphony2.0/workspace/altimetria.csv”);

                }

        catch (IOException e) {

                e.printStackTrace();

        }

               

        // Laço de repetição que cria as células e define

        // sua altitude de acordo com o valor definido no

        // arquivo e armazenado no array arquivoAltimetria

        int cont = 0;

        for (int i = 0; i < TAMANHO; i++)

        {

                for (int j = 0; j < TAMANHO; j++)

                {

                        // Cria um objeto celula na coordernada (i, j)

                        Celula c = new Celula(i, j, arquivoAltimetria[cont]);

                                  

                        // Adiciona ao container de celulas

                        celulas.add(c);

 

                        // Adiciona a celula no contexto da simulação

                        context.add(c);

                        cont++;

                }

        }

 

        // Adiciona no contexto da simulação

        context.add(new Fluxo());             

       

        return context;

}

Código 6 – Código de leitura da altimetria, criação dos objetos “Celula” e adição ao contexto e ao container de células. Adição do objetos “Chuva” de “Fluxo” ao contexto.

Definindo a projeção espacial

As relações são definidas por projeções. O Repast possibilita a descrição de modelos que usam várias projeções espaciais: projeção espacial contínua é um espaço onde os agentes presentes nele possuem localização representada por coordenadas de ponto flutuante; projeção discreta é um espaço representado por coordenadas inteiras em que os agentes são dispostos em uma grade; projeção geográfica são projeções em que os agentes estão associados a uma geometria espacial (por exemplo: polígono e ponto); projeção em rede permite representar relações abstratas entre agentes. Sendo assim, para criarmos uma relação espacial entre as células do espaço, definiremos uma projeção espacial discreta através do objeto “Grid” do pacote “repast.simphony.space.grid”. Para isso, vá ao método build() do objeto “Drenagem” e insira as linhas apresentadas no Código 7. O método createGrid() não possui valores padrões logo, é necessário passar alguns parâmetros.

        // Cria uma fábrica de construção de objetos grid

        GridFactory gridFactory = GridFactoryFinder.createGridFactory(null);

        // Especifica propriedades para um objeto grid

        GridBuilderParameters<Object> gridBuildParam = new GridBuilderParameters<Object>(

                        new WrapAroundBorders(), new SimpleGridAdder<Object>(),

                        true, TAMANHO, TAMANHO);

        // Inicializa o objeto Grid

        grid = gridFactory.createGrid(“grid”, context, gridBuildParam);

Código 7 – Inicialização do objeto Grid

Código 6 contém a lista de classes de arquivos de importação necessários para que as definições dos objetos sejam encontradas.

import repast.simphony.space.grid.GridBuilderParameters;

import repast.simphony.space.grid.SimpleGridAdder;

import repast.simphony.space.grid.WrapAroundBorders;

import repast.simphony.context.space.grid.GridFactory;

import repast.simphony.context.space.grid.GridFactoryFinder;

Código 8 – Lista de arquivos de importação

O objeto “Grid” como apresentado no Código 7 irá acarretar em erro pois ele ainda não foi declarado. Sendo assim, insira a linha em destaque no Código 9 (inicio do objeto “Drenagem”) a declaração desse objeto.

        // Variáveis

        private double[] arquivoAltimetria;

        public staticArrayList<Celula> celulas;

        public static Grid<Object> grid;

Código 9 – Declaração do objeto “Grid”

Ainda no método build(), é necessário inserir o objeto “Celula” na projeção então, insira a linha em negrito no Código 10 imediatamente após a linha “contex.add(c)” dentro do laço de repetição.

 

// Laço de repetição que cria as células e define

// sua altitude de acordo com o valor definido no

// arquivo e armazenado no array arquivoAltimetria

int cont = 0;

for (int i = 0; i < TAMANHO; i++)

{

        for (int j = 0; j < TAMANHO; j++)

        {

                // Cria um objeto celula na coordernada (i, j)

                Celula c = new Celula(i, j, arquivoAltimetria[cont]);

       

                // Adiciona a celula no contexto da simulação

                context.add(c);

               

                // Insere o objeto c na coordenada (i, -j)

                // O valor de negativo para j é necessário para o posicionamento

                // correto das células no espaço pois o Repast considera o canto

                // inferior esquerdo como ponto (0, 0)

                grid.moveTo(c, i, -j);

                cont++;

        }

}

Código 10 – Adição da célula na projeção espacial

O Repast utiliza arquivos XML para a criação de interface gráfica do usuário (GUI) e de outros componentes em tempo de execução. Para que possamos rodar nosso modelo, é necessário adicionar qual o tipo e o nome da projeção que será usada na simulação. Para tanto, localize o arquivo “context.xml” (Figura 9), insira a linha em negrito no Código 11 e salve o arquivo.

Figura 9 – Arquivo context.xml

 

<context id=“drenagem”

        xmlns:xsi=“http://www.w3.org/2001/XMLSchema-instance”


        xsi:noNamespaceSchemaLocation=“http://repast.org/scenario/context”>

        <projection type=“grid” id=“grid”></projection>

</context>

Código 11 – Definição do tipo e do nome da projeção utilizada

 

Configurando a GUI do modelo de drenagem

Agora, execute o modelo. Vá à barra de ferramentas e clique no botão Run As… no menu que se abre, clique em drenagem Model (Figura 10).

Figura 10 – Botão “Run As…” e a opção de executar o modelo drenagem

 

Irá surgir uma tela similar a esta apresenta na Figura 11. Em azul, destacamos a barra de ferramentas do ambiente de simulação e em vermelho, na listagem à esquerda tem a opção “Data Loaders”. Clicando sobre ela com o botão direito do mouse será exibido um menu de contexto, clique em Set Data Loaders, em seguida abrirá o assistente de configuração, selecione a opção “Custom ContextBuilder Implementation” e em seguida clique em Next. Na tela seguinte, aparecerá na caixa de combinação o nome “drenagem.Drenagem” selecionado, clique em Next e depois em Finish.

Para efetivar as alterações, clique no botão Save Model na barra de ferramentas no ambiente de simulação.

A criação da interface gráfica é feita clicando na opção “Displays”, logo abaixo de “Data Loaders”, com o botão direito e depois em Add Display abrirá o assistente apresentando em Figura 12. No assistente, clique em “grid”, depois na seta para a direita e em seguida, em Next.

Figura 11 – Ambiente de simulação do Repast Simphony

 

Figura 12 – Assistente de configuração do Display – Seleção da projeção

Figura 13 – Assistente de configuração do Display – Seleção do agente

 

A tela do assistente será similar à apresentada em Figura 13. Selecione “Celula” na lista à esquerda , clique na seta para a direita e depois, em Next. Nas próximas telas, manteremos as configurações pré-definidas. Então, clique em Next até que o botão Finish fique ativo e clique nele. Após o assistente fechar, salve o modelo.

Na barra de ferramenta do ambiente, clique no botão Initialize Run. Deverá aparecer uma tela similar à Figura 14. Clicando no botão Start, o tempo de simulação (em destaque na Figura 14) ficará negativo pois não há nada escalonado.

Figura 14 – Visualização do estado inicial do modelo

 

Escalonamento de métodos

O tempo pode ser usado de forma síncrona, em que se considera que todos os agentes da simulação mudam simultaneamente, e de forma assíncrona, padrão utilizado pela plataforma, em que os agentes mudam considerando a realidade deixada pelo agente anterior [Crooks, 2007]. Classificado como um simulador de eventos discretos, o Repast organiza a execução dos eventos por meio de um escalonador. De maneira geral, o escalonador representa o tempo de simulação. O escalonamento de alguma ação pode ser feito de três formas: Annotations são comentários especiais inseridos na descrição do modelo e usados para escalonar estaticamente algum evento; Watchers são anotações especiais usadas para escalonar dinamicamente algum evento. Esse escalonamento requer maiores conhecimentos sobre o domínio do problema em estudo pois não ocorrer em períodos pré-estabelecidos; O escalonamento direto é feito por meio do objeto Schedule do pacote “repast.simphony.engine.schedule” que através do método schedule() organiza a execução de uma ação de agente.

A simulação somente ocorre se o modelador definir quais serão os passos que o ambiente de simulação deverá fazer e portanto, nesta parte do tutorial iremos

definir quais serão esses passos que o Repast deverá seguir para que a simulação ocorra.

Começaremos pelo objeto “Celula”, no Package Explorer dê um duplo clique sobre o nome “celula.java”, encontre o método execute() e insira a linha em negrito (anotação) antes da declaração do método conforme apresentado no Código 12. Essa anotação informa ao Repast que esse método será escalonado iniciando no tempo start, a cada intervalo interval e com prioridade priority.

// Método que será executado pelo escalonador

@ScheduledMethod(start = 1, interval = 1, priority = 1)

public void execute()

{

        …

}

Código 12 – Escalonamento do método execute()

Como feito para o objeto “Celula”, faremos de maneira igual para os objetos “Chuva” e “Fluxo”. Portanto, insira a mesma anotação para esses dois objetos.

Neste momento, se executarmos a simulação o resultado apresentado pela GUI será idêntico àquele mostrado pela Figura 14. Para que a GUI apresente corretamente as linhas de drenagem, é necessário informar ao Repast como os valores de quantidade de água deverão ser pintado.

Criando estilos para os agentes

Um estilo define algumas característica visuais que permitirão ao Repast pintar corretamente um agente. Essas características podem ser forma geométrica, cor, tamanho, etc. Em nosso modelo, iremos definir a cor, o formato e o tamanho de uma célula.

Para começar, crie uma nova classe como o nome “EstiloAguaCelula”. Copie o Código 13 e sobrescreva o conteúdo do arquivo.

package drenagem;

 

import drenagem.Celula;

import java.awt.Color;

import repast.simphony.visualizationOGL2D.DefaultStyleOGL2D;

import saf.v3d.ShapeFactory2D;

import saf.v3d.scene.VSpatial;

public class EstiloAguaCelula extends DefaultStyleOGL2D

{

        private ShapeFactory2D shapeFactory;

        private SimpleColorMap simpleColorMap;

 

        // Mapas de tons de azul

        privatefinalstatic Color[] colorMap = new Color[]

        {

                new Color(255, 255, 255, 255),

                new Color(170, 255, 255, 255),

                new Color(  0, 170, 255, 255),

                new Color(  0,85, 255, 255),

                new Color(  0,0, 255, 255),

                new Color(  0,0, 127, 255)

        };

 

        @Override

        publicvoid init(ShapeFactory2D factory)

        {

                this.shapeFactory = factory;

                simpleColorMap = new SimpleColorMap(

                               colorMap,

                               0,              // minLevel

                               305,            // maxLevel

                               new Color(255, 255, 255, 255), // minColor

                               new Color(  0,  0, 127, 255));// maxColor

        }

         @Override

        public Color getColor(Object agent)

        {

                if (agent instanceof Celula)

                {

                        Celula c = (Celula) agent;

                        // Mapea o valor da quantidade de água em uma cor

                        Color color = simpleColorMap.getColor(c.qtdAgua * Drenagem.VAL2COR);

                        return color;

                }

                returnnew Color(255, 255, 255, 255);

        }

        @Override

        public VSpatial getVSpatial(Object agent, VSpatial spatial) {

                if (spatial == null) {

                        spatial = shapeFactory.createRectangle(15, 15);

                }

                return spatial;

        }

}

Código 13 – Estilo que será usado para pintar um objeto “Celula”

O Repast não possui nenhuma forma pré-definida para mapear valores em cores. Por isso, utilizando outro simulador, de código livre, chamado Mason (http://cs.gmu.edu/~eclab/projects/mason/), será o foco de um próximo post, encontramos um conjunto de classes que permitirá fazer facilmente esse mapeamento. Essas classes podem ser baixadas aqui e devem ser salvas na mesma pasta onde os arquivos do modelo estão (Diretório padrão: C:\RepastSimphony-2.0\workspace\drenagem\src\drenagem).

Após criarmos o estilo, devermos informar ao Repast que este estilo dever ser utilizado. Assim, salve as modificações feitas no arquivo ” EstiloAguaCelula.java” e execute novamente a simulação clicando em “Run As…” e depois em “drenagem Model”. Abrirá o ambiente Repast, procure pela opção “Displays” na listagem à esquerda, clique em “A Display” com o botão direito e depois em delete. Agora, novamente clique em “Displays” e depois em “Add Display”. No assistente que surge, insira o nome “Grid Display”, depois selecione “grid”, clique na seta para a direita e em seguida, em Next. Na próxima tela, selecione “Celula” na lista que aparece à direita, clique na seta para a direita e depois em Next. Neste momento, você verá uma tela similar ao apresentado pela Figura 15.

Figura 15 – Assistente de configuração – Definindo o estilo

 

Na caixa de combinação, selecione o estilo “drenagem.EstiloAguaCelula” e depois em Next. Na tela seguinte, desmarque a opção “Show Grid” e depois clique em Next, Next e por fim em Finish. Salve as alterações feitas.

Para finalizarmos, inicialize a simulação (botão Initialize Run) e clique em Start. Neste momento, a simulação deverá iniciar e as linhas de drenagem devem emergir em tons de azul na GUI do ambiente.

O código fonte completo do exemplo por ser encontrado nesse link (drenagem.zip).

 

Referencias

ALLAN, R. J. Survey of Agent Based Modelling and Simulation Tools. . [S.l: s.n.]. Disponível em: <http://epubs.cclrc.ac.uk/work-details?w=50398>, 2010.

COLLIER, N. RePast: An Extensible Framework for Agent Simulation. . [S.l: s.n.]. Disponível em: <http://www.econ.iastate.edu/tesfatsi/RepastTutorial.Collier.pdf>, 2002.

CROOKS, A. T. The repast simulation/modelling system for geospatial simulation. . [S.l: s.n.]. Disponível em: <http://discovery.ucl.ac.uk/15176/>. Acesso em: 18 out. 2012, 2007.

HOWE, T. R. et al. CONTAINING AGENTS: CONTEXTS, PROJECTIONS, AND AGENTS. Proceedings of the Agent 2006 Conference on Social Agents Results and Prospects, p. 107-114, 2006.

THE MENDELEY SUPPORT TEAM. Getting Started with Mendeley. Mendeley Desktop. London: Mendeley Ltd. Disponível em: <http://www.mendeley.com>, 2011.

REPAST SIMPHONY TEAM. Repast Simphony Reference. Disponível em < http://repast.sourceforge.net/docs.html>, 2012.

 

 

[1] http://repast.sourceforge.net/

[2] Swarm é primeira ferramenta de modelagem baseada em agente. Mais informações em http://www.swarm.org/

[3] http://groovy.codehaus.org/"
O TerraLAB realizou o primero Meetup DevOps em Ouro Preto,http://www2.decom.ufop.br/terralab/devops-bootcamp-ouro-preto/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2019/12/meetupOP.jpg,"
No dia 6 de dezembro, ocorreu no auditório do Departamento de Ciência da Computação (DECOM) da Universidade Federal de Ouro Preto o primeiro Meetup DevOps Bootcamp de Ouro Preto. O DevOps Bootcamp é um projeto de capacitação e mentoria em educação corporativa, focada em Inovação, Automação, Segurança, Gestão e aplicação das melhores Ferramentas e Tecnologias existentes no mercado.

Tivemos o prazer de receber em nosso auditório os profissionais Amanda Pinto e Zandler Oliveira, integrantes do DevOps BH, que generosamente nos trouxeram muito conhecimento e novidades que prometem agitar a cidade de Ouro Preto.  Nossa gratidão e reconhecimento à dedicação e ao talento vocês é desmedida!

Nós tamém gostaríamos de agradecer imensamente o apoio das nossas empresas parceiras: Silingue, GerenciaNet, Usemobile e Cachaça Gestor. Sem o apoio de vocês, o evento não teria tanto brilho.

Fique atento às nossas redes sociais e ao nosso blog para não perder nenhuma novidade na área de tecnologia e inovação!

 

Nosso auditório ficou lotado! O próximo meetup terá de ser ainda maior…"
Encerramento do Programa Trainee TerraLAB 2019/1,http://www2.decom.ufop.br/terralab/encerramento-do-processo-trainee-terralab-20191/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2019/12/WhatsApp-Image-2019-12-06-at-21.19.202.jpeg,"É com muito orgulho e alegria que anunciamos o fim e os resultados da primeira edição do Programa de Trainee TerraLAB que se iniciou no dia 23 de setembro. Dentre os 25 candidatos inscritos, apenas 7 foram selecionados. Os trainees iniciaram sua jornada no TerraLAB,no dia 21 de Outubro, onde vivenciaram todas as etapas do desenvolvimento de um produto de software.

Nosso time trabalhou duro durante 8 semanas, venceu barreiras e desafios, aprendeu novas tecnologias e entregou a primeira versão beta do produto com versão para WEB e Android. Durante todo o processo, desde a concepção do produto até a fase de testes, tivemos como mentores algumas das principais empresas geradoras de tecnologia do Brasil como GerenciaNet, Usemobile, Cachaça Gestor e Stillingue.

Os trainees tiveram a oportunidade de desenvolver aplicativos móveis na plataforma React-Native portáveis para Android e IOS. Também desenvolveram aplicativos WEB nas plataformas Angular JS e React. Além disso, eles desenvolveram serviços na nuvem AWS utilizando a plataforma NodeJS integrada a um banco de dados não estruturado em MongoDB. Para o desenvolvimento destes produtos, o time seguiu um processo ágil que misturas os aspectos mais fortes do processo SCRUM e as diretivas de gestão de projetos enunciadas no PMBOK (Project Management Body of Knowledge). A qualidade dos produtos foi garantida por adoção das técnicas BDD (Behavior Driven Development) para a concepção de histórias de usuário e para o projeto de cenários de testes, além da adoção de testes regressivos automatizados em diferentes níveis, contemplando desde verificações unitárias e funcionais até verificações de integração entre  módulos e de aceitação sistêmica por parte dos clientes. Para deploy dos produtos, todo ciclo CI/CD (continuous integration / continuous delivery) foi automatizado por meio do ferramenta para versionamento de software GitLAB.

Gostaríamos de agradecer imensamente a todas as empresas parceiras pelo apoio e parabenizar os trainees pelo empenho e seriedade.

Se você é um aluno de graduação ou pós-graduação e tem interesse em trabalhar com profissionais experientes, aprender a desenvolver aplicações a nível de mercado e aumentar sua empregabilidade, fique atento ao nosso blog e redes sociais para mais informações sobre o processo Trainee TerraLAB 2020/1.

Abaixo, destacamos as opiniões de três de nossos trainees sobre a experiência vivenciada:

——————-

“Estando perto de formar, já tive contato com bastante coisa na UFOP e, dentre elas, poucas me instigaram a querer aprender algo quanto o tempo no TerraLAB.

A experiencia é unica dentre todas as oferecidas na faculdade e extremamente enriquecedora e me sinto privilegiado de poder aproveitar disso antes de me formar.

Antes, eu sentia um certo receio da entrada no mercado de trabalho e uma duvida sobre como começar. O tempo no laboratório acabou resolvendo isso e servindo como preparo para coisas bem maiores do que até então a faculdade tinha oferecido. Se você deseja ficar na área cientifica, talvez todo o foco em pesquisa seja satisfatório porem, para alguém que deseja seguir para o mercado, o despreparo é enorme.

Aprender a trabalhar em grupo, enfrentar os problemas decorrentes do convívio e os problemas que surgem do próprio processo de produção do software são, ao menos para mim, experiencias mais importantes e interessantes do que aprender as tecnologias. Com um grupo ou sem, eu teria aprendido React-Native lendo as centenas de tutoriais na internet porem isso nunca teria me dado todo o resto da experiencia pratica de um ambiente coletivo.

Fico muito feliz pela oportunidade de participar do laboratório e a sensação de lançamento do software é algo que não tinha sentido antes.”

(Arilton é estudante graduação em Ciência da Computação/UFOP com expectativa de 1 ano para sua formatura)

————-

“Minha experiencia no TerraLab foi muito positiva , tive contato com muitas tecnologias que nao conhecia,além de ter tido a vivencia de participar  de um projeto de perto.O TerraLab tambem me motivou no proprio curso,pois vivenciei areas do curso no laboratório.”

(Guilherme é aluno do primeiro semetre do curso em Ciência da Computação/UFOP)

————-

“A lacuna entre o conhecimento desenvolvido na universidade
e o mercado de trabalho não nos causa espanto.

Mas me causou inicialmente surpresa e agora admiração a forma
como um laboratório universitário se posiciona frente essa lacuna.

Mais autodidada, voltei a academia sobretudo por questões
pessoais que por necessidade. Áqui vejo, pela primeira vez no meio acadêmico,
a construção de uma ponte sobre essa lacuna.

Acessível a estudantes interessados desde já em fazer acontecer.

Tudo isso sem se distanciar do embasamento teórico que podemos
(e devemos) mesclar com o conhecimento prático”

(Kleiber é aluno do quinto semestre do curso de Ciência da Computação/UFOP, recém chegado a Ouro Preto e traz consigo três anos de experiência como desenvolvedor fullstack em Belo Horizonte, MG)

—————-

Parabéns aos nossos novos colaboradores! 

Siga nossa redes sociais para  por dentro das novidades do processo seletivo Trainee TerraLAB 2020/1.

 "
Programa Trainee TerraLAB 2019.2,http://www2.decom.ufop.br/terralab/programa-trainee-terralab-2019-2/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2014/10/3.jpg,"Trabalhe com profissionais experientes e aprenda a desenvolver aplicativos móveis e web integrados serviços na nuvem.

 Inscrições de 11 a 18 de setembro de 2019

Graduando e pós-graduandos da UFOP:  Aumente sua empregabilidade e domine as mais atuais tecnologias da industria de software.

Conheça detalhes desta iniciativa.

Faça sua inscrição preenchendo o seguinte formulário.

A proposta está aberta aos estudantes dos mais variados cursos de graduação e pós-graduação da UFOP.

POR FAVOR, AJUDE-NOS A DIVULGAR ESTA INCIATIVA!!!"
"Marketing de Geolocalização, saiba mais sobre este assunto",http://www2.decom.ufop.br/terralab/marketing-de-geolocalizacao/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2018/08/uber-para-lavanderia.jpg,"Marketing de Geolocalização : Entendendo a Segmentação geográfica, Geo-Fencing e Beacons

O termo marketing de geolocalização é muito usado , mas muitas equipes  têm apenas uma vaga idéia do que isso significa.

Qualquer um pode atacar: o marketing de geolocalização provavelmente tem algo a ver com a personalização baseada em localização. Mas o que exatamente os profissionais de marketing personalizam com os dados de localização do usuário? E como coletamos esses dados em primeiro lugar?

Continue lendo para ter uma visão geral rápida do que é marketing de geolocalização e por que é importante.

O que é Marketing de Geolocalização?

De um modo geral, o marketing de geolocalização refere-se a dados sobre a localização física de uma pessoa. Os dados são geralmente fornecidos através de satélites GPS. Se você já abriu um aplicativo de mapa e ampliou o zoom para ver a precisão do pequeno ponto azul, os dados de geolocalização fornecidos pelo GPSdo.

Se o GPS do telefone estiver desligado (ou se não pu estão funcionander encontrar um sinal), os dados de localização serão triangulados das torres de celular . Este método é menos preciso, mas funciona em um aperto. Se você abriu seu mapa no subsolo ou no meio de uma viagem, provavelmente obteve seus dados de localização de uma torre de celular.

Até agora, só falamos sobre marketing de geolocalização da perspectiva do usuário – o dispositivo pinga uma torre de satélite ou celular para determinar em que parte do mundo está. Mas assim que o dispositivo obtiver essas informações, ele poderá compartilhá-las comaplicativos  (como um mapa ou um banco de dados de restaurantes locais).

Como as equipes móveis podem usar o marketing de geolocalização?

Você pode segmentar usuários com base nos dados de localização de algumas maneiras diferentes. Estes são os três mais comuns:

 Segmentação geográfica

 A segmentação geográfica é o ato de alcançar alguém com base em sua localização. Esses segmentos geralmente usam o endereço IP do visitante, em vez da localização por GPS, portanto, a segmentação por área geográfica é realmente anterior à mobilidade. Desde os primeiros dias da internet, os sites usavam o endereço IP do visitante para servir conteúdo personalizado. Um exemplo seria personalizar a moeda exibida em um site de varejo com base no país do visitante.

Naturalmente, os endereços IP não são muito precisos e é complicado para o profissional de marketing segmentar bairros específicos com base em blocos de endereços IP. É por isso que esse tipo de segmentação por área geográfica é mais comumente usado para regiões amplas, como uma cidade inteira (ou estado). Para as equipes de marketing que querem ser mais precisas, é melhor segmentar os usuários com base em uma delimitação geográfica.

Geo-Fencing

Geo-esgrima é a resposta da era móvel para geo-targeting tradicional. Esse tipo de segmentação usa a localização GPS do dispositivo em vez de seu endereço IP, portanto, os dados são muito mais precisos. Ele também é atualizado enquanto a pessoa está em trânsito, por isso é adequado para mensagens móveis.

Uma geo-fence pode ser tão ampla quanto uma cidade, mas é mais eficaz ao segmentar regiões menores, como bairros ou ruas específicas. Essas metas são especialmente úteis para aplicativos que desejam direcionar o tráfego de pedestres para lojas físicas.

Beacons

Os beacons são os mais estreitos dos três métodos de segmentação por local. Um beacon é um objeto físico pequeno que recebe dados de localização de dispositivos próximos via Bluetooth. Por ser baseado em Bluetooth, os beacons podem ser implantados em áreas com pouca recepção de células, como o interior de uma loja.

Os dados do Beacon informam ao aplicativo precisamente onde, na loja, os clientes estão caminhando, o que ajuda os editores a otimizar a experiência na loja. Mas a desvantagem é que o sinal do Bluetooth do usuário deve estar ligado. Além disso, os beacons são difíceis de usar fora de sua propriedade, porque eles devem estar fisicamente colocados também.

Qual é a melhor maneira de melhorar o engajamento de aplicativos com a segmentação por área geográfica?

Para equipes móveis em busca de táticas de marketing que aumentam o engajamento, a geofencing é um bom lugar para começar. A precisão dos públicos geográficos torna-os perfeitos para campanhas móveis, mas eles não exigem uma presença física para serem eficazes.

Por exemplo, um aplicativo de viagem pode querer alertar os panfletos que seu portão mudou por meio de notificação por push. Em vez de acionar a notificação com base no horário, o editor do aplicativo poderia estabelecer uma delimitação geográfica em torno de um aeroporto e acionar a mensagem com base na localização. Desta forma, eles vão entregar a mensagem com um timing perfeito.

Da mesma forma, um aplicativo que organiza restaurantes ou eventos locais pode acionar recomendações com base na vizinhança do usuário. Em vez de oferecer sugestões amplas (por exemplo, “Restaurantes Populares em sua cidade”), o geofencing possibilita sugestões pessoais e imediatamente valiosas (por exemplo, “Bem-vindo a [bairro]! Veja o que você precisa ver”).

Como posso começar com marketing de geolocalização?

A localização geográfica é intuitiva de uma perspectiva de marketing, mas pode ser difícil de implementar a partir de uma perspectiva de engenharia. A maneira mais fácil de começar é com uma plataforma de marketing para celular que ofereça suporte a campanhas baseadas em localização.

Para uma rápida olhada em como a Leanplum gerencia o marketing de geolocalização, leia nossa solução de personalização . Com as ferramentas certas, é fácil envolver seu público com mensagens baseadas em localização.

Para saber mais sobre geolocalização entre em contato com a Usemobile."
SIGHabitar: Um sistema de inteligência de negócios para a gestão municipal,http://www2.decom.ufop.br/terralab/sighabitar-uma-sistema-de-inteligencia-de-negocios-para-a-gestao-municipal/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/10/arquitetura_SIGHabitar.png,"Interessado em GERIR seu município eficientemente, em conformidade com o governo federal e usando sistemas de informação geográfica para tomada de decisão?

Segundo estudo realizado pela Fundação Getúlio Vargas, no ano de 2007, dentre 3359 municípios brasileiros avaliados, somente 2,82% pode ser considerado eficiente na gestão do seu território [1]. No entanto, já existe tecnologia disponível para gerir os recursos e serviços de um município de maneira integrada, a partir de informações continuamente atualizadas, resultantes da análise de diversas fontes de dados alfanuméricos, imagens de satélite e mapas digitais. Informações importantes para as diversas secretarias de um município podem ser reunidas em um único repositório para apoiar o processo decisório em análises customizadas às necessidades de cada secretaria: Fazenda, Obra, Segurança, Saúde, Educação, etc. Neste contexto, apresentamos o sistema de inteligência de negócio para gestão de dados espaço-temporais, SIGHabitar, e apresentamos um estudo de caso realizado no município de Ouro Preto (OP), MG.

O convênio SIGHabitar em Ouro Preto e a portaria 511 de 2009 do Ministério das Cidades

O convênio SIGHabitar firmado entre a Universidade Federal de Ouro Preto (UFOP) e a Prefeitura Municipal de Ouro Preto – PMOP, através do Laboratório para Modelagem e Simulação do Sistema Terrestre (TerraLAB) do Departamento de Computação (DECOM), levou ao desenvolvimento de uma metodologia e de um conjunto de programas de computador que qualquer município pode utilizar para coletar, integrar, visualizar e analisar dados que servirão ao gerenciamento e ao planejamento do município. Todos os programas são gratuitos e estão disponíveis no site do laboratório. Para realizar o download clique: AQUI. Dentro desta inciativa, o projeto de mesmo nome, SIGHabitar, surgiu para atender às recomendações do Ministério das Cidades, que no ano de 2009 criou a portaria 511 [2], com diretrizes para criar, instituir e manter o Cadastro Técnico Multifinalitário no território nacional.

Por meio dos programas desenvolvidos é possível promover (i) uma tributação mais eficaz e justa, (ii) o uso racional dos recursos municipais, (iii) a manutenção e o desenvolvimento de serviços mais eficientes. Por exemplo, é possível dimensionar os serviços de saúde, segurança e educação. Manter atualizado o cadastro de imóveis. Planejar ações da defesa civil, do corpo de bombeiros, das secretarias de trânsito e obra. Além, de gerenciar o uso dos recursos naturais e a coleta de resíduos.

O que é um Cadastro Técnico Multifinalitário?

O Cadastro Técnico Multifinalitário (CTM) é um cadastro, ou banco de dados, capaz de manter informações alfanuméricas e espaciais sobre o território de um município  e de diversos fatos a ele relacionados. Além de contemplar aspectos do tradicional cadastro imobiliário concernentes aos imóveis, é possível relacioná-las com informações de bancos de dados heterogêneas e até mesmo desconectadas sobre uma diversidade de assuntos, como: fatores sociais (trabalho, lazer, saúde, educação e segurança); econômicos (produção e comercialização de bens e serviços) e urbanos (sistemas de abastecimento de água e coleta de esgoto, transporte e rede de comunicação). Isso permite conhecer detalhadamente qualquer porção do território, promover a distribuição justa de recursos e tornar o planejamento urbano mais acurado [3][4]. A figura ao lado ilustra o conceito CTM.

O que é um sistema de Inteligência de Negócios (ou Business Intelligence)?

 

Um Sistema de Inteligência de Negócios é um sistema de informação, que fornece ferramentas e métodos que subsidiam a tomada de decisão para o planejamento estratégico e tático de uma organização. Ele permite extrair, armazenar, explorar e transformar e analisar um grande volume de dados, de maneira rápida e simples para o decisor. Elementos fundamentais são: armazém de dados (DW – data warehouse), ferramentas extração, transformação e carga  (ETL – extract, transform & load), ferramentas para processamento analítico online (OLAP – online analytical processing)  e ferramenta de mineração de dados (DM – data mining).

Qual a filosofia do projeto SIGHabitar?

A filosofia do SIGHabitar é não mexer no que já está funcionando. Implanta-lo deve exigir pouco investimento tecnológico e permitir a continuidade dos trabalhos da prefeitura. Sua implantação deve ser incremental de forma a integrar tudo que já existe causando o mínimo de distúrbio possível, exigindo apenas que o município se adapte a uma cultura baseada na organização e sistematização da coleta de dados a seu respeito. Qualquer município, mesmo que precariamente, possui um passivo em termos tecnológicos que já existe, já foi pago e está em operação. Os bancos de dados, as planilhas, os documentos, as imagens, os mapas e os sistemas de informação já existentes devem permanecer onde estão e continuar em operação. O SIGHabitar deve apenas integra-los afim de permitir análises completas e atualizadas.

A despeito de todos os domínios de problemas administrados por meio do SIGHabitar, ele deve ser capaz de lidar com a dinâmica de uma cidade. Deve estar preparado para mudanças e evoluir com elas e ser capaz de atualizar frequentemente um volume massivo de dados espaciais e temporais. Por isso, flexibilidade para absorver mudanças com pouco esforço e retrabalho mínimo é uma característica indispensável.  Ferramentas que coletam, transformam e analisam os dados do CTM não dependem de seu modelo de dados. Para que resultem em decisões mais acuradas, suas ferramentas permitem análises sobre dados sumarizados em hierarquias espaciais, como país, cidade e imóvel, e hierarquias temporais, como ano, mês e dia.

O desenvolvimento um de CTM pode ser dispendioso. Entretanto, a sustentabilidade do SIGHabitar é garantida. O desenvolvimento do CTM pode envolver a contratação e treinamento de uma equipe multidisciplinar e especializada, a realização contínua de coleta de dados em campo e a aquisição de dados espaciais onerosos, como imagens de satélite de alta resolução, levantamentos topográficos, mapas de arruamento e de imóveis. A necessidade, quantidade e o nível de detalhe dos dados devem ser ponderados frente aos benefícios que resultarão de seu custo. É preciso desenvolver um CTM cujo custo de desenvolvimento seja pago nos primeiros anos após sua implantação através da ampliação do cadastro de imóveis registrados e da cobrança mais eficaz de taxas e impostos imobiliários. Nos anos posteriores, os ganhos serão menores, mas deverão financiar a manutenção e evolução do sistema.

No Brasil, para estarem em conformidade com as diretrizes nacionais para Governo Eletrônico, projetos de desenvolvimento de Sistemas de Informação públicos devem priorizar soluções, programas e serviços baseados em software livre, adotar padrões abertos no desenvolvimento de tecnologia de informação, garantir a auditabilidade plena e a segurança dos sistemas, restringir o crescimento do legado baseado em tecnologia proprietária, além de promover a capacitação de servidores públicos para utilização de software livre. Por estas razões, o SIGHabitar  priorizou o uso de tecnologias livres e de código aberto cujo estágio de maturidade garantem a produtividade do projeto e sua continuidade à longo prazo [5].

Como o sistema SIGHabitar funciona? Qual a inovação?

Apesar das tecnologias para o desenvolvimento e implantação de CTMs existir há, pelo menos, uma década, a maioria dos municípios brasileiros não utilizam  sistemas de computação que apóiem seu planejamento ou a gestão de seus recursos e serviços. Este fato pode ser justificado porque os investimentos em tecnologia são bastante restritos na maioria dos municípios de pequeno e médio porte.  Além disso, a ausência de uma metodologia padronizada e reconhecida para a construção e manutenção de um CTM é uma limitação que desencoraja muitos municípios a investir recursos na reforma do cadastro imobiliário, pelo fato de não terem garantia de sucesso. No Brasil, essa situação é agravada porque o número de profissionais capacitados nessa área é insuficiente para um país com dimensões continentais. O sistema SIGHabitar inova ao fornecer métodos sistematizados e ferramentas de automação destinados à construção e a manutenção de um CTM arquitetado como um sistema de inteligência de negócios.

INTEGRAÇÃO COM SISTEMAS LEGADOS

No SIGHabitar, o CTM é organizado como um armazém de dados espaço-temporal (STDW  – Spatio-temporal Data Warehouse) no qual ferramentas de atualização, implementadas como fluxos ETL, extraem dados de diversas fontes, tais como planilhas eletrônicas, arquivos texto, arquivos shape e bancos de dados relacionais transformando-os em informações de entrada para o CTM. Esses fluxos são executados periodicamente (de hora em hora, toda noite) ou sob demanda (ao clicar do mouse). Os fluxos ETL para integração de dados aparecem em vermelho na figura acima. Para facilitar o desenvolvimento e a manutenção dos  fluxos ETL pela própria equipe da prefeitura, o projeto SIGHabitar integra-se com a ferramenta GeoKettle, que permite que tais fluxos sejam definidos graficamente, por meio de diagramas como o apresentado à esquerda.

DISPOSITIVOS MÓVEIS APOIAM A COLETA DE DADOS EM CAMPO

Para ajudar a equipe a georreferenciar os dados de sistemas legados, ferramentas de coleta de dados foram desenvolvidas como aplicações para dispositivos móveis. A figura ao lado apresenta a tela de uma destas ferramentas. Ela apóia as pesquisas de campo, nas quais equipes percorrem a cidade levantando os limites, as coordenadas geográfica ou os endereços de pontos de interesse como, por exemplo, imóveis, equipamentos urbanos (escolas, hospitais, pontos de ônibus, etc), áreas de preservacão ambiental, árvores, postes, etc.

SANEAMENTO CÍCLICO E INCREMENTAL DOS DADOS MUNICIPAIS

Durante a integração dos dados para a construção do CTM, os trabalhos de campo ajudam a equipe da prefeitura a conhecer melhor a realidade do território municipal. À medida que os endereços e limites de  imóveis ou equipementos urbanos são conhecidos, estes dados são padronizados e inseridos no CTM. Posteriormente, são associados a informações provenientes de outras fontes da dados da prefeitura. Porém, para que a associação aconteça de maneira correta, os dados do município também precisam ser corrigidos. Por exemplo, em Ouro Preto a rua “Conde de Borbadela” é conhecida popularmente como  rua “Direita”. Contudo, muitas vezes os registros e documentos mantidos pelo municipio se referem a esta rua como rua “Conde Borb.”, rua “Dir.”, ou rua “Direita”. Então, para que os dados possam ser associados, a equipe da prefeitura pecisa definir o nome que será utilizado pelo CTM e corrigir os dados antigos da prefeitura. Esse processo pode ser automatizado e é  representado pelos fluxos ETL que aparecem em azul na figura acima. Desta maneira, todo o sistemas de informação do município evolui, cíclica e incrementalmente.

FERRAMENTAS PARA ANÁLISE DE DADOS ESPAÇO TEMPORAIS – MAPAS DINÂMICOS

A análise de dados espaço-temporais são baseadas em ferramentas que combinam técnicas de mineração de dados, OLAP e GIS. Elas são integradas a ferramentas como Google Maps, Google Earth e Microsoft Bing. Elas permitem ao decisor analisar séries temporais de mapas que mostram medidas agregadas a cerca de suas regiões de interesse. Por exemplo, a média de crimes por bairro, o soma do número de casos de dengue por setor censitário, o IPTU mínimo e máximo de determinadas regiões, etc. A figura abaixo ilustra o resultado da primeira campanha de campo e a integração destas ferramentas com o Google Earth. OS DADOS APRESENTADOS SÃO FICTÍCIOS!

A figura a seguir mostra a tela da ferramenta DW-ReportMaker cujos relatórios mostram análises de medidas espaciais agregadas.

A figura a seguir mostra um relatório no qual uma série temporal de mapas gerados por uma das ferramentas de análise desenvolvidas neste projeto. É possível observar a evolução do valor do IPTU nos anos de 2009, 2010 e 2011 para a região próxima à UFOP, conhecida como “Bauxita”.

Quais são as lições aprendidas?

Durante a realização deste trabalho, as seguintes lições foram aprendidas:

Lição 1 – É preciso uma equipe multidisciplinar: Dispor de uma equipe multidisciplinar é o principal fator determinante para sucesso de um CTM. Além do conhecimento de especialistas na administração do município, é preciso mão de obra capacitada nas seguintes áreas: edificações e engenharia civil, computação e geoprocessamento. Durante os trabalhos de campo, especialistas em construção civil apresentaram maior facilidade no uso de mapas, em entender o processo de urbanização a partir de imagens defasadas da realidade e em registrar a geometria de imóveis e logradouros. O desenvolvimento de um banco de dados geográfico requer especialistas em geoprocessamento. É ideal que especialistas em computação se responsabilizem pela gestão do CTM e pela customização das ferramentas de coleta, atualização e análise de dados.
Lição 2 – É preciso envolvimento direto dos administradores municipais (stakeholders): O empenho de tomadores de decisão, técnicos administrativos e especialista nos sistemas legados é vital para a avaliação de versões do CTM e fluxos ETL produzidos. Eles devem assegurar a evolução incremental do CTM. O conhecimento sobre o território do município e sua história é essencial para o saneamento dos dados.
Lição 3 – É possível desenvolver um sistema de BI espaço temporal com ferramentas livres. Contudo, a produtividade da equipe é afetada: Existem excelentes frameworks para o desenvolvimento de sistemas de BI que fornecem ferramentas ETL, DM e OLAP para dados espaciais integradas a SIGs GeoKettle e Talend. No entanto, a deficiência ou inexistência de serviços de suporte técnico e treinamento, a escassez de documentação exemplificada e de casos de sucesso para serem copiados limitam a desempenho da equipe de desenvolvimento.
Lição 4 – É preciso contrabalancear a precisão temporal e a precisão espacial do CTM: O dinamismo da ocupação do solo torna dispendiosa e extenuante a tarefa de manter atualizado um CTM baseado em polígonos que representam as parcelas do território e as unidades habitacionais. No entanto, representar as parcelas e os respectivos objetos que as sobrepõe, por meio de pontos simplifica o processo de atualização, permite o mapeamento eficiente de construções verticais e permite que métodos de análises espaciais sejam aplicados a dados de sistemas legados: estimadores de kernel, krigagem, formação de grupamentos espaciais, etc. Por isso, os gestores do CTM devem considerar o esforço envolvido na atualização dos dados e retorno trazido pela representação espacial escolhida. É preciso balancear precisão espacial e precisão temporal.
Lição 5 – Ferramentas móveis para coleta de dados trazem enorme ganho de produtividade. Durante os trabalhos de campo, o uso de plataformas móveis com GPS e conexão de dados em banda larga permite a navegação assistida por mapas digitais e reduz erros introduzidos por processos manuais de coleta de dados. Resultados de mesma qualidade obtidos por processos manuais requerem, em geral, mais que o dobro do esforço.
Lição 6: É viável abordar um CTM como um sistema para inteligência de negócio:  O uso de tecnologias de Inteligência de Negócios atendeu às expectativas da pesquisa. Elas permitem a análise de volumes massivos de dados. Foi possível atender as diretrizes nacionais para governo eletrônico, a portaria 511 do Ministério das Cidades e as diretivas do projeto Cadastre 2014. Os tomadores de decisão reportaram enorme vantagem em conhecer os valores de indicadores sumarizados por diferentes hierarquias temporais (ano, mês, dia) e espaciais (distrito, bairro, quadra). A continuidade do CTM é assegurada pela maturidade das ferramentas de BI e por serviços para customização de ferramentas através de diagramas e interfaces gráficas. O uso de fluxos ETL se mostrou uma estratégia flexível e eficiente na atualização do CTM. Estima-se que devido à ampliação da base imobiliária cadastrada e de uma tributação mais efetiva, cinquenta por cento do investimento neste projeto tenha sido recuperado no primeiro ano em que ele foi utilizado para cobranças de taxas e impostos. Isto demonstra a sustentabilidade da abordagem proposta. No entanto, a partir de certo momento, o esforço de campo não resultará na evolução do CTM até que ocorra a reorganização do espaço geográfico (estabelecer limites, oficializar logradouros, normatizar numeração, etc.) e o recadastramento de imóveis. É necessário recobrir todo universo do cadastro, para que as políticas sociais sejam efetivas.
Lição 7: O SIGHabitar vs outras abordagens: Os trabalhos encontrados na literatura não apresentam uma metodologia consolidada, para criar e manter um CTM.  A abordagem SIGHabitar consolidou em um estudo de caso real, um conjunto de métodos, softwares livres e modelo de banco de dados multidimensional, para instituir o CTM em municípios bresileiros de pequeno e médio porte. Esse conjunto permitiu georreferenciar informações em campo, integrá-las aos sistemas legados em um DW e criar fluxos automatizados para manter o DW atualizado. Com isso, é possível explorar com maior eficiência um grande volume de dados e gerar relatórios mais completos e atualizados. A utilização de softwares livres torna a solução mais barata, o que permite ser utilizada por municípios que possuem menos poder de investimento destinado às evoluções tecnológicas.
Contato:

Se gostou do trabalho entre em contato. O TerraLAB pode ofecer uma solução eficiente, pessoal capacitado e resultados em curto prazo (6 meses aproximadamente). Certamente poderemos lhe ajudar a GERIR seu município eficientemente, em conformidade com o governo federal e usando mapas para tomada de decisão.

Mais informações podem ser obtidas no wiki do TerraLAB. Este trabalho é resultado da dissertação de mestrado em Ciência da Computação, do DECOM – UFOP, de autoria do Msc. João Tácio Corrêa da Silva [6]. foi publicado na Conferência Internacional “Computational Science and Its Applications, 2012” [7].

Contato com os professores Tiago Carneiro e Joubert Lima: tiago@iceb.ufop.br e joubertlima@gmail.com.

Referências

[1] Fundação Getúlio Vargas (2007). “Estudo Comprova Eficiência Fiscal dos Municípios do PNAFM”. Disponível em:  http://www.ucp.fazenda.gov.br/PNAFM/, Acesso: Novembro de 2011.

[2] Ministério das Cidades (2009). “Portaria 511: Diretrizes para a Criação, Instituição e Atualização do Cadastro Territorial Multifinalitário (CTM) nos Municípios Brasileiros”. 2009. ISSN 1676-2339. Disponível em: http://www.cidades.gov.br. Acesso Agosto, 2011

[3] Loch, C. and Erba, D. A. (2007). “Cadastro Técnico Multifinalitário – Rural e Urbano”. Cambridge, MA: Lincoln Institute of Land Policy, 2007. 142p.

[4] Pereira N. E. C. (2002). “Repensando o valor do cadastro técnico urbano”. In: Congresso Brasileiro de Cadastro Técnico Multifinalitário, 5, Florianópolis. Anais. 2002.

[5] Governo Eletrônico (2012). “Diretrizes”. Disponível em: http://www.governoeletronico. gov.br/o-gov.br/principios/, Acesso: Julho de 2012.

[6] da Silva,  João Tácio  Corrêa. “SIGHabitar – Uma abordagem baseada em Inteligência de Negócios para o desenvolvimento de Sistemas de Informação Territorial: O Cadastro Técnico Multifinalitário do Município de Ouro Preto, MG“, Dissertação de mestrado apresentado ao Programa de Pós-Graduação em Ciência da Computação. Departamento de Comutação – DECOM. Universidade Federal de Ouro Preto – UFOP, 2012. Orientador: Tiago Garcia de Senna Carneiro. Co-Orientador: Joubert de Castro Lima.

[7] Silva, J. T. C. ; Rezende, J. F. V. ; Fidêncio, E. ; Melo, T. ; Neves, B. ; LIMA, J. ; Carneiro, Tiago . SIGHabitar Business Intelligence based approach for the development of Land Information Systems: The Multipurpose Technical Cadastre of Ouro Preto, Brazil. In: ICCSA – International Conference on Computational Science and Its Applications, 2012, Salvador,BA. International Conference on Computational Science and Its Applications, 2012.

 

 "
TerraME: A software platform for modeling and simulation of nature-society interactions,http://www2.decom.ufop.br/terralab/terrame-a-software-platform-to-modeling-and-simulation-of-nature-society-interactions/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/08/amazon_deforestation.gif,"Translated to English language by Tamyres

On Earth, several social and natural systems interact and co-evolve. The understanding of natural phenomena challenges science since its emergence. Various scientists tried to know and describe, using mathematical formulas, the rules that govern these phenomena behaviors, in other words, they constructed mathematical models of these phenomena. On physics and chemistry areas, various models evolved to represent realistically the observed phenomena. In 1807, Dalton modeled the atom as a spherical and indivisible particle without any electric load. In 1913, Bohr demonstrated that the atom had a positive nucleus surrounded by electrons with negative loads and organized in orbits with different energy levels. In past, the Earth was considered flattened. The mathematical models known by science allow the construction of future predictions about several natural systems. To understand the importance the continuous evolution of these models, consider the utility of climatic models to the agribusiness and to the early warning systems of nature disasters.

Despite efforts of anthropology and sociology, the social systems functioning are still not understood. However, currently the social systems are seen as the main driver of changes acting upon nature systems. On the other hand, the nature systems are the main conditioners of social systems behavior. Thus, the social systems affect and are affected by nature systems. So, computer-mathematical models able to simulate the interactions between social and natural systems constitute essential tools to the stakeholders responsible for the definition of politics that aim to regulate the use of Earth’s resources.

Indeed, any decision of governments or private enterprises can be benefited by computational models able to simulate scenarios that evaluate impacts of different strategies: (1) of resource use – space, time, vegetation, water, etc; or (2) of control and response to risks – epidemics, fires, floods, etc. In Brazil, The National Institute of Spatial Researches (INPE) uses TerraME to answer questions related to Land Use and  Cover Change (LUUC) in Amazon [1][2][3] region, to answer questions related to Monitoring, Risk Analysis and Early Warning, and to answer questions about Emission of Greenhouse Effect Gases [4]. Researches of Osvaldo Cruz Foundation (FIOCRUZ) used TerraME to evaluate questions about Dengue Control [5].

TerraME Overview

The TerraME – Terra Modeling Environment – (www.terrame.org) is a toolkit that supports all the phases of environmental model development, namely, model that reproduce the behavior of social and natural processes, showing their interactions and  different changes they promote on each geographic space location. In TerraME, an environmental model is seen as micro-world, namely, a virtual world whose landscape represents a determined region on the Earth, and whose agents, automata and systems that live on it are able to simulate real actors and process of change. TerraME tools are developed by the TerraLAB – Earth Systems Modeling and Simulation Laboratory of the Computer Science Department (DECOM) of the Federal University of Ouro Preto (UFOP) in partnership with the National Institute of Spatial Research (INPE). They are distributed freely to Linux, Windows, and Mac platforms (32 and 64 bits).

The main differential of TerraME in relation with other environment modeling platforms currently available is on the support services for the development of models that consider multiple spatial-temporal scales and on the support to the simultaneous use to multiple modeling paradigms: Systems General Theory, Agents Theory and Cellular Automata Theory. Thus, the modeler does not feel restricted by the choice of a single modeling paradigm and can realistically represent the different scales in which changes occur and in which act the different driving forces of these changes. For instance, in Amazon, the deforestation process is affected locally by the economic situation of small producers, while that globally is affected by commodities prices as soybeans and cattle.

TerraME Graphical Interface for Modeling and Simulation

To support the conception and design of environmental models, TerraME offers a integrated development environment (IDE) known as TerraME GIMS – Graphical Interface for Modeling and Simulation. It allows modelers visually describe their models using diagrams. These diagrams explicit the entities considered in the model, the way how they are hierarchically organized and how they interact with each other. This way, TerraME GIMS facilitates the verification of assumptions in which the models are based. It communicate these assumptions to the decision makers and to the members of the development team that is, routinely, interdisciplinary. The TerraME GIMS is distributed freely as a plugin to the Eclipse platform (www.eclipse.org).

TerraME Interpreter

To support the development of realistic models, TerraME simulator is integrated with Geographical Information System (SIG) that allows modelers to parameterize models from data stored in geographical databases, i.e., temporal series of satellite images and digital maps describing the studied regions. The simulator TerraME is distributed as an interpreter that receives, as entry, the program that represents the environmental model and, so, executes it producing output data that are automatically stored in geographical databases. To facilitate the management and evolution of environmental models, TerraME models are represented on a high level modeling language named TerraML – TerraME Modeling Language, which extends LUA programming language. It was designed to make the job of describe spatial properties, the actors and the processes that changes them. In this language, a complex model can be easily decomposed on simpler models that represent different scales of a same process.

TerraME – Modeling in Multiple Scales

To support model debugging and the analysis of model outcomes, TerraME has a set of components called TerraME Observers. These components are able to show model outcomes, in real-time, as dynamic graphics and maps, on two or three dimensions.

TerraME Observers 

To support the analysis of model sensibility and the projection of simulated scenarios, TerraME simulation nucleus offers a high performance version called TerraME HPA – High Performance Architecture. This simulator version is able to take advantage from the aggregated processing and storage power of multiprocessor and multicomputer hardware architectures, as well as, from computer networks aggregated power. This way, experiments for model sensibility analysis or for the simulation of different scenarios of change can be executed in parallel and in a reduced time.

To support the calibration of environmental models, TerraME offers a set of methods to measure the adjustment between real and simulated maps, in addition to a set of methods to optimize the model parameters so that they maximize the adjustment between the simulated outcomes and the observed data."
"Configurando uma rede linux com servidor web (NAT, DNS, DHCP)",http://www2.decom.ufop.br/terralab/configurando-uma-rede-linux-com-servidor-web-nat-dns-dhcp/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/09/linux-networking.jpg,"Nesse tutorial usaremos como exemplo a rede do TerraLAB que utiliza o OpenSuse 12.1 devido as facilidades que suas ferramentas de configuração oferecem.

Antes de mais nada alguns pré-requisitos:
* OpenSuse 12.1 (atualizado até a data de hoje 20/09/2012)
* Apache 2.2.21
* PHP 5.3.8
* 2 Placas de rede

No nosso caso a interface Eth0 foi configurada para a rede interna no IP (Internet Protocol) 192.168.0.1 e Eth1 configurada via DHCP (Dynamic Host Configuration Protocol), porém ela recebe um IP estático do provedor de acesso à Internet  o qual possui o registro do domínio: www.terralab.ufop.br.

Usando a ferramenta YaST2, clique em Network Settings e confira se está como a imagem a seguir:

Como utilizamos um computador da rede interna como servidor FTP (File Transfer Protocol)  e de alguns serviços do laboratório precisamos ativar o redirecionamento de IP e fazer algumas configurações de NAT (Network Address Translator). Para ativar o redirecionamento escolha a aba Routing e clique em Enable IP Forwarding.

Para configurar o NAT edite as configurações de Firewall do YaST2 da seguinte maneira:


Vamos agora configurar nosso servidor HTTP (Hypertext Transfer Protocol), também chamado de servidor WEB ou WWW, que no OpenSuse fica no diretório /srv/www/htdocs/ de maneira bem simples. No YaST2 clique em HTTP Server e deixe como na imagem a baixo:

Nesse ponto nós já temos o servidor HTTP  funcionando na porta 80 (padrão web) e um redirecionamento para o IP 192.168.0.234 quando acessado na porta 8080, agora vamos configurar o servidor DHCP para que todas as máquinas da rede interna recebam automaticamente um IP inválido ao serem ligadas. Elas terão acesso  à Internet por meio do roteador (gateway) 192.168.0.1 que estamos configurando. Este roteador também terá  o serviço de firewall executando. Desta maneira, buscamos a aumentar a segurança da rede interna.

Para configurar o servidor DHCP clique em DHCP Server, selecione Global Options e clique em edit

Configure da seguinte maneira:

Logo após configure a subnet da seguinte maneira:

Nossa rede está quase pronta ! Para facilitar o acesso aos computadores o YaST também disponibiliza uma ferramenta para configuração de um servidor DNS
Clique em DNS Sever e siga os passos:


Na imagem acima escolhemos servidores DNS Globais para resolução de nomes de domínios da Internet, e não na rede interna. Os dois primeiros são servidores disponibilizados no nosso provedor de acesso, o último é o do Google.

Crie um novo domínio para ser usado na rede interna, no nosso caso, “terralab”.

insira um novo computador para o domínio:

Pronto ! temos nossa rede interna com servidor linux completamente operante e um servidor HTTP com redirecionamento de porta.

Dúvidas, sugestões, críticas deixe seu comentário abaixo."
Como desenvolver geo-aplicações em realidade virtual sobre a plataforma TerraVR,http://www2.decom.ufop.br/terralab/como-desenvolver-aplicacoes-em-realidade-virtual-sobre-a-plataforma-terravr/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/09/rv.jpg,"Oaplicativo TerraVR é um interpretador destinado ao desenvolvimento de aplicações em realidade virtual (RV).  Ele recebe como entrada  e executa um script escrito na linguagem de programação Lua [Ierusalimschy et. Al, 1996]. Este programa em linguagem de alto nível define todas as propriedades da aplicação de realidade virtual: objetos que comporão a cena tridimensional, topografia do terreno do ambiente virtual, animações e a interatividade do usuário. Para isso, o interpretador TerraVR estende a linguagem de programação Lua com novos tipos, objetos e algoritmos especialmente projetados para aumentar a produtividade dos programadores de aplicações em RV e, consequentemente, tornar mais eficiente o processo de desenvolvimento. Para ilustrar, suponhamos o que o código fonte da aplicação em RV esteja armazenado no arquivo de script “Aplicacao.lua”. Então comando “c:\> TerraVR Aplicacao.lua” coloca a aplicação em execução.

O TerraVR é um exemplo de que a Computação provê soluções para:

Empresas e organizações:

O TerraVR permite a criação de jogos sérios para visualização e monitoramento de operações industruais em espaços abertos ou fechados,  como grandes obras, para visualização arquitetural, para treinamentos, e para reconstituição de acidentes. Veja o video de uma aplicação para treinamento de operadores de guindastes em plataformas oceânicas: P-16.

Indivíduos:

O TerraVR permite a criação de   jogos, animações, apresentações e demais soluções de entreterimento.

 

 

 

1. Histórico do projeto TerraVR

O projeto TerraVR teve inicio com uma parceria firmada entre o laboratório TerraLAB do Departamento de Computação (DECOM) da Universidade Federal de Ouro Preto (UFOP) e a empresa especializada em animações digitais Visual Virtual sediada em Belo Horizonte, MG. Esta primeira iniciativa foi documentada na monografia de conclusão de curso do estudante Douglas Oliveira Matoso, em 2007.  Após dois anos de desenvolvimento e contando com uma equipe formada por três ex-alunos do laboratório TerraLAB e vários profissionais de jogos, design 3D e comunicação da empresa Visual Virtual, a primeira aplicação do framework TerraVR foi comercializada, dando origem à startup MineInside.  A aplicação de mesmo nome, MineInside, permitia que as operações da maior mina de minério de ferro à céu aberto do mundo fossem acompanhadas em tempo real por meio de animações tridimensionais com as quais os usuários podiam interagir através de equipamentos em realidade virtual, como joysticks e capacetes com visores acoplados. Na aplicação MineInside, equipamentos de lavra como caminhões fora de estrada, escavadeiras e perfuratrizes são posicionados em tempo real em um ambiente virtual georreferenciado (mina) a partir de dados fornecidos por equipamentos GPS (Global Position System) embarcados.  Mais tarde, a empresa MineInside foi vendida para o grupo DEVEX que levou esta tecnologia a patamares de qualidade e retorno de investimento sem precedentes.

2. Funcionamento básico de uma aplicação em realidade virtual

Antes de começarmos a falar sobre como é composta uma aplicação TerraVR, devemos ter uma noção de como é o fluxo de execução de um script. O código fonte da aplicação de RV (script Lua) é dividido em duas partes. A primeira parte é chamada inicialização, onde o usuário define os cenários, cria entidades (terreno, prédios, carros, personagens, etc). É a parte declarativa do código da aplicação. O usuário cria e  posiciona tudo em seu lugar. A segunda parte, chamada laço principal, é executada repetidamente até o fim da execução da aplicação, é onde está a lógica da simulação, as transformações que as entidades sofrem, suas ações e reações.

O laço principal da aplicação RV é executado o maior número de vezes possível por segundo. O tempo consumido na execução de uma iteração do laço principal, isto é, o tempo  consumido para recalcular e  enviar a cena tridimensional para a placa de vídeo, é chamado tempo de simulação, e depende da complexidade da simulação, da capacidade de processamento do hardware (processador) onde a aplicação estiver sendo executada e da capacidade de comunicação do barramento da placa gráfica. Quando somado ao tempo que a placa gráfica consome para renderizar a cena tridimensional, chamado tempo de renderização, juntos passam a definir a taxa na qual a aplicação RV consegue responder a qualquer interação com o usuário.  Essa taxa é geralmente expressa em número de quadros por segundo (qps), e deve ser mantida em um patamar sempre igual ou superior a 10 qps para garantir ao usuário da aplicação RV a sensação de imersão [Pullen, 1995]. Contudo, idealmente, a taxa de quadros por segundo de uma aplicação RV deve ser, no minimo, igual a 30 qps. Assim, será garantida a continuidade dos movimentos das entidades presentes na cena tridimensional, mesmo os mais rápidos como o deslocamento de um avião. Essa é a taxa de quadros utilizados por filmes na indústria do cinema.

3. Código fonte de uma aplicação mínima em realidade virtual

A mais simples aplicação RV que pode ser desenvolvida na plataforma TerraVR ilustra como um desenvolvedor pode criar um ambiente virtual estático e não interativo  na área central de sua janela principal. O código dessa aplicação é apresentado na figura 1. O trecho de código compreendido entre as linhas 1 a 11 define a interface com o usuário (GUI) da aplicação RV e inicializa seu motor gráfico.

Figura 1. Código fonte para uma aplicação RV mínima: uma câmera para o ambiente virtual.

                Através da chamada ao método createWindow() a janela principal da aplicação é criada e armazena na variável window. A janela principal da aplicação RV conterá toda a interface da aplicação RV, inclusive uma ou mais janelas de renderização, criadas através do método createRenderView(), que delimitam áreas da janela principal onde a cena tridimensional será projetada em duas dimensões .

O método createWindow() é responsável pela inicialização do motor gráfico Ogre 3D, seus parâmetros são: (a) e (b) o tamanho da janela principal da aplicação RV, no código o tamanho da janela de renderização é definido em 800X600, (b) um valor lógico que em caso verdadeiro indica que a resolução máxima do monitor de vídeo deve ser utilizada para definir o tamanho da janela de renderização, e (c) o sistema de renderização que será utilizado em tempo de execução, que pode variar entre os valores OPENGL ou DIRECT3D.

Em seguida, é criado um frame para aplicação RV que agirá como um container para todos os elementos da GUI da aplicação RV. Esse frame é armazenado na variável frame3d (Qt::QFrame). Um layout vertical, variável layout (Qt::QVBoxLayout), é criado para organizar os elementos de interface no interior do frame. Então, por meio da chamada ao método createRenderView(), uma única janela de renderização é criada e inserida no layout vertical e, automaticamente, registrada no motor gráfico Ogre 3D sob o identificador único “render”. Finalmente, o objeto frame3d é definido como frame central da janela principal, por meio de uma chamada ao método setCentralWidget() do próprio objeto janela principal.

No trecho de código definido pelas linhas 13 e 14, o modelo tridimensional que representa o ambiente virtual é carregado em memória a partir do arquivo “apartamento.scene”,  é possivel fazer o download do arquivo utilizado clicando aqui, por meio do método loadScene(). Arquivos do tipo “.scene” são criados através de um plugin chamado OgreMax para o software Autodesk 3DStudio Max que não será abordado nesse documento. O segundo parâmetro do método loadScene() é uma referência para a janela de renderização, render, onde os elementos da cena serão carregados.

Para visualizar o ambiente virtual é necessário um objeto do tipo Camera. O trecho de código entre as linhas 16 e 25 cria a câmera cam usando o construtor Camera(), que recebe como parâmetro uma string que, em tempo de execução, será utilizada para identificar unicamente a câmera criada. Na figura 1, a câmera e posicionada na coordenada (0,50,0) e rotacionada em 90 graus em torno do eixo Y. Em seguida, o objeto cam é anexado ao objeto render pelo método setCamera(). O método setRange() é utilizado para definir o campo de visão da câmera. Nesse exemplo, somente objetos entre 1 e 2500 unidades de medida (que pode ser centímetros, metros, etc dependendo da escala da cena gráfica) serão visualizados.

O último trecho de código, entre as linhas 27 e 30, define o método updateWorld() que deve OBRIGATORIAMENTE ser definido pelo desenvolvedor da aplicação RV. Nesse exemplo, como não há animações na cena tridimensional apresentada, nem qualquer forma interatividade com o usuário definida, função updateWorld() deve apenas retornar o valor true indicado seu termino com êxito. O valor false indicaria o térmíno a aplicação em uma condição de falha.

Figura 2. Resultado da aplicação RV mínima: Uma câmera para o ambiente virtual

                Esse exemplo, apesar de simples, apresenta uma visão geral de como uma aplicação RV pode ser desenvolvida por meio do uso do framework TerraVR.

 

4. Arquitetura de Software do framework TerraVR

O framework TerraVR foi desenvolvido em uma arquitetura em camadas, onde cada camada de software oferece serviços para que a camada de software no nível imediatamente superior possa utilizá-los para  implementar seus próprios serviços. A arquitetura de software do framework TerraVR é estruturada em 4 camadas:

Figura 1.1. Arquitetura de software do framework TerraVR: organizada em camadas.

Na primeira camada, chamada camada de jogos, as bibliotecas C++ Ogre 3D, OIS, OpenAL e PhysX implementam os serviços básicos necessários ao desenvolvimento de aplicações em realidade virtual, cujas demandas são similares àquelas presentes em projetos de desenvolvimento de jogos tridimensionais. A biblioteca Ogre 3D fornece serviços para gerenciamento de cenário, como carga de objetos modelados em sistemas CAD (Computer Aidded Design), serviços para animação de cenários, como operações de rotação e translação de objetos tridimensionais e serviços para renderização da cena gráfica, isto é, serviços para a definição de texturas e de iluminação dos objetos que formam a cena e serviços para transformar a cena tridimensional em imagens bidimensionais que serão continuamente apresentadas ao usuário da aplicação em realidade virtual. A biblioteca OIS oferece serviços para o tratamento de eventos gerados por dispositivos de entrada e saída como joystiks, mouse ou teclado. Esses são serviços críticos para o desenvolvimento de ambientes virtuais interativos onde o usuário da aplicação em RV pode agir sobre os objetos presentes no ambiente virtual e observar a reação desses objetos. Essas reações são modeladas através dos serviços fornecidos pela biblioteca de simulação física chamada PhisX.  A biblioteca OpenAL fornece serviços para gerenciamento de áudio como, por exemplo, a definição de fontes emissoras de som e a renderização do som em 3 dimensões. Desta maneira, o som percebido pelo usuário da aplicação em RV dependerá da sua posição no ambiente virtual e da posição da fonte emissora. A biblioteca Qt fornece serviços para a definição de interfaces gráficas com o usuário (GUI) da aplicação em RV, exemplos de interfaces construídas como a biblioteca Qt são caixas de diálogos, menus popups, botões, etc.

Na segunda camada, chamada camada de realidade virtual, os serviços fornecidos pela camada de jogos são integrados em um único módulo de software chamado kernel TerraVR  desenvolvido na linguagem C++. Portanto, a interface de programação (Application Programming Interface – API) desse módulo integra todos os serviços necessários para a definição, gerência, simulação e renderização (visual e áudio), em tempo-real, dos objetos presentes no ambiente virtual.

A terceira camada, chamada camada Lua, exporta todas as suas funcionalidades (API) do kernel TerraVR para o ambiente de execução da linguagem de programação Lua.  Desta maneira, o desenvolvedor de aplicações em RV pode codificar toda a aplicação em uma linguagem de alto-nível e ainda se beneficiar do alto-desempenho fornecido pelos serviços implementados em C++.

Na quarta e última camada, chamada camada de aplicação, se encontram as aplicações em RV desenvolvidas pelo usuário através do uso da linguagem Lua  e das funcionalidades oferecidas pelo framework TerraVR.

Referências

MATOSO, Douglas Oliveira. Interatividade em Tempo-Real com Simulações Tridimensionais e Georeferenciadas. 2007. Trabalho de Conclusão de Curso. (Graduação em Bacharelado em Ciência da Computação) – Universidade Federal de Ouro Preto. Orientador: Tiago Garcia de Senna Carneiro.

IERUSALIMSCHY, Roberto, “Programming in LUA”, lua.org, 2a. Edição, 1996

PULLEN, J. M. & Wood, D. C., “Networking Technology and DIS”, Proc. IEEE, vol. 83, no. 8, agosto, 1995, p.1156-1167.

 "
DengueME: Um software para modelagem e simulação da transmissão do vírus dengue.,http://www2.decom.ufop.br/terralab/dengueme-um-software-para-modelagem-e-simulacao-da-transmissao-do-virus-dengue/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/09/download.jpg,"A dengue acomete cerca de 50 a 100 milhões de pessoas no mundo anualmente [WHO, 2012].  A doença é causa pela picada dos mosquitos Aedes aegypti e Aedes albopictus, que podem transmitir quatro tipos de vírus caso o mosquito esteja infectado: DENV-1, DENV-2, DENV-3 e DENV-4. No Brasil, o Aedes aegypti é o principal vetor da dengue. Como ainda não existe uma vacina eficaz contra nenhum destes vírus, o combate à dengue acontece principalmente pelo controle das populações deste mosquito.

O ciclo de vida do Aedes aegypti se dá em quatro estágios representados pela figura 1. Os estágios de ovo, larva e pupa acontecem na água, de preferência limpa. Portanto, qualquer recipiente que possa acumular água da chuva serve como criadouro para o mosquito.

 

Figura 1 – Ciclo de vida do mosquito Aedes aegypti. Fonte: portalsaofrancisco.com.br

 No Brasil, três formas de controle populacional do vetor são amplamente utilizadas: (1) controle químico por larvicida, (2) controle químico por adulticida e (3) controle mecânico através da remoção dos criadouros (Donalísio e Glasser, 2002). Os controles feitos por inseticidas, ao mesmo tempo em que são indispensáveis, causam resistência da população de mosquitos (Macoris et al. 1997; Braga e Valle, 2007) e impacto ambiental, muitas vezes comprometendo a efetividade da aplicação (Barreto, 2005). Por outro lado, várias pesquisas têm demonstrado que a remoção e destruição de criadouros é a maneira mais eficiente de controlar da doença. Desta forma, uma vez que o mosquito possui hábitos urbanos, a participação sistemática dos cidadãos no controle das populações do A. aegypti é indispensável.

Contudo, a falta de um programa de conscientização contínuo da população é um dos maiores agravantes dos índices de infestação do A. aegypti. As campanhas de erradicação são feitas basicamente nas épocas de aumento da população do mosquito com a chegada das chuvas no verão, o contrário do que muitos autores e pesquisas sugerem: as campanhas devem ser sistemáticas ao longo do ano [Otero et al., 2008].

O uso de modelos computacionais e matemáticos tem sido adotado por diversos estudos [Focks et al. 1993a; Yang et al. 2009; Lana et al. 2011] com o objetivo de buscar padrões de transmissão do vírus da dengue e de determinar quais os fatores ambientais, sociais e climáticos que, de fato, contribuem para essa dinâmica. Além disto, as simulações de tais modelos permitem avaliar a relação custo/benefício das diversas formas de controle da doença. As simulações podem ser utilizadas para determinar a melhor combinação dessas formas de controle, a intensidade e a frequência de cada uma. Entretanto, a busca pela estratégia ideal de controle da doença ainda está em aberto e muitas questões ainda precisam ser respondidas. A figura 2 mostra mapas de infestação de A. aegypti para o bairro de Higienópolis, no Rio de Janeiro, RJ. É possível comprar os mapas simulados (com fundo preto) ccom mapas construídos a partir de dados coletados em campo por meio de armadilhas (fundo branco). As simulações subestimam a infestação no período de seca e sobrestimam no período de chuva. No entanto, capturam o padrão espacial de infestação.

Figura 2. Resultados das simulações de infestação por Aedes aegypti no bairro de Higienópolis, Rio de Janeiro, RJ.

 

Dentro desse contexto, o DengueME é um ambiente computacional para simulação da dispersão da dengue e de populações de Aedes aegypti em um espaço geográfico real. Ele permite a projeção de cenários de risco e intervenção, a partir da aplicação de modelos matemáticos de dinâmica populacional e de transmissão da doença sobre representações realistas dos espaços urbanos, que incorporam imagens de satélites e mapas digitais armazenados em sistemas de informação geográfica.

Do ponto de vista técnico, o DengueME é um aplicativo desenvolvido sobre o ambiente de modelagem TerraME que fornece serviços específicos para o estudo da Dengue. Entre estes serviços destacam-se:

a) parametrização interativa dos modelos por meio de interfaces gráficas (entrada de dados de chuva, temperatura, características do ambiente urbano, etc);

b) customização interativa dos componentes de modelos (um modelo é formado por vários componentes cujas versões em uso podem variar de acordo com as necessidades de seu usuário, por exemplo, modelo de crescimento das populações de mosquitos, modelos de crescimento das populações humanas, modelo de mobilidade das populações humanas, modelos de dispersão viral, etc);

c) execução interativa dos modelos integrados para projeção de cenários e risco e intervenção.

É importante ressaltar que o DengueME tem como objetivo ser de fácil manipulação para permitir a qualquer pesquisador ou agente de saúde realizar análises voltadas ao combate da dengue. Essa iniciativa tem um objetivo maior de viabilizar e incentivar estudos nessa área, disponibilizando informações relevantes para as Secretarias de Saúde que poderão planejar suas ações de controle da doença.

O projeto DengueME vem sendo desenvolvido no âmbito da Rede PRONEX de Modelagem em Dengue financiada pelo Conselho Nacional de Pesquisa (CNPq). Os desenvolvimento do software é realizado pelo laboratório TerraLAB do Departamento de Computação (DECOM) da Universidade Federal de Ouro Preto (UFOP) , em parceria com o Programa de Computação Científica (PROCC) da  Fundação Osvaldo Cruz  (FIOCRUZ)  e o Centro de Ciência do Sistema Terrestre (CCST) do Istituto Nacional de Pesquisas Espaciais (INPE).

Em breve contaremos um pouco mais sobre modelos para Dengue!

Referências:

Barreto C.F. 2005. Aedes aegypti – Resistencia aos Inseticidas Químicos e as Novas Alternativas de Controle. Revista Eletronica Faculdades Monte Belo, Goiás, ISSN 1808-8597, v.1, n.2, p. 62-73

Braga I.A., Valle D. 2007. Aedes aegypti: histórico do controle no Brasil. Epidemiologia e Serviços de Saúde; v. 16, p. 113-118.

Donalísio M.R., Glasser C.M. 2002. Vigilância Entomológica e Controle de Vetores do Dengue, Rev. Bras. Epidemiol. V. 5,p. 259-272

Focks D.A., Haile D.C., Daniels E., Moun G.A. 1993a. Dynamics life table model for Aedes aegypti: Analysis of the literature and model development. J. Med. Entomol. v.30, p. 1003–1018.

Lana, R.M., Carneiro, T.G.S., Honório, N.A. & Codeço, C.T. 2011. Multiscale Analysis and Modelling of Aedes Aegyti Population Spatial Dynamics. Journal of Information and Data Management, Brazilian Computer Society, v. 2, p. 211-221.

Macoris M.L.G., Mazine C.A.B., Andrighetti M.T.M., Yasumaro S. 1997. Factors favoring houseplant container infestation with Aedes aegypti larvae in Marília SP, Brazil. Rev Panam Salud Publica v. 1, p. 280-286.

Otero M., Schweigmann N., Solari H.G.2008. AStochastic Spatial Dynamical Model for Aedes aegypti, Bulletin of Mathematical Biology.

World Health Organization, 2012. http://www.who.int/mediacentre/factsheets/fs117/en/

Yang, H.M., Macoris, M.L.G., Galvani, K.C., Andrighetti, M.T.M. & Wanderley, D.M.V. 2009. Assessing the effects of temperature on the population of Aedes aegypti, the vector of dengue. Epidemiol. Infect., v. 137, p. 1188-1202."
Oportunidade: Inscrições abertas para o Mestrado em Análise e Modelagem de Sistemas Ambientais,http://www2.decom.ufop.br/terralab/oportunidade-inscricoes-abertas-para-o-mestrado-em-analise-e-modelagem-de-sistemas-ambientais/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/09/modelagemUFMG.gif," 

Estão abertas as inscrições para o processo de seleção 2013 do programa de Mestrado em Análise e Modelagem de Sistemas Ambientais da Universidade Federal de Minas Gerais (UFMG).

O curso de mestrado em Análise e Modelagem de Sistemas Ambientais enfatiza a aplicação do instrumental do geoprocessamento, sensoriamento remoto e modelagem computacional para o estudo de sistemas ambientais – sistemas com expressão territorial -, sob uma visão integrada das inter-relações entre seus componentes socioeconômicos, políticos, culturais e naturais – meios físico e biótico.

Sob o comitê multidisciplinar da CAPES, o mestrado em Análise e Modelagem de Sistemas Ambientais abre oportunidades para graduados em diversos campos do saber, como no exemplo de ecólogos, engenheiros ambientais, analistas de sistema, matemáticos computacionais, estatísticos, geógrafos, geólogos, arquitetos, biólogos e muitos outros mais profissionais; todos candidatos a se tornarem analistas e/ou modeladores de sistemas ambientais.

Portanto, como objetivo, o mestrado busca formar profissionais capacitados para análise espacial e modelagem de sistemas, incluindo aí a concepção e solução de uma série de métodos analíticos e de simulação orientados à avaliação qualitativa e quantitativa de complexos temas ambientais em escalas local, regional e global, ou seja, um profissional com perfil de modelador capaz de transitar em diversos campos do saber em busca de soluções às questões ambientais e dirigidas à organização, planejamento e gestão do território.

Maiores informações no site"
Modelos de Crescimento e a Dinâmica de Populações,http://www2.decom.ufop.br/terralab/modelos-de-crescimento-e-a-dinamica-de-populacoes/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/09/graficoCrescimentoCaotico2.png,"Modelos de crescimento populacionais de tempo contínuo

A dinâmica de populações trata das variações, no tempo e no espaço, das densidades e tamanhos de população. Seu estudo visa à melhor compreensão da variação do número de indivíduos de uma determinada população e também, dos fatores que a influenciam em tais variações. Para isso, é necessário o conhecimento das taxas em que se verificam perdas e ganhos de indivíduos e identificar os processos que regulam a variação da população. O interesse neste estudo não é apenas teórico, sendo importante para o controle de pragas, criação de animais, etc (Rachide 2006).

Crescimento linear

O modelo mais simples de crescimento de uma população pode ser definido através de uma função de crescimento linear, onde o incremento da população responde a uma taxa fixa de crescimento (Figura 25), não correlacionada com o tamanho da população em questão, onde N é igual à população e r a taxa constante de incremento (Equação 1). Por exemplo, um rebanho bovíno em que a população cresce por inseminação artificial e o produtor realiza um número fixo deste procedimento a cada ano.

                              
Equação 1


Gráfico de crescimento linear.

Crescimento exponencial

O modelo exponencial de crescimento populacional foi descrito por Malthus (1798). Sua dinâmica surge de processos cumulativos (retroalimentação positiva ou de reforço). Esses processos ocorrem quando a variação liquida do sistema é proporcional ao seu estado atual, reforçando a tendência existente. Neste modelo uma população cresce de acordo com a taxa de natalidade constante r. O crescimento populacional exponencial é definido pela seguinte equação:

                         

Equação 2

onde dN / dt é a taxa instantânea de mudança populacional  e r é a taxa constante de mudança.

Curva de crescimento exponencial

Crescimento logístico

O matemático belga Pierre F. Verhurst propôs em 1837 um modelo que supõe que uma população poderá crescer até um limite máximo, a partir do qual tende a se estabilizar. O modelo proposto por Verhurst atende a uma condição em que a taxa de crescimento efetiva de uma população varia ao longo do tempo. Esse modelo é uma alternativa ao modelo de crescimento exponencial em que a taxa de crescimento é constante e não há limitação para o crescimento do tamanho da população.

Quase todos os textos introdutórios de ecologia usam a versão de tempo contínuo do modelo logístico como o modelo que descreve o crescimento populacional. Esse modelo é uma ferramenta útil para entender como funcionam várias populações, mas não descreve a dinâmica de algumas populações reais. Essas populações exibem comportamento mais complexo e suas taxas de crescimento também estão sob os efeitos de outras populações.

Sob as condições do modelo de tempo contínuo, o fluxo de crescimento se ajusta instantaneamente para desacelerar o crescimento populacional quando a população, N, se aproxima da capacidade de suporte, k, do ambiente que a envolve . Por isso, dificilmente uma população ultrapassa essa capacidade suporte. Qualquer perturbação que cause o crescimento acima desse limite, por exemplo, a entrada instantânea de novos indivíduos na população, é absorvida por um mecanismo de retroalimentação negativa que anula o fluxo de crescimento e permite que o fluxo de mortes rapidamente restaure a população ao nível k (Equação 3).

                 
   Equação 3

Curva de crescimento logístico

Modelos de crescimento populacionais de tempo discreto

Os modelos de tempo discreto evoluem em intervalos de tempo, geralmente, fixos e chamados passos. Presume-se que cada passo o sistema representado possa mudar instantaneamente seu estado. No modelo de tempo contínuo não existem passos, mudanças acontecem continuamente.

Dessa forma, a principal diferença entre modelos populacionais de tempo discreto e contínuo é que o modelo de tempo discreto descreve o número de indivíduos no próximo intervalo temporal, enquanto que o modelo de tempo contínuo descreve a taxa de mudança do tamanho populacional. Em ambos modelos logísticos, a constante k determina a capacidade de suporte do ambiente, ou seja, o número máximo de indivíduos que um habitat é capaz de sustentar.

No modelo populacional de tempo discreto, é mais concreta a possibilidade de uma população ultrapassar a capacidade de suporte de seu ambiente. Neste caso não existe o ajuste instantâneo no fluxo de crescimento populacional. O modelo de tempo discreto descreve uma retroalimentação negativa baseada na dependência da densidade populacional, que não é instantânea, ela acontece após atrasos no tempo. Esses atrasos podem ser entendidos como uma demora na resposta da população, ou sistema, em relação à aproximação da capacidade suporte. Por exemplo, em populações de plantas anuais ou insetos, os indivíduos crescem e reproduzem simultaneamente, mas os jovens não germinam ou eclodem até o próximo ano. Por isso, após um ano em que muitos indivíduos foram produzidos, a população pode ultrapassar a capacidade de suporte do ambiente.

No caso das formigas, onde podemos observar altas taxas de reprodução associadas a um tempo de geração extremamente curto, podemos perceber fortes associações com os modelos de tempo discreto.

Caos determinístico em ecologia de populações

Até recentemente os sistemas dinâmicos eram classificados em três categorias, segundo o padrão de variação no tempo das grandezas que caracterizam os seus estados:

a) estáveis, convergindo para um valor fixo;

b) periódicos, estabelecendo-se em oscilações periódicas; ou

c) imprevisíveis, caracterizado por flutuações irregulares, também denominados aleatórios ou ruidosos.

Porém, em 1963, Lorenz fez uma descoberta que surpreendeu o mundo, enquanto estudava um modelo de previsão do tempo. Seu modelo seguiu um curso que não se enquadrava como aleatório, periódico ou convergente, exibindo um comportamento bastante complexo, embora fosse definido apenas por poucas e simples equações diferenciais. A dinâmica gerada pelo modelo exibia uma característica não usual: dois pontos localizados a uma distância ínfima seguiam trajetórias bastante divergentes. Esta observação levou Lorenz a concluir que a previsão do tempo em um intervalo de tempo longo não seria possível. Sistemas como o de Lorenz são denominados “caótico determinísticos” ou simplesmente “caóticos”; ou seja, embora apresentem um comportamento aperiódico e imprevisível, a sua dinâmica é governada por equações diferenciais determinísticas simples.

A sensibilidade crítica às condições iniciais é a característica fundamental que diferencia os sistemas caóticos determinísticos dos sistemas que apresentam respostas aleatórias ou estocásticas. Para esses últimos sistemas, a mesma condição inicial pode conduzi-los a estados bastante distintos em pequenos intervalos de tempo, o que não ocorre nos sistemas caóticos determinísticos (Bricmont 1996).

Após a descoberta desse fenômeno nos estudos de sistemas físicos a evolução de sua aplicabilidade para a descrição de outros tipos de sistemas se mostrou extremamente interessante, em especial para os sistemas ecológicos. Em 1976, Robert May, trabalhando com modelos de crescimento populacional extremamente simples, não lineares e com atraso na resposta (discretos), mostrou que eles podiam ter um comportamento dinâmico fantasticamente complexo. Este comportamento incluía flutuações populacionais aparentemente aleatórias que eram geradas por modelos determinísticos, o chamado caos determinístico. As descobertas alcançadas por May na ecologia, e por vários outros pesquisadores em uma ampla variedade de outras ciências, provocaram uma das maiores revoluções científicas e filosóficas do século XX (Fernandez 2004).

Partindo de uma equação logística de tempo discreto (Equação 4), May estudou as possibilidades de flutuações populacionais para diferentes valores de r, onde cada valor representaria diferentes populações.

                Equação 4

           

Variando-se o valor da constante r, a iteração desta equação em Nt pode conduzir a soluções estáveis, periódicas ou caóticas . Na figura abaixo, em (a), observa-se uma solução estável. Em (b) tem-se oscilações tendendo a estabilidade. Em (c) tem-se soluções periódicas de período 2. Já em (d), observa-se uma solução aperiódica e imprevisível, característica dos sistemas caóticos. Nos anos que seguiram, os estudos realizados pelo físico matemático Mitchell Feigenbaum (1983) revelaram o processo de duplicação de períodos através do qual os sistemas dinâmicos passavam de um regime laminar e bem comportando para um regime de desordem ou caótico.

Gráficos de flutuações populacionais gerados a partir da equação logística em tempo discreto, nos quais Pop = Nt e  k = 100: (a) r = 1,2, (b) r = 3,0, (c) r =  3,5 e (d) r = 4,0.

Nos diga se gostou deste texto e se ele lhe foi útil? Também não deixe de tirar duvidas ou acrescentar qualquer tópico a discussão. Fique de olho em nossas redes sociais e demais publicações!

Você sabe como a escassez de profissionais qualificados impacta as empresas de Tecnologia da Informação? Clique aqui para conhecer os desafios desta indústria e como uma parceria com o TerraLAB pode ajudar você e a sua empresa. O TerraLAB é um celeiro de talentos que prepara estudantes para o mercado de trabalho, oferecendo vivências em projetos reais e experiência nos mais modernos processos e ferramentas de desenvolvimento de software.

Autoria 

Texto extraído e adaptado da dissertação de mestrado em Ecologia de Biomas Tropicias do biólogo Msc. Alexandre Bahia Gontijo, orienatado pelos professores Dr. Sérvio Ribeiro Pontes e Dr. Tiago Garcia de Senna Carneiro.

Referências Bibliográficas

R. RACHIDE, “Dinâmica de Populações: Um Breve Histórico”, Universidade Federal de Viçosa, III Bienal de SBM, 2006, Brasil

MALTHUS, T. 1798. An Essay on the Principle of Population. Printed for J. Johnson, inSt. Paul’s Church-Yard,London

VERHULTt, P.F. 1838. Notice sur la loi que la population suit dans son accroissement. Correspondances Mathematiques et Physiques, 10, 113-121.

LORENZ, E. 1963 Deterministic nonperiodic flow. J. Atmospheric Sci. 20, 130-141.

BRICMONT, J. 1996. Science of chaos or chaos in science?, em: http://xyz.lanl.gov/abs/chaodyn/9603009.

MAY, R.M. 1976. Simple mathematical models with very complicated dynamics. Nature, Vol. 261, p.459.

FERNANDEZ, F. 2004. O poema imperfeito: Crônicas de biologia, conservação da natureza e seus heróis. 2ª EDIÇÃO. Editora UFPR.

FEIGENBAUM, M. J. 1983. Universal behavior in nonlinear systems, in Order in chaos, Los Alamos, N.M., 1982, Phys. D 7 (1-3) (), 16-39.

Alexandre Bahia Gontijo. Estudo e modelagem das dinâmicas estruturais de assembleias de formigas tropicais em diferentes escalas ecológicas. 2008. Dissertação (Mestrado em Ecologia de Biomas Tropicais) – Universidade Federal de Ouro Preto, Coordenação de Aperfeiçoamento de Pessoal de Nível Superior. Co-Orientador: Tiago Garcia de Senna Carneiro."
TerraME: Uma plataforma para modelagem e simulação das interações sociedade-natureza,http://www2.decom.ufop.br/terralab/o-que-e-o-terrame/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/08/amazon_deforestation.gif,"Na Terra, diversos sistemas naturais e sociais interagem e co-evoluem. A busca pelo entendimento dos fenômenos naturais desafia a ciência desde seu surgimento. Muitos cientistas buscaram conhecer e descrever  por meio de fórmulas matemáticas as regras que regem o comportamento destes fenômenos, ou seja, eles contruíram os modelos matemáticos desses fenômenos.  Nos campos da Física e da Química, muitos modelos evoluíram para representar de maneira realista os fenômenos observados. Dalton em 1807, modelou o átomo como uma partícula esférica e indivisível sem qualquer carga elétrica. Em 1913, Borh demonstrou que o átomo possuia um núcelo positivo cercado por eletrons com cargas negativas e organizados em órbitas com diferentes níveis de energia. A Terra já foi considerada achatada. Os modelos matemáticos conhecidos pela ciência permitem a construção de prognósticos futuros sobre muitos sistemas naturais. Para entender a importância da continua evolução de tais modelos, considere a utilidade dos modelos climáticos para o agronegócio e  para os sistemas de alerta de desastres naturais.

Apesar dos esforços da antropologia e da sociologia, o funcionamento dos sistemas sociais continua mal entendido. Contudo, atualmente os sistemas sociais são vistos como os principais direcionadores das mudanças por que passam os sistemas naturais. Em contrapartida, os sistemas naturais são os principais condicionadores do comportamento dos sistemas sociais. Desta maneira, os sistemas sociais afetam e são afetados pelos sistema naturais. Portanto, modelos matemático-computacionais capazes de simular as interações entre os sistemas sociais e naturais constituem ferramentas essenciais para os responsáveis pela definição de políticas que regulam o uso dos recursos Terrestres.

Na verdade, qualquer decisor do setor público ou privado pode se beneficiar de modelos computacionais capazes de simular cenários que avaliam os impactos de diferentes estratégias: (1) de uso de recursos – espaço, tempo, vegetação, água, etc.; ou (2) de controle e resposta a riscos – epidemias, incêndios, inundações, etc. No Brasil, o Instituto Nacional de Pesquisas Espaciais (INPE) utiliza o TerraME para responder questões relacionadas às Mudanças de Uso e Cobertura do Solo na região Amazônica [1][2][3], para responder questões relativas ao Monitoramento, Análise e Alerta de Riscos, e para responder questões relacionadas à Emissão de Gases de Efeito Estufa [4]. Pesquisadores da Fundação Osvaldo Cruz (FIOCRUZ) utilizam o TerraME para avaliar questões relativas ao Controle da Dengue [5].

TerraME Overview

O TerraME – TerraME Modeling Environment – (www.terrame.org) é um conjunto de ferramentas que apoiam todas as fases do processo de desenvolvimento de modelos ambientais, isto é, modelos que reproduzem o comportamento de processos naturais e sociais, mostrando suas interações e as mudança que promovem de forma diferenciada em cada localização do espaço geográfico. No TerraME, um modelo ambiental é visto como um micro-mundo, isto é, um mundo virtual cuja paisagem representa uma determinada região do planeta Terra.e cujos agentes, autômatos  ou sistemas que o habitam são capazes de simular atores e processos de mudanças reais. As ferramentas TerraME são desenvolvidas pelo TerraLAB – Laboratório para Modelagem e Simulação do Sistemas Terrestre do Departamento de Computação (DECOM) da Universidade Federal de Ouro Preto (UFOP) em parceria com o Instituto Nacional de Pesquisas Espaciais (INPE). Elas são distribuídas gratuitamente para as plataformas Linux, Windows e Mac de 32 e 64 bits.

O principal diferencial do TerraME com relação as plataformas de modelagem ambiental disponíveis atualmente está no suporte ao desenvolvimento de modelos que consideram múltiplas escalas espaço-temporais e no suporte ao uso simultâneo a múltiplos paradigmas de modelagem: Teoria Geral de Sistemas, Teoria de Agentes e Teoria de Autômatos Celulares. Desta maneira, o modelador não se vê restringido pela a escolha de um único paradigma de modelagem e pode representar de maneira realista as diferentes escalas nas quais mudanças ocorrem e nas quais agem as forças direcionadoras destas mudança. Por exemplo, na Amazônia, o processo de desmatamento é influenciado localmente pela situação econômica dos pequenos produtores, enquanto que globalmente é influenciado pelos preços de commodities como soja e gado.

TerraME Graphical Interface for Modeling and Simulation

Para apoiar a concepção e o projeto de modelos ambientais, o TerraME oferece um ambiente integrado de desenvolvimento chamado TerraME GIMS – Graphical Interface for Modeling and Simulation que permite ao modelador descrever seu modelo visualmente, isto é, por meio de diagramas.  Estes diagramas explicitam as entidades consideradas no modelo, a maneira como estão organizadas hierarquicamente e como interagem umas com as outras. Desta forma, o TerraME GIMS facilita a verificação das premissas nas quais os modelos se baseiam. Ele as comunica aos tomadores de decisão e aos membros da equipe de desenvolvimento que é, rotineiramente, interdisciplinar. O TerraME GIMS é distribuído gratuitamente na forma de um plugin para a plataforma Eclipse (www.eclipse.org).

TerraME Interpreter

Para apoiar a construção de modelos realistas, o simulador TerraME é integrado com Sistema de Informação Geográfica (SIG) que permite ao modelador parametrizar os modelos a partir de bancos de dados geográficos, no qual séries temporais de imagens de satélites e de mapas digitais descrevem a região sob estudo. O simulador TerraME é distribuído na forma de um interpretador que recebe como entrada o programa que representa o modelo ambiental e, então, o executa produzindo dados de saída que são automaticamente armazenados no banco de dados geográfico. Para facilitar a manutenção e evolução dos modelos ambientais, os modelos TerraME são representados em uma linguagem de modelagem de altíssimo nível, chamada TerraML – TerraME Modeling Language, que estende a linguagem de programação Lua  e na qual é fácil descrever as propriedades do espaço e os atores e processos que as alteram. Nesta linguagem, um modelo pode ser facilmente decomposto em modelos mais simples que representam diferentes escalas de um mesmo processo.

TerraME – Modeling in Multiple Scales

Para apoiar a depuração dos modelos e a análise dos resultados por eles produzidos, o TerraME possui uma série de componentes denominados TerraME Observers. Estes componentes são capazes de exibir os resultados, em tempo-real, na forma de gráficos e mapas dinâmicos em duas e três dimensões.

TerraME Observers 

Para apoiar a análise de sensibilidade do modelo e a projeção de cenários simulados, o núcleo de simulação TerraME oferece uma versão de alto desempenho chamada TerraME  HPA – High Performance Architecture. Esta versão do simulador é capaz de tirar proveito do poder de processamento e armazenamento de arquiteturas de hardware multiprocessadas ou de rede de computadores. Desta forma, experimentos para análise de sensibilidade dos modelos ou para a simulação de diferentes cenários de mudança podem ser executas em paralelo e em menor tempo.

Para apoiar a calibração dos modelos ambientais, o TerraME oferece um conjunto de métodos para aferir o ajuste entre mapas reais e mapas simulados, além de um conjunto de métodos para otimizar os parâmetros dos modelos de forma que eles maximizem o ajuste entre os resultados das simulações e os dados observados.

 

Referências Bibliográficas

 

[1] G. Câmara, A. P. Aguiar, M. I. Escada, et al. Amazon Deforestation Models. Science, vol 307, 15 February 2005, pp. 1043-1044.

[2] A.P. Aguiar, G. Câmara, M.I.S. Escada, Spatial statistical analysis of land-use determinants in the Brazilian Amazon: exploring intra-regional heterogeneity. Ecological Modelling, vol 209(1-2):169–188, 2007.

[3] Evaldinólia Moreira, Sergio Costa, Ana Paula Aguiar, Gilberto Camara, Tiago Carneiro. Dynamical coupling of multiscale land change models, Landscape Ecology, 24(9), p. 1183-1194, 2009.

[4] Aguiar APD, Ometto JP, Nobre CA, Lapola DM, Almeida C, Vieira IC, Soares JV, Alvala R, Saatchi S, Valeriano V, Castilla-Rubio JC (2012) Modeling the spatial and temporal heterogeneity of deforestation-driven carbon emissions: the INPE-EM framework applied to the Brazilian Amazon. Global Change Biology (Print)

[5]  LANA, R. M. ; Carneiro, T. G. S. ; Honório, N. A. ; Codeço, C. T. . Multiscale analysis and modeling of Aedes aegypti population spatial dynamics. Journal of Information and Data Management,2(2), p.211-220, 2011."
"Estimativa de emissões brasileiras dos gases do efeito estufa baseiam-se em tecnologia do TerraLAB, o simulador TerraME",http://www2.decom.ufop.br/terralab/estimativa-de-emissoes-dos-gases-do-efeito-estufa-brasileira-baseiam-se-em-produtos-terralab-o-simulador-terrame/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/08/download.jpg," 

O sistema INPE-EM (INPE – Emission Model) é um novo serviço do Instituto Nacional de Pesquisas Espaciais (INPE) que visa tornar disponíveis estimativas anuais de emissões de gases do efeito estufa (GEE) por mudanças de cobertura da terra no Brasil.

O arcabouço de modelagem INPE-EM foi construído no ambiente de modelagem TerraME, desenvolvido pelo TerraLAB em parceira com a Divisão de Processamento de Imagem (DPI) o Centro de Ciência do Sistema Terrestre (CCST) do INPE.

A plataforma de modelagem ambiental TerraME pemite que simulações sejam utilizadas para avaliar cenários futuros de mudanças e também para avaliar os impactos de diferentes políticas públicas. Ele oferece suporte simultâneo a vários paradigmas de modelagem ambiental: baseado em agentes, automatos celulares, teoria geral de sistemas, teoria de jogos, etc. É possivel densenvolver simulações em multiplas escalas espaciais e temporais, nas quais agentes autônomos e interativos alteram propriedades do espaço geográfico. Para ofercer mais realismo às simulações, o espaço geográfico pode ser representado por bancos de dados geográficos armazenados em Sistemas de Informação Geográfica, como a biblioteca TerraLib.

Para fazer download do TerraME clique aqui."
Workshop VANTs: ArDrone E Aplicações,http://www2.decom.ufop.br/terralab/workshop-vants-ardrone-e-aplicacoes/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/08/IMG_20120814_140355.jpg,"No dia 22/08 foi organizado entre os laboratórios  TerraLAB e iMobilis o workshop de VANTs e aplicações. Vants são veículos terrestres, aquáticos e subaquáticos autônomos e aéreos não tripulados.

O aluno de graduação Johnnatan Messias fez uma apresentação sobre o “brinquedo”/plataforma ArDrone, um VANT comercial de baixo custo.

O ArDrone, fabricado pela empresa Parrot [http://www.parrot.com/usa/], é vendido a baixo custo, comparado com outros veículos de mesmo porte. Movido por quatro hélices, o veículo pode ser acionado pelo próprio computador e pilotado por controle remoto. Associado a ele existem varios jogos de realidade ampliada, que permite a manipulação do quadricoptero via tablet ou celular.

O ArDrone possui codigo livre, com sistema operacional Linux e uma API de programação para desenvolvimento de jogos e aplicações de realidade ampliada.

Durante o workshop, os pesquisadores do TerraLAB apresentaram propostas de integração entre o ArDrone e suas plataformas de simulação ambiental.

Os slides da apresentação [Slides:Projeto Imobilis – ArDrone ]

 

[youtube width=”600″ height=”365″ video_id=”aMwLjRYIjbU&amp;feature”]

 

 

 "
INPE e UFOP desenvolvem tecnologias que fundamentam políticas públicas em bases científicas,http://www2.decom.ufop.br/terralab/inpe-e-ufop-desenvolvem-tecnologias-que-fundamentam-politicas-publicas-em-bases-cientificas/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/08/clue_amazonia_baseline.jpg,"O INPE e o TerraLAB – Laboratório para Modelagem e Simulação do Sistema Terrestre, do DECOM/UFOP são responsáveis por desenvolvimentos  tecnológicos que permitem que a definição de políticas públicas brasileiras seja feita com base em critérios científicos. Estas tecnologias vêm sendo utilizadas para definir as politicas nacionais de uso do solo na Amazônia e para definir as políticas externas com relação a emissões de gases de efeito estufa.

Nesta sexta-feira (18), no workshop do Global Land Project (GLP) – Land Use Transitions in South America: framing the present, preparing for a sustainable future, foi lançado oficialmente e disponibilizado na Internet o LuccME, uma ferramenta de código aberto para a construção e customização de modelos de mudança de uso e cobertura da terra. O LuccME é uma extensão do ambiente de modelagem TerraME.

O reconhecimento internacional a esses esforços vem com a mudança do escritório internacional do GLP para o Brasil. O GLP é uma iniciativa do International Human Dimensions Programme on Global Environmental Change (IHDP) e do International Geosphere–Biosphere Programme (IGBP), que possui escritório regional no INPE, para o estudo da interação entre homem e ambiente terrestre em prol do melhor entendimento científico das mudanças globais.

Veja matéria completa aqui."
Workshop de Metodologia de Testes: TerraLAB e IMobilis,http://www2.decom.ufop.br/terralab/workshop-de-metodologia-de-testes-terralab-e-imobilis/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/08/IMG_20120814_140355.jpg," 

No dia 14/08/2012, os membros dos laboratórios  TerraLab e  iMobilis, coordenado pelo Prof. Dr. Tiago Garcia de Senna Carneiro e Prof. Dr. Ricardo Rabelo, respectivamente, reuniram-se para um workshop de metodologia de testes, no qual foram apresentados estudos sobre diferentes métodos feitos pelos participantes do TerraLab e os métodos de teste de software do iMobilis.

A Programação:

14:00 – 14:40h  – How Google Test Software (James Whittaker, Jason Arbon, Jeff Carollo) — Apresentador: Henrique – TerraLab [Slides:  How Google Tests Software cap 1]
14:50 – 15:30h –  Effective Methods for Software Testing (William E. Perry) — Apresentador: Breno – TerraLab  [Slides:  Effective Methods for Software Testing ]
15:40 – 16:20h – Metodologia de testes para sistemas embarcados – Uso de caso sistema uGuide — Apresentador: Gustavo Quintão – iMobilis [Slides: Metodologia de testes]
16:20 – 16:40  – COFFE BREAK
16:40 – 17:20h – The Agile Samurai: How Agile Masters Deliver Great Software (Jonathan Rasmusson) — Apresentador: Rafael – – TerraLab [Slides: AgileSamurai]
17:30 – 18:10h – Agile Testing: A Practical Guide for Testers and Agile Teams  (Lisa Crispin, Janet Gregory)  – Apresentador: Washington – TerraLab [Slides: Agile Testing]

Fotos do Workshop:

 

 

 

 

 

 

 

 "
O bem estar no ambiente de trabalho – TerraLAB,http://www2.decom.ufop.br/terralab/o-bem-estar-no-ambiente-de-trabalho-terralab/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/08/2012-08-22-20.20.10.jpg,"Ambiente de trabalho saudável é essencial para para qualidade de vida. Saúde e bem-estar no ambiente de trabalho exercem grande influência sobre a motivação, permanência, criatividade e produtividade de qualquer equipe. Contudo, sentir-se pressionado é um fato normal.

Novas atividades trazem novos desafios. Um aumento temporário da carga de trabalho ou um prazo que se aproxima podem incomodar. Por isso,  ambiente de trabalho deve ser  confortavel e organizado, previlegiar a criatividade  e bem estar membros das equipes de trabalho.

Pensando nisso,  o TerraLAB se organizou para mudar e crescer melhor. 

Fizemos uma nova decoração. Mapas dos século XVII enfeitam nossas paredes, uma mesinha de café espera pelos visitantes e a arte está por ali.

Nélia, a mascote do laboratório, veio dar vida ao ambiente.
Esperamos que gostem…"
Blog do TerraLAB foi lançado!,http://www2.decom.ufop.br/terralab/hello-world/,http://www2.decom.ufop.br/terralab/wp-content/uploads/2012/08/31.jpg,Bem vindo ao novo blog do TerraLAB. Encotre aqui novidades e muito mais!
